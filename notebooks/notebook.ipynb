{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b504a1d6-28b7-4242-9844-b07b4f61973b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data scraping and pre processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "054b3064-b74d-4c84-921f-25f703d707cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Spark Session"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, avg, sum, regexp_replace, split, trim, lit, udf\n",
    ")\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MultiSourceDataIntegration\").getOrCreate()\n",
    "\n",
    "# Define DBFS data directory\n",
    "data_dir = \"/FileStore/data/\"\n",
    "\n",
    "print(\"Spark Session initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4a118c4a-f6c0-4587-9c07-319091868a83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Datasets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from DBFS...\n\nAll datasets loaded successfully.\n\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Loading datasets from DBFS...\\n\")\n",
    "\n",
    "    # 1. NHTSA FARS Accident Data (2023)\n",
    "    accident_df = spark.read.csv(f\"{data_dir}accident.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # 2. SAMHSA Substance Abuse Data\n",
    "    map_data_df = spark.read.csv(f\"{data_dir}map_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # 3. FBI UCR Crime Data (2024)\n",
    "    url = \"https://www.beautifydata.com/united-states-crimes/fbi-ucr/2024/ranking-of-us-cities-by-crime-rate-by-crime-type/violent?utm_source=chatgpt.com\"\n",
    "    tables = pd.read_html(url, flavor='lxml')\n",
    "    crime_df = tables[0] if tables else None\n",
    "\n",
    "    # 4. Kaggle US Household Income Data\n",
    "    kaggle_income_df = spark.read.csv(f\"{data_dir}kaggle_income.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # 5. CDC PLACES County Health Data (2025)\n",
    "    county_health_df = spark.read.csv(\n",
    "        f\"{data_dir}PLACES__Local_Data_for_Better_Health__County_Data__2025_release (1).csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "\n",
    "    # 6. EPA Air Quality Index Data (2025)\n",
    "    aqi_df = spark.read.csv(f\"{data_dir}annual_aqi_by_county_2025.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    print(\"All datasets loaded successfully.\\n\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18baecb3-fbed-482f-a50b-27bcaae5b0ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create State Mapping"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State standardization mapping created.\n\n"
     ]
    }
   ],
   "source": [
    "# Create state abbreviation to full name mapping\n",
    "state_abbreviations = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "state_name_udf = udf(lambda abbr: state_abbreviations.get(abbr, abbr), StringType())\n",
    "\n",
    "print(\"State standardization mapping created.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c273f7fc-5357-4d6e-a056-618ac459823f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Crime Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing crime data...\nCrime data processed.\n\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Processing crime data...\")\n",
    "\n",
    "    # Convert crime data to Spark DataFrame\n",
    "    crime_spark_df = spark.createDataFrame(crime_df)\n",
    "\n",
    "    # Extract City and State from combined 'City' column (format: \"CityName, StateCode\")\n",
    "    crime_spark_df = (\n",
    "        crime_spark_df\n",
    "        .withColumn(\"City_State_Split\", split(col(\"City\"), \", \"))\n",
    "        .withColumn(\"City\", lower(trim(col(\"City_State_Split\").getItem(0))))\n",
    "        .withColumn(\"State\", lower(state_name_udf(trim(col(\"City_State_Split\").getItem(1)))))\n",
    "        .select(\n",
    "            col(\"City\"),\n",
    "            col(\"State\"),\n",
    "            col(\"Crime Rate Per 100K\").alias(\"Crime_Rate_Per_100K\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create city-to-county mapping from income data\n",
    "    city_county_mapping_df = (\n",
    "        kaggle_income_df\n",
    "        .select(\n",
    "            lower(col(\"City\")).alias(\"City\"),\n",
    "            lower(col(\"State_Name\")).alias(\"State\"),\n",
    "            lower(regexp_replace(col(\"County\"), \"(?i) county$\", \"\")).alias(\"County\")\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Map counties to crime data\n",
    "    crime_with_county_df = (\n",
    "        crime_spark_df\n",
    "        .join(city_county_mapping_df, on=[\"City\", \"State\"], how=\"left\")\n",
    "        .filter(col(\"County\").isNotNull())\n",
    "        .select(\"City\", \"State\", \"County\", \"Crime_Rate_Per_100K\")\n",
    "    )\n",
    "\n",
    "    print(\"Crime data processed.\\n\")\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8da9b590-9969-449f-b00d-4d62aaf5a14b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Accident Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing accident data...\nAccident data processed.\n\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    print(\"Processing accident data...\")\n",
    "\n",
    "    accident_fatalities_df = (\n",
    "        accident_df\n",
    "        .select(\n",
    "            col(\"STATENAME\").alias(\"State\"),\n",
    "            col(\"COUNTYNAME\").alias(\"County\"),\n",
    "            col(\"CITYNAME\").alias(\"City\"),\n",
    "            col(\"FATALS\")\n",
    "        )\n",
    "        .groupBy(\"State\", \"County\", \"City\")\n",
    "        .agg(sum(\"FATALS\").alias(\"Total_Fatalities\"))\n",
    "        # Clean county names: remove FIPS codes in parentheses\n",
    "        .withColumn(\"County\", regexp_replace(col(\"County\"), \"\\\\s*\\\\(\\\\d+\\\\)\", \"\"))\n",
    "        # Standardize to lowercase\n",
    "        .withColumn(\"State\", lower(col(\"State\")))\n",
    "        .withColumn(\"County\", lower(col(\"County\")))\n",
    "        .withColumn(\"City\", lower(col(\"City\")))\n",
    "    )\n",
    "\n",
    "    print(\"Accident data processed.\\n\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d96b4206-6571-419e-9a14-393688965d83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Income Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing income data...\nIncome data processed.\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing income data...\")\n",
    "\n",
    "income_df = (\n",
    "    kaggle_income_df\n",
    "    .select(\n",
    "        col(\"State_Name\").alias(\"State\"),\n",
    "        col(\"County\"),\n",
    "        col(\"City\"),\n",
    "        col(\"Median\")\n",
    "    )\n",
    "    .groupBy(\"State\", \"County\", \"City\")\n",
    "    .agg(avg(\"Median\").alias(\"Median_Income\"))\n",
    "    # Clean county names: remove ' County' suffix\n",
    "    .withColumn(\"County\", regexp_replace(col(\"County\"), \"(?i) County$\", \"\"))\n",
    "    # Standardize to lowercase\n",
    "    .withColumn(\"State\", lower(col(\"State\")))\n",
    "    .withColumn(\"County\", lower(col(\"County\")))\n",
    "    .withColumn(\"City\", lower(col(\"City\")))\n",
    ")\n",
    "\n",
    "print(\"Income data processed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d60330-ca69-4645-9d06-87af7a0bd759",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Health & Air Quality Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing health and air quality data...\nHealth and air quality data processed.\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing health and air quality data...\")\n",
    "\n",
    "# Health Data Processing (County Level)\n",
    "prepared_health_df = (\n",
    "    county_health_df\n",
    "    .filter(col(\"Measure\") == \"Any disability among adults\")\n",
    "    .select(\n",
    "        lower(col(\"StateDesc\")).alias(\"State\"),\n",
    "        lower(col(\"LocationName\")).alias(\"County\"),\n",
    "        col(\"Data_Value\").alias(\"Disability_Rate\")\n",
    "    )\n",
    "    .groupBy(\"State\", \"County\")\n",
    "    .agg(avg(\"Disability_Rate\").alias(\"Disability_Rate\"))\n",
    ")\n",
    "\n",
    "# Air Quality Data Processing (County Level)\n",
    "prepared_aqi_df = (\n",
    "    aqi_df\n",
    "    .select(\n",
    "        lower(col(\"State\")).alias(\"State\"),\n",
    "        lower(col(\"County\")).alias(\"County\"),\n",
    "        col(\"Median AQI\").alias(\"Median_AQI\")\n",
    "    )\n",
    "    .groupBy(\"State\", \"County\")\n",
    "    .agg(avg(\"Median_AQI\").alias(\"Median_AQI\"))\n",
    ")\n",
    "\n",
    "print(\"Health and air quality data processed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c39f7d-87f0-4cce-af8b-ba272e7b51f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build Integrated Dataset"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building integrated dataset...\nIntegrated dataset built successfully.\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Building integrated dataset...\")\n",
    "\n",
    "# Start with city-level crime data as base\n",
    "mega_df = crime_with_county_df.select(\n",
    "    col(\"City\"),\n",
    "    col(\"State\"),\n",
    "    col(\"County\"),\n",
    "    col(\"Crime_Rate_Per_100K\").alias(\"City_Crime_Rate_Per_100K\")\n",
    ")\n",
    "\n",
    "# Join city-level accident data\n",
    "mega_df = mega_df.join(\n",
    "    accident_fatalities_df,\n",
    "    on=[\"State\", \"County\", \"City\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# Join city-level income data\n",
    "mega_df = mega_df.join(\n",
    "    income_df,\n",
    "    on=[\"State\", \"County\", \"City\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# Join county-level health data\n",
    "if prepared_health_df is not None and not prepared_health_df.isEmpty():\n",
    "    mega_df = mega_df.join(\n",
    "        prepared_health_df,\n",
    "        on=[\"State\", \"County\"],\n",
    "        how=\"outer\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: Health data unavailable. Adding null Disability_Rate column.\")\n",
    "    mega_df = mega_df.withColumn(\"Disability_Rate\", lit(None).cast(DoubleType()))\n",
    "\n",
    "# Join county-level air quality data\n",
    "mega_df = mega_df.join(\n",
    "    prepared_aqi_df,\n",
    "    on=[\"State\", \"County\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "print(\"Integrated dataset built successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5894dd1-829f-40c5-9786-e2c3765737f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Integrated Dataset"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nFINAL INTEGRATED DATASET (mega_df)\n================================================================================\n\nSchema:\nroot\n |-- State: string (nullable = true)\n |-- County: string (nullable = true)\n |-- City: string (nullable = true)\n |-- City_Crime_Rate_Per_100K: double (nullable = true)\n |-- Total_Fatalities: long (nullable = true)\n |-- Median_Income: double (nullable = true)\n |-- Disability_Rate: double (nullable = true)\n |-- Median_AQI: double (nullable = true)\n\n\nSample Data (first 10 rows):\n+-------+-------+--------------+------------------------+----------------+------------------+------------------+----------+\n|State  |County |City          |City_Crime_Rate_Per_100K|Total_Fatalities|Median_Income     |Disability_Rate   |Median_AQI|\n+-------+-------+--------------+------------------------+----------------+------------------+------------------+----------+\n|alabama|autauga|abbeville     |NULL                    |NULL            |25216.0           |33.099999999999994|NULL      |\n|alabama|autauga|adamsville    |NULL                    |NULL            |47460.0           |33.099999999999994|NULL      |\n|alabama|autauga|alabaster     |NULL                    |NULL            |149711.33333333334|33.099999999999994|NULL      |\n|alabama|autauga|albertville   |NULL                    |NULL            |160358.5          |33.099999999999994|NULL      |\n|alabama|autauga|alexander city|NULL                    |NULL            |31306.0           |33.099999999999994|NULL      |\n|alabama|autauga|alexandria    |NULL                    |NULL            |32017.0           |33.099999999999994|NULL      |\n|alabama|autauga|aliceville    |NULL                    |NULL            |23175.0           |33.099999999999994|NULL      |\n|alabama|autauga|andalusia     |NULL                    |NULL            |122645.33333333333|33.099999999999994|NULL      |\n|alabama|autauga|anniston      |NULL                    |NULL            |36615.75          |33.099999999999994|NULL      |\n|alabama|autauga|arab          |NULL                    |NULL            |300000.0          |33.099999999999994|NULL      |\n+-------+-------+--------------+------------------------+----------------+------------------+------------------+----------+\nonly showing top 10 rows\n\n\nDataset Statistics:\nTotal rows: 20628\nTotal columns: 8\n\nColumn Summary:\n  State: 20628 non-null values\n  County: 20627 non-null values\n  City: 20347 non-null values\n  City_Crime_Rate_Per_100K: 622 non-null values\n  Total_Fatalities: 8199 non-null values\n  Median_Income: 12891 non-null values\n  Disability_Rate: 18468 non-null values\n  Median_AQI: 9470 non-null values\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL INTEGRATED DATASET (mega_df)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "mega_df.printSchema()\n",
    "\n",
    "print(\"\\nSample Data (first 10 rows):\")\n",
    "mega_df.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total rows: {mega_df.count()}\")\n",
    "print(f\"Total columns: {len(mega_df.columns)}\")\n",
    "\n",
    "print(\"\\nColumn Summary:\")\n",
    "for column in mega_df.columns:\n",
    "    non_null_count = mega_df.filter(col(column).isNotNull()).count()\n",
    "    print(f\"  {column}: {non_null_count} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e597b9e-6471-44bd-a634-0d8f23160261",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Azure Storage Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Azure Storage Configuration\n",
    "storage_account = \"lab94290\"\n",
    "container = \"airbnb\"\n",
    "data_path = \"airbnb_1_12_parquet\"\n",
    "\n",
    "# SAS token for authentication (new token from email)\n",
    "sas_token = \"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fcdef4-0c8b-412d-99ac-41fe6cdf80d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure Spark for Azure Storage"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Azure Storage connection configured successfully\n  Storage Account: lab94290\n  Container: airbnb\n"
     ]
    }
   ],
   "source": [
    "# Configure Spark to use SAS token authentication for ADLS Gen2\n",
    "# This is the exact configuration from the instructor's notebook\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\",\n",
    "    \"SAS\"\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "print(\"✓ Azure Storage connection configured successfully\")\n",
    "print(f\"  Storage Account: {storage_account}\")\n",
    "print(f\"  Container: {container}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fb14c1-903b-4508-b34d-a4dd941b74c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Data from Azure Storage"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nLoading Airbnb data from: abfss://airbnb@lab94290.dfs.core.windows.net/airbnb_1_12_parquet\n✓ Loaded 2,098,880 rows\n  Columns: 50\n+--------------------+-----+--------------------+--------------------+--------+------------+--------+--------------------+-------+--------------------+--------------------+--------------------+---------+---------+------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------------------+--------------------+---------------------+-------------+----------------------+-----------+----------+------------------+-----------------+--------------------+--------------------+-----------+--------+--------------------+--------------------------+-------+----------------+--------------------+------------+\n|                name|price|               image|         description|category|availability|discount|             reviews|ratings|         seller_info|         breadcrumbs|            location|      lat|     long|guests|pets_allowed|   description_items|     category_rating|         house_rules|             details|          highlights| arrangement_details|           amenities|              images|     available_dates|                 url|           final_url|       listing_title|property_id|        listing_name|    location_details|description_by_sections|    description_html|location_details_html|is_supperhost|host_number_of_reviews|host_rating|hosts_year|host_response_rate|is_guest_favorite|      travel_details|     pricing_details|total_price|currency| cancellation_policy|property_number_of_reviews|country|postcode_map_url|          host_image|host_details|\n+--------------------+-----+--------------------+--------------------+--------+------------+--------+--------------------+-------+--------------------+--------------------+--------------------+---------+---------+------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------------------+--------------------+---------------------+-------------+----------------------+-----------+----------+------------------+-----------------+--------------------+--------------------+-----------+--------+--------------------+--------------------------+-------+----------------+--------------------+------------+\n|Rental unit in Br...|  238|https://a0.muscac...|Our exceptionally...|   Stays|        true|    NULL|[\"We had an amazi...|   4.96|{\"seller_id\":\"193...|Broadbeach, Austr...|Broadbeach, Austr...| -28.0307| 153.4319|     4|       false|[\"Entire rental u...|[{\"name\":\"Cleanli...|[\"Check-in: 3:00 ...|[\"4 guests\",\"2 be...|[{\"name\":\"Dive ri...|[{\"name\":\"Bedroom...|[{\"group_name\":\"S...|[\"https://a0.musc...|[\"2025-06-23\",\"20...|https://www.airbn...|https://www.airbn...|Boutique BEACH Lu...|   40458495|Entire rental uni...|[{\"title\":\"Neighb...|   [{\"title\":null,\"v...|\\n<span class=\"l1...| \\n<span class=\"l1...|         true|                   323|       4.95|         6|               100|             true|{\"check_in\":null,...|{\"airbnb_service_...|        404|     USD|[{\"cancellation_n...|                       192|   NULL|            NULL|https://a0.muscac...|        NULL|\n|Home in Port Wels...| NULL|https://a0.muscac...|“VICTORIA’S SECRE...|   Stays|       false|    NULL|[\"It’s a nice pea...|   4.74|{\"seller_id\":\"279...|Port Welshpool, V...|Port Welshpool, V...|-38.70057|146.46478|     5|       false|[\"Entire home\",\"3...|[{\"name\":\"Cleanli...|[\"Check-in after ...|[\"5 guests\",\"2 be...|[{\"name\":\"Self ch...|[{\"name\":\"Bedroom...|[{\"group_name\":\"B...|[\"https://a0.musc...|[\"2025-06-22\",\"20...|https://www.airbn...|https://www.airbn...|\"Victorias Secret...|    5395975|Entire home in Po...|[{\"title\":\"Neighb...|   [{\"title\":null,\"v...|\\n<span class=\"l1...| \\n<span class=\"l1...|         true|                   175|       4.74|        10|               100|             true|{\"check_in\":null,...|{\"airbnb_service_...|       NULL|     USD|[{\"cancellation_n...|                       175|   NULL|            NULL|https://a0.muscac...|        NULL|\n|Rental unit in Br...| NULL|https://a0.muscac...|Cuba apartment wi...|   Stays|       false|    NULL|[\"Lovely apartmen...|   4.91|{\"seller_id\":\"102...|Brașov, Județul B...|Brașov, Județul B...|  45.6415|  25.5874|     4|       false|[\"Entire rental u...|[{\"name\":\"Cleanli...|[\"Check-in after ...|[\"4 guests\",\"2 be...|[{\"name\":\"Self ch...|[{\"name\":\"Bedroom...|[{\"group_name\":\"S...|[\"https://a0.musc...|[\"2025-06-18\",\"20...|https://www.airbn...|https://www.airbn...|Cuba Apartment in...|   53776428|Entire rental uni...|[{\"title\":\"Brașov...|   [{\"title\":null,\"v...|\\n<span class=\"l1...| \\n<span class=\"l1...|         true|                   229|       4.92|         4|               100|             true|{\"check_in\":null,...|{\"airbnb_service_...|       NULL|     USD|[{\"cancellation_n...|                       107|   NULL|            NULL|https://a0.muscac...|        NULL|\n|cycladic house in...|15118|https://a0.muscac...|Villa “Theros”,  ...|   Stays|       false|    NULL|[\"We truly enjoye...|      5|{\"seller_id\":\"113...|       Aliki, Greece|       Aliki, Greece|  37.0023|  25.1394|     6|       false|[\"Cycladic home\",...|[{\"name\":\"Cleanli...|[\"Check-in: 3:00 ...|[\"6 guests\",\"3 be...|[{\"name\":\"Mountai...|[{\"name\":\"Bedroom...|[{\"group_name\":\"S...|[\"https://a0.musc...|[\"2025-06-27\",\"20...|https://www.airbn...|https://www.airbn...|      Villa “Theros”|   17988678|Cycladic home in ...|[{\"title\":\"Aliki,...|   [{\"title\":null,\"v...|\\n<span class=\"l1...| \\n<span class=\"l1...|        false|                    22|          5|         8|               100|             true|{\"check_in\":null,...|{\"airbnb_service_...|      15118|     USD|[{\"cancellation_n...|                        22|   NULL|            NULL|https://a0.muscac...|        NULL|\n|Home in Guía · ★4...|  174|https://a0.muscac...|* Conjunto de dos...|   Stays|        true|    NULL|[\"Recommended. Pe...|   4.83|{\"seller_id\":\"122...|Guía, Canarias, S...|Guía, Canarias, S...| 28.11846|-15.62416|    11|        true|[\"Entire home\",\"6...|[{\"name\":\"Cleanli...|[\"Check-in after ...|[\"11 guests\",\"4 b...|[{\"name\":\"Enjoy t...|[{\"name\":\"Bedroom...|[{\"group_name\":\"B...|[\"https://a0.musc...|[\"2025-06-17\",\"20...|https://www.airbn...|https://www.airbn...|         House Guide|   17857973|Entire home in Gu...|                  []|   [{\"title\":null,\"v...|\\n<span class=\"l1...|                 NULL|        false|                     6|       4.83|         8|              NULL|            false|{\"check_in\":\"2025...|{\"airbnb_service_...|        478|     USD|[{\"cancellation_n...|                         6|   NULL|            NULL|https://a0.muscac...|        NULL|\n+--------------------+-----+--------------------+--------------------+--------+------------+--------+--------------------+-------+--------------------+--------------------+--------------------+---------+---------+------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------------------+--------------------+---------------------+-------------+----------------------+-----------+----------+------------------+-----------------+--------------------+--------------------+-----------+--------+--------------------+--------------------------+-------+----------------+--------------------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Use abfss:// protocol with dfs.core.windows.net (ADLS Gen2)\n",
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{data_path}\"\n",
    "\n",
    "print(f\"\\nLoading Airbnb data from: {path}\")\n",
    "\n",
    "# Read the parquet dataset\n",
    "airbnb_df = spark.read.parquet(path)\n",
    "\n",
    "print(f\"✓ Loaded {airbnb_df.count():,} rows\")\n",
    "print(f\"  Columns: {len(airbnb_df.columns)}\")\n",
    "\n",
    "airbnb_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf73a2cd-bd7b-4879-b54a-91e7e6be5b9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Debug Mode & Data Sampling"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Using full dataset: 2,098,880 rows\n"
     ]
    }
   ],
   "source": [
    "# Debug mode - sample data for faster development\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    original_count = airbnb_df.count()\n",
    "    airbnb_df = airbnb_df.sample(fraction=4000/original_count, seed=42).limit(4000)\n",
    "    print(f\"\\n  DEBUG MODE: Sampled to {airbnb_df.count():,} rows for faster processing\")\n",
    "else:\n",
    "    print(f\"\\n Using full dataset: {airbnb_df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23b5a15-dbec-4004-8c99-c636e17f7a47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports & Setup"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nPRICE SANITY CHECK: FILTER UNREALISTIC LOW PRICES\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, expr, lit, count as spark_count\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRICE SANITY CHECK: FILTER UNREALISTIC LOW PRICES\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6fa584d-f443-466b-9e9e-a2cc2babe985",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast Price to Numeric and Analyze Distribution"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1. Analyzing price distribution...\n   Total rows: 2,098,880\n   Rows with valid numeric price: 1,479,472\n   Null prices: 619,408\n\n   Price percentiles:\n     1st  : $7.00\n     5th  : $28.62\n     10th : $41.85\n     25th : $75.57\n     50th : $136.00\n     75th : $253.00\n     90th : $544.00\n     95th : $1,499.00\n     99th : $11,684.17\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Analyzing price distribution...\")\n",
    "\n",
    "# Cast price to double\n",
    "airbnb_df = airbnb_df.withColumn(\"price_numeric\", col(\"price\").cast(DoubleType()))\n",
    "\n",
    "# Get overall statistics (distributed computation)\n",
    "total_rows = airbnb_df.count()\n",
    "valid_price_rows = airbnb_df.filter(col(\"price_numeric\").isNotNull()).count()\n",
    "\n",
    "print(f\"   Total rows: {total_rows:,}\")\n",
    "print(f\"   Rows with valid numeric price: {valid_price_rows:,}\")\n",
    "print(f\"   Null prices: {total_rows - valid_price_rows:,}\")\n",
    "\n",
    "# Compute price percentiles (distributed using approx_percentile)\n",
    "price_percentiles = airbnb_df.filter(col(\"price_numeric\").isNotNull()).select(\n",
    "    expr(\"approx_percentile(price_numeric, array(0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\").alias(\"percentiles\")\n",
    ").collect()[0]['percentiles']\n",
    "\n",
    "print(f\"\\n   Price percentiles:\")\n",
    "percentile_labels = ['1st', '5th', '10th', '25th', '50th', '75th', '90th', '95th', '99th']\n",
    "for label, value in zip(percentile_labels, price_percentiles):\n",
    "    print(f\"     {label:5s}: ${value:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad2075a-2123-4e6e-83e1-d31bfdb7e05f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify Unrealistic Prices"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Identifying unrealistic prices...\n\n   Hard minimum threshold: $10.00\n   Adaptive threshold (1st percentile): $7.00\n   Final minimum realistic price: $10.00\n\n   Unrealistic prices found: 15,540 (1.05% of valid prices)\n\n   Sample of unrealistic prices:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>property_id</th><th>listing_name</th><th>location</th><th>price</th><th>price_numeric</th></tr></thead><tbody><tr><td>29964171</td><td>Entire home in Conyers, Georgia, United States</td><td>Conyers, Georgia, United States</td><td>0</td><td>0.0</td></tr><tr><td>null</td><td>null</td><td>Aeroport</td><td>0</td><td>0.0</td></tr><tr><td>1214111481846115863</td><td>Entire rental unit in Niagara Falls, Canada</td><td>Niagara Falls, Ontario, Canada</td><td>0.1</td><td>0.1</td></tr><tr><td>675630790188195632</td><td>Entire home in Fresno, California, United States</td><td>Fresno, California, United States</td><td>1</td><td>1.0</td></tr><tr><td>1129144755688307243</td><td>Entire guest suite in Moorpark, California, United States</td><td>Moorpark, California, United States</td><td>1</td><td>1.0</td></tr><tr><td>41955923</td><td>Entire rental unit in Long Beach, California, United States</td><td>Long Beach, California, United States</td><td>1</td><td>1.0</td></tr><tr><td>897254946843342773</td><td>Entire home in Indio, California, United States</td><td>Indio, California, United States</td><td>1</td><td>1.0</td></tr><tr><td>1135463422565962887</td><td>Entire rental unit in Duluth, Minnesota, United States</td><td>Duluth, Minnesota, United States</td><td>1</td><td>1.0</td></tr><tr><td>1063539358530843821</td><td>Entire rental unit in Santa Ana, California, United States</td><td>Santa Ana, California, United States</td><td>1</td><td>1.0</td></tr><tr><td>44408051</td><td>Entire rental unit in Hawthorne, California, United States</td><td>Hawthorne, California, United States</td><td>1</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "29964171",
         "Entire home in Conyers, Georgia, United States",
         "Conyers, Georgia, United States",
         "0",
         0.0
        ],
        [
         null,
         null,
         "Aeroport",
         "0",
         0.0
        ],
        [
         "1214111481846115863",
         "Entire rental unit in Niagara Falls, Canada",
         "Niagara Falls, Ontario, Canada",
         "0.1",
         0.1
        ],
        [
         "675630790188195632",
         "Entire home in Fresno, California, United States",
         "Fresno, California, United States",
         "1",
         1.0
        ],
        [
         "1129144755688307243",
         "Entire guest suite in Moorpark, California, United States",
         "Moorpark, California, United States",
         "1",
         1.0
        ],
        [
         "41955923",
         "Entire rental unit in Long Beach, California, United States",
         "Long Beach, California, United States",
         "1",
         1.0
        ],
        [
         "897254946843342773",
         "Entire home in Indio, California, United States",
         "Indio, California, United States",
         "1",
         1.0
        ],
        [
         "1135463422565962887",
         "Entire rental unit in Duluth, Minnesota, United States",
         "Duluth, Minnesota, United States",
         "1",
         1.0
        ],
        [
         "1063539358530843821",
         "Entire rental unit in Santa Ana, California, United States",
         "Santa Ana, California, United States",
         "1",
         1.0
        ],
        [
         "44408051",
         "Entire rental unit in Hawthorne, California, United States",
         "Hawthorne, California, United States",
         "1",
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "property_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "listing_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price_numeric",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n2. Identifying unrealistic prices...\")\n",
    "\n",
    "# Strategy: Use multiple criteria\n",
    "# Criterion 1: Hard minimum threshold (e.g., $10 - no realistic nightly rental is < $10)\n",
    "hard_min_threshold = 10.0\n",
    "\n",
    "# Criterion 2: Adaptive threshold - 1st percentile (catches extreme outliers)\n",
    "adaptive_threshold = price_percentiles[0]  # 1st percentile\n",
    "\n",
    "# Use the more conservative (higher) of the two\n",
    "min_realistic_price = max(hard_min_threshold, adaptive_threshold)\n",
    "\n",
    "print(f\"\\n   Hard minimum threshold: ${hard_min_threshold:.2f}\")\n",
    "print(f\"   Adaptive threshold (1st percentile): ${adaptive_threshold:.2f}\")\n",
    "print(f\"   Final minimum realistic price: ${min_realistic_price:.2f}\")\n",
    "\n",
    "# Count unrealistic prices\n",
    "unrealistic_count = airbnb_df.filter(\n",
    "    (col(\"price_numeric\").isNotNull()) & \n",
    "    (col(\"price_numeric\") < min_realistic_price)\n",
    ").count()\n",
    "\n",
    "print(f\"\\n   Unrealistic prices found: {unrealistic_count:,} ({unrealistic_count/valid_price_rows*100:.2f}% of valid prices)\")\n",
    "\n",
    "# Show examples of unrealistic prices\n",
    "print(\"\\n   Sample of unrealistic prices:\")\n",
    "unrealistic_sample = airbnb_df.filter(\n",
    "    (col(\"price_numeric\").isNotNull()) & \n",
    "    (col(\"price_numeric\") < min_realistic_price)\n",
    ").select(\n",
    "    \"property_id\", \"listing_name\", \"location\", \"price\", \"price_numeric\"\n",
    ").orderBy(\"price_numeric\").limit(10)\n",
    "\n",
    "display(unrealistic_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64df341c-fb73-443c-a8ba-4d2232f6a033",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add Price Sanity Flag"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. Adding price sanity flag...\n   ✓ Price sanity flag added\n\n   Price sanity flag distribution:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>price_sanity_flag</th><th>count</th></tr></thead><tbody><tr><td>valid</td><td>1463932</td></tr><tr><td>null_price</td><td>619408</td></tr><tr><td>unrealistic_low</td><td>15540</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "valid",
         1463932
        ],
        [
         "null_price",
         619408
        ],
        [
         "unrealistic_low",
         15540
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "price_sanity_flag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n3. Adding price sanity flag...\")\n",
    "\n",
    "airbnb_df = airbnb_df.withColumn(\n",
    "    \"price_sanity_flag\",\n",
    "    when(\n",
    "        col(\"price_numeric\").isNull(),\n",
    "        \"null_price\"\n",
    "    ).when(\n",
    "        col(\"price_numeric\") < min_realistic_price,\n",
    "        \"unrealistic_low\"\n",
    "    ).when(\n",
    "        col(\"price_numeric\") >= min_realistic_price,\n",
    "        \"valid\"\n",
    "    ).otherwise(\"unknown\")\n",
    ")\n",
    "\n",
    "print(\"   ✓ Price sanity flag added\")\n",
    "\n",
    "# Show flag distribution\n",
    "print(\"\\n   Price sanity flag distribution:\")\n",
    "flag_dist = airbnb_df.groupBy(\"price_sanity_flag\").agg(\n",
    "    spark_count(\"*\").alias(\"count\")\n",
    ").orderBy(col(\"count\").desc())\n",
    "\n",
    "display(flag_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ece15c-79c7-4bf6-8157-87ad9f0d5543",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter to Realistic Prices Only"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n4. Filtering to realistic prices only...\n   Rows after filtering: 1,463,932\n   Rows removed: 634,948 (30.25%)\n   ✓ Price column updated to numeric type\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Filtering to realistic prices only...\")\n",
    "\n",
    "# Create filtered dataset\n",
    "airbnb_df_filtered = airbnb_df.filter(\n",
    "    col(\"price_sanity_flag\") == \"valid\"\n",
    ")\n",
    "\n",
    "filtered_count = airbnb_df_filtered.count()\n",
    "removed_count = total_rows - filtered_count\n",
    "\n",
    "print(f\"   Rows after filtering: {filtered_count:,}\")\n",
    "print(f\"   Rows removed: {removed_count:,} ({removed_count/total_rows*100:.2f}%)\")\n",
    "\n",
    "# Replace price with price_numeric for consistency\n",
    "airbnb_df_filtered = airbnb_df_filtered.withColumn(\"price\", col(\"price_numeric\"))\n",
    "\n",
    "print(\"   ✓ Price column updated to numeric type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b86d726-8ee0-4e2f-9ca6-d4b19b7164fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Filtered Price Distribution"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n5. Verifying filtered price distribution...\n\n   Filtered price percentiles:\n     1st  : $17.48\n     5th  : $31.01\n     10th : $44.00\n     25th : $78.00\n     50th : $137.79\n     75th : $255.15\n     90th : $550.00\n     95th : $1,513.00\n     99th : $11,833.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Verifying filtered price distribution...\")\n",
    "\n",
    "# Recompute percentiles on filtered data\n",
    "filtered_percentiles = airbnb_df_filtered.select(\n",
    "    expr(\"approx_percentile(price, array(0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\").alias(\"percentiles\")\n",
    ").collect()[0]['percentiles']\n",
    "\n",
    "print(f\"\\n   Filtered price percentiles:\")\n",
    "for label, value in zip(percentile_labels, filtered_percentiles):\n",
    "    print(f\"     {label:5s}: ${value:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4488a0b3-3b6a-4d11-aea8-4e4c67e38b8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Summary and Display"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nPRICE FILTERING COMPLETE\n================================================================================\n\nFiltered dataset: airbnb_df_filtered\n  Rows: 1,463,932\n  Minimum price: $10.00\n  Removed: 634,948 unrealistic prices\n\nPrice range after filtering:\n  Min: $17.48\n  Median: $137.79\n  Max: $11833.00\n\n✓ Dataset ready for downstream anomaly detection!\n\nNext steps:\n  - Use 'airbnb_df_filtered' instead of 'airbnb_df' in subsequent cells\n  - All unrealistic prices have been removed\n  - Price column is now numeric (DoubleType)\n\nSample of filtered data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>property_id</th><th>listing_name</th><th>location</th><th>price</th><th>price_sanity_flag</th></tr></thead><tbody><tr><td>40458495</td><td>Entire rental unit in Broadbeach, Australia</td><td>Broadbeach, Australia</td><td>238.0</td><td>valid</td></tr><tr><td>17988678</td><td>Cycladic home in Aliki, Greece</td><td>Aliki, Greece</td><td>15118.0</td><td>valid</td></tr><tr><td>17857973</td><td>Entire home in Guía, Spain</td><td>Guía, Canarias, Spain</td><td>174.0</td><td>valid</td></tr><tr><td>53932806</td><td>Private room in guesthouse in Rovereto, Italy</td><td>Rovereto, Trentino-Alto Adige, Italy</td><td>2367.0</td><td>valid</td></tr><tr><td>18535615</td><td>Entire rental unit in Peniscola, Spain</td><td>Peniscola, Comunidad Valenciana, Spain</td><td>116.0</td><td>valid</td></tr><tr><td>23049623</td><td>Entire rental unit in Sallanches, France</td><td>Sallanches, Auvergne-Rhône-Alpes, France</td><td>237.0</td><td>valid</td></tr><tr><td>14160725</td><td>Entire home in Chania, Greece</td><td>Chania, Greece</td><td>375.0</td><td>valid</td></tr><tr><td>18058823</td><td>Casa particular in Cienfuegos, Cuba</td><td>Cienfuegos, Cuba</td><td>20.0</td><td>valid</td></tr><tr><td>53816734</td><td>Room in boutique hotel in Isla Mujeres, Mexico</td><td>Isla Mujeres, Quintana Roo, Mexico</td><td>139.0</td><td>valid</td></tr><tr><td>13850728</td><td>Entire home in Lostanges, France</td><td>Lostanges, Aquitaine Limousin Poitou-Charentes, France</td><td>139.0</td><td>valid</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "40458495",
         "Entire rental unit in Broadbeach, Australia",
         "Broadbeach, Australia",
         238.0,
         "valid"
        ],
        [
         "17988678",
         "Cycladic home in Aliki, Greece",
         "Aliki, Greece",
         15118.0,
         "valid"
        ],
        [
         "17857973",
         "Entire home in Guía, Spain",
         "Guía, Canarias, Spain",
         174.0,
         "valid"
        ],
        [
         "53932806",
         "Private room in guesthouse in Rovereto, Italy",
         "Rovereto, Trentino-Alto Adige, Italy",
         2367.0,
         "valid"
        ],
        [
         "18535615",
         "Entire rental unit in Peniscola, Spain",
         "Peniscola, Comunidad Valenciana, Spain",
         116.0,
         "valid"
        ],
        [
         "23049623",
         "Entire rental unit in Sallanches, France",
         "Sallanches, Auvergne-Rhône-Alpes, France",
         237.0,
         "valid"
        ],
        [
         "14160725",
         "Entire home in Chania, Greece",
         "Chania, Greece",
         375.0,
         "valid"
        ],
        [
         "18058823",
         "Casa particular in Cienfuegos, Cuba",
         "Cienfuegos, Cuba",
         20.0,
         "valid"
        ],
        [
         "53816734",
         "Room in boutique hotel in Isla Mujeres, Mexico",
         "Isla Mujeres, Quintana Roo, Mexico",
         139.0,
         "valid"
        ],
        [
         "13850728",
         "Entire home in Lostanges, France",
         "Lostanges, Aquitaine Limousin Poitou-Charentes, France",
         139.0,
         "valid"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "property_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "listing_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "price_sanity_flag",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRICE FILTERING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFiltered dataset: airbnb_df_filtered\")\n",
    "print(f\"  Rows: {filtered_count:,}\")\n",
    "print(f\"  Minimum price: ${min_realistic_price:.2f}\")\n",
    "print(f\"  Removed: {removed_count:,} unrealistic prices\")\n",
    "\n",
    "print(f\"\\nPrice range after filtering:\")\n",
    "print(f\"  Min: ${filtered_percentiles[0]:.2f}\")\n",
    "print(f\"  Median: ${filtered_percentiles[4]:.2f}\")\n",
    "print(f\"  Max: ${filtered_percentiles[-1]:.2f}\")\n",
    "\n",
    "print(\"\\n✓ Dataset ready for downstream anomaly detection!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  - Use 'airbnb_df_filtered' instead of 'airbnb_df' in subsequent cells\")\n",
    "print(\"  - All unrealistic prices have been removed\")\n",
    "print(\"  - Price column is now numeric (DoubleType)\")\n",
    "\n",
    "# Show sample of filtered data\n",
    "print(\"\\nSample of filtered data:\")\n",
    "display(airbnb_df_filtered.select(\n",
    "    \"property_id\", \"listing_name\", \"location\", \"price\", \"price_sanity_flag\"\n",
    ").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25b623d-5091-4695-973e-2c0725de0d83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter USA Listings"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ USA listings: 453,841 rows (31.0% of total)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, trim, lower\n",
    "\n",
    "# Filter for United States listings - USE FILTERED DATA\n",
    "usa_df = airbnb_df_filtered.filter(col(\"location\").contains(\"United States\"))\n",
    "\n",
    "print(f\"✓ USA listings: {usa_df.count():,} rows ({usa_df.count()/airbnb_df_filtered.count()*100:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34710bd5-ea97-426a-a88b-7d328f112d2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parse Location into City, State, Country"
    }
   },
   "outputs": [],
   "source": [
    "# Parse location into city, state, country\n",
    "# Format: \"City, State, Country\"\n",
    "usa_df = usa_df.withColumn(\"location_split\", split(col(\"location\"), \", \"))\n",
    "\n",
    "usa_df = usa_df.withColumn(\"city\", trim(col(\"location_split\")[0])) \\\n",
    "               .withColumn(\"state\", trim(col(\"location_split\")[1])) \\\n",
    "               .withColumn(\"country_parsed\", trim(col(\"location_split\")[2])) \\\n",
    "               .drop(\"location_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259c35b4-6af9-4d86-934d-861e97938421",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter USA Listings and Normalize Location"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ USA listings: 453,841 rows (31.0% of total)\n✓ Location parsed and normalized to lowercase\nEnriching USA data with environmental indicators...\n✓ Environmental data enriched: 730,949 rows\n  Matched records: 153,680 (21.0%)\n  Unmatched records: 577,269 (79.0%)\n\n1. Filling missing crime data...\n   Minimum crime rate: 249.31\n   ✓ Filled 0 null crime values with minimum rate\n   ✓ Crime data now complete for all 730,949 listings\n"
     ]
    }
   ],
   "source": [
    "# DBTITLE 1,Filter USA Listings and Normalize Location\n",
    "from pyspark.sql.functions import col, split, trim, lower\n",
    "\n",
    "# Filter for United States listings\n",
    "usa_df = airbnb_df_filtered.filter(col(\"location\").contains(\"United States\"))\n",
    "\n",
    "print(f\"✓ USA listings: {usa_df.count():,} rows ({usa_df.count()/airbnb_df_filtered.count()*100:.1f}% of total)\")\n",
    "\n",
    "# Parse location into city, state, country (with lowercase normalization)\n",
    "usa_df = usa_df.withColumn(\"location_split\", split(col(\"location\"), \", \"))\n",
    "\n",
    "usa_df = usa_df.withColumn(\"city\", lower(trim(col(\"location_split\")[0]))) \\\n",
    "               .withColumn(\"state\", lower(trim(col(\"location_split\")[1]))) \\\n",
    "               .withColumn(\"country_parsed\", trim(col(\"location_split\")[2])) \\\n",
    "               .drop(\"location_split\")\n",
    "\n",
    "print(\"✓ Location parsed and normalized to lowercase\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Join USA Data with Environmental Data\n",
    "print(\"Enriching USA data with environmental indicators...\")\n",
    "\n",
    "# Prepare environmental data\n",
    "env_df_prepared = mega_df.select(\n",
    "    lower(col(\"City\")).alias(\"city\"),\n",
    "    lower(col(\"State\")).alias(\"state\"),\n",
    "    col(\"County\").alias(\"county\"),\n",
    "    col(\"City_Crime_Rate_Per_100K\"),\n",
    "    col(\"Total_Fatalities\"),\n",
    "    col(\"Median_Income\"),\n",
    "    col(\"Disability_Rate\"),\n",
    "    col(\"Median_AQI\")\n",
    ")\n",
    "\n",
    "# Join on city and state\n",
    "airbnb_enriched_df = usa_df.join(\n",
    "    env_df_prepared,\n",
    "    on=[\"city\", \"state\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Environmental data enriched: {airbnb_enriched_df.count():,} rows\")\n",
    "\n",
    "# Verify the join worked\n",
    "matches = airbnb_enriched_df.filter(\n",
    "    col(\"City_Crime_Rate_Per_100K\").isNotNull()\n",
    ").count()\n",
    "total = airbnb_enriched_df.count()\n",
    "match_rate = (matches / total * 100) if total > 0 else 0\n",
    "print(f\"  Matched records: {matches:,} ({match_rate:.1f}%)\")\n",
    "print(f\"  Unmatched records: {total - matches:,} ({100-match_rate:.1f}%)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Fill Null Crime Values (NOW SHOULD WORK)\n",
    "from pyspark.sql.functions import col, when, lit, count\n",
    "\n",
    "print(\"\\n1. Filling missing crime data...\")\n",
    "\n",
    "# Check if there are any non-null crime values at all\n",
    "crime_non_null_count = airbnb_enriched_df.filter(\n",
    "    col(\"City_Crime_Rate_Per_100K\").isNotNull()\n",
    ").count()\n",
    "\n",
    "if crime_non_null_count == 0:\n",
    "    print(\"   ⚠️  WARNING: No crime data found! Join may have failed.\")\n",
    "    print(\"   Debug info:\")\n",
    "    print(f\"     Total rows: {airbnb_enriched_df.count():,}\")\n",
    "    print(\"\\n   Sample of airbnb_enriched_df:\")\n",
    "    airbnb_enriched_df.select(\"city\", \"state\", \"City_Crime_Rate_Per_100K\").show(10)\n",
    "    raise Exception(\"No crime data after join. Check join logic.\")\n",
    "else:\n",
    "    # Calculate minimum non-null crime rate\n",
    "    min_crime_rate = airbnb_enriched_df.filter(\n",
    "        col(\"City_Crime_Rate_Per_100K\").isNotNull()\n",
    "    ).agg({\"City_Crime_Rate_Per_100K\": \"min\"}).collect()[0][0]\n",
    "    \n",
    "    if min_crime_rate is not None:\n",
    "        print(f\"   Minimum crime rate: {min_crime_rate:.2f}\")\n",
    "    else:\n",
    "        raise Exception(\"Cannot compute minimum crime rate\")\n",
    "    \n",
    "    # Fill null crime values\n",
    "    airbnb_enriched_df = airbnb_enriched_df.withColumn(\n",
    "        \"City_Crime_Rate_Per_100K\",\n",
    "        when(col(\"City_Crime_Rate_Per_100K\").isNull(), lit(min_crime_rate))\n",
    "        .otherwise(col(\"City_Crime_Rate_Per_100K\"))\n",
    "    )\n",
    "    \n",
    "    null_count_before = airbnb_enriched_df.filter(col('City_Crime_Rate_Per_100K').isNull()).count()\n",
    "    print(f\"   ✓ Filled {null_count_before:,} null crime values with minimum rate\")\n",
    "    print(f\"   ✓ Crime data now complete for all {airbnb_enriched_df.count():,} listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6c4e48-7600-4553-9903-06315204a569",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nCREATING RISK INDICATORS\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, lit, percentile_approx\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING RISK INDICATORS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f8ce94-a9bc-4594-928f-c10669ce2c8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate Risk Thresholds"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Calculating risk thresholds (top 10%)...\n   Crime threshold (90th percentile): 539.85\n   Fatalities threshold (90th percentile): 89\n   AQI threshold (10th percentile - worst): 26\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Calculate thresholds for top 10% (90th percentile)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. Calculating risk thresholds (top 10%)...\")\n",
    "\n",
    "# Crime threshold (90th percentile of non-null values)\n",
    "crime_threshold = airbnb_enriched_df.filter(col(\"City_Crime_Rate_Per_100K\").isNotNull()) \\\n",
    "    .approxQuantile(\"City_Crime_Rate_Per_100K\", [0.9], 0.01)[0]\n",
    "\n",
    "print(f\"   Crime threshold (90th percentile): {crime_threshold:.2f}\")\n",
    "\n",
    "# Fatalities threshold (90th percentile, excluding nulls and zeros)\n",
    "fatalities_threshold = airbnb_enriched_df.filter(\n",
    "    (col(\"Total_Fatalities\").isNotNull()) & (col(\"Total_Fatalities\") > 0)\n",
    ").approxQuantile(\"Total_Fatalities\", [0.9], 0.01)[0]\n",
    "\n",
    "print(f\"   Fatalities threshold (90th percentile): {fatalities_threshold:.0f}\")\n",
    "\n",
    "# AQI threshold (10th percentile = worst air quality, excluding nulls)\n",
    "# Lower AQI is better, so 10th percentile means bottom 10% (worst)\n",
    "aqi_threshold = airbnb_enriched_df.filter(col(\"Median_AQI\").isNotNull()) \\\n",
    "    .approxQuantile(\"Median_AQI\", [0.1], 0.01)[0]\n",
    "\n",
    "print(f\"   AQI threshold (10th percentile - worst): {aqi_threshold:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97de2184-1fb4-4d86-b10e-18261938dca1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Risk Indicators"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. Creating high-risk indicator...\n   ✓ Risk indicators created\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Create binary risk indicator column\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3. Creating high-risk indicator...\")\n",
    "\n",
    "# Create individual risk flags\n",
    "airbnb_with_risk = airbnb_enriched_df.withColumn(\n",
    "    \"is_high_crime\",\n",
    "    when(\n",
    "        col(\"City_Crime_Rate_Per_100K\").isNotNull() & \n",
    "        (col(\"City_Crime_Rate_Per_100K\") >= crime_threshold),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_high_fatalities\",\n",
    "    when(\n",
    "        col(\"Total_Fatalities\").isNotNull() & \n",
    "        (col(\"Total_Fatalities\") > 0) &\n",
    "        (col(\"Total_Fatalities\") >= fatalities_threshold),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_poor_aqi\",\n",
    "    when(\n",
    "        col(\"Median_AQI\").isNotNull() & \n",
    "        (col(\"Median_AQI\") <= aqi_threshold),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Create combined high-risk indicator (1 if ANY of the risk factors is present)\n",
    "airbnb_final = airbnb_with_risk.withColumn(\n",
    "    \"is_high_risk_area\",\n",
    "    when(\n",
    "        (col(\"is_high_crime\") == 1) | \n",
    "        (col(\"is_high_fatalities\") == 1) | \n",
    "        (col(\"is_poor_aqi\") == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"   ✓ Risk indicators created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d3d35b-f1fb-4a91-b1e5-c1038075096f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Statistics"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nRISK INDICATOR STATISTICS\n================================================================================\n\nTotal listings: 730,949\n\nIndividual Risk Factors:\n  High crime areas: 82,231 (11.2%)\n  High fatalities areas: 29,857 (4.1%)\n  Poor air quality areas: 41,170 (5.6%)\n\nCombined High-Risk Areas: 132,016 (18.1%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: Display statistics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RISK INDICATOR STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_count = airbnb_final.count()\n",
    "high_crime_count = airbnb_final.filter(col(\"is_high_crime\") == 1).count()\n",
    "high_fatalities_count = airbnb_final.filter(col(\"is_high_fatalities\") == 1).count()\n",
    "poor_aqi_count = airbnb_final.filter(col(\"is_poor_aqi\") == 1).count()\n",
    "high_risk_count = airbnb_final.filter(col(\"is_high_risk_area\") == 1).count()\n",
    "\n",
    "print(f\"\\nTotal listings: {total_count:,}\")\n",
    "print(f\"\\nIndividual Risk Factors:\")\n",
    "print(f\"  High crime areas: {high_crime_count:,} ({high_crime_count/total_count*100:.1f}%)\")\n",
    "print(f\"  High fatalities areas: {high_fatalities_count:,} ({high_fatalities_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Poor air quality areas: {poor_aqi_count:,} ({poor_aqi_count/total_count*100:.1f}%)\")\n",
    "print(f\"\\nCombined High-Risk Areas: {high_risk_count:,} ({high_risk_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668c8e3d-e3ba-4ed7-8120-da998724793a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Sample Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSample of data with risk indicators:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>state</th><th>listing_name</th><th>price</th><th>ratings</th><th>City_Crime_Rate_Per_100K</th><th>Total_Fatalities</th><th>Median_AQI</th><th>is_high_crime</th><th>is_high_fatalities</th><th>is_poor_aqi</th><th>is_high_risk_area</th></tr></thead><tbody><tr><td>benton</td><td>tennessee</td><td>Dome in Benton, Tennessee, United States</td><td>128.0</td><td>5</td><td>249.31</td><td>null</td><td>42.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>carrabassett valley</td><td>maine</td><td>Entire condo in Carrabassett Valley, Maine, United States</td><td>225.0</td><td>4.96</td><td>249.31</td><td>1</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>bonita springs</td><td>florida</td><td>Entire rental unit in Bonita Springs, Florida, United States</td><td>88.0</td><td>4.91</td><td>249.31</td><td>4</td><td>42.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>bonita springs</td><td>florida</td><td>Entire rental unit in Bonita Springs, Florida, United States</td><td>88.0</td><td>4.91</td><td>249.31</td><td>null</td><td>37.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>gordonville</td><td>pennsylvania</td><td>Entire home in Gordonville, Pennsylvania, United States</td><td>255.0</td><td>4.89</td><td>249.31</td><td>null</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>kittery</td><td>maine</td><td>Entire rental unit in Kittery, Maine, United States</td><td>225.0</td><td>5</td><td>249.31</td><td>1</td><td>36.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>innsbrook</td><td>missouri</td><td>Entire chalet in Innsbrook, Missouri, United States</td><td>264.0</td><td>4.8</td><td>249.31</td><td>null</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>eden</td><td>utah</td><td>Entire rental unit in Eden, Utah, United States</td><td>110.0</td><td>4.83</td><td>249.31</td><td>1</td><td>47.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>auburn</td><td>alabama</td><td>Room in Auburn, Alabama, United States</td><td>605.36</td><td>4.64</td><td>249.31</td><td>8</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>auburn</td><td>alabama</td><td>Room in Auburn, Alabama, United States</td><td>605.36</td><td>4.64</td><td>249.31</td><td>null</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>cosby</td><td>tennessee</td><td>Entire guest house in Cosby, Tennessee, United States</td><td>134.0</td><td>5</td><td>249.31</td><td>1</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>cosby</td><td>tennessee</td><td>Entire guest house in Cosby, Tennessee, United States</td><td>134.0</td><td>5</td><td>249.31</td><td>null</td><td>42.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>bonita springs</td><td>florida</td><td>Entire home in Bonita Springs, Florida, United States</td><td>467.0</td><td>4.91</td><td>249.31</td><td>4</td><td>42.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>bonita springs</td><td>florida</td><td>Entire home in Bonita Springs, Florida, United States</td><td>467.0</td><td>4.91</td><td>249.31</td><td>null</td><td>37.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>bonita springs</td><td>florida</td><td>Entire condo in Bonita Springs, Florida, United States</td><td>375.0</td><td>4.94</td><td>249.31</td><td>4</td><td>42.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>bonita springs</td><td>florida</td><td>Entire condo in Bonita Springs, Florida, United States</td><td>375.0</td><td>4.94</td><td>249.31</td><td>null</td><td>37.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>kittery</td><td>maine</td><td>Entire cottage in Kittery, Maine, United States</td><td>154.0</td><td>4.93</td><td>249.31</td><td>1</td><td>36.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>eden</td><td>utah</td><td>Entire condo in Eden, Utah, United States</td><td>314.0</td><td>4.85</td><td>249.31</td><td>1</td><td>47.0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>auburn</td><td>alabama</td><td>Entire home in Auburn, Alabama, United States</td><td>390.0</td><td>5</td><td>249.31</td><td>8</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>auburn</td><td>alabama</td><td>Entire home in Auburn, Alabama, United States</td><td>390.0</td><td>5</td><td>249.31</td><td>null</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "benton",
         "tennessee",
         "Dome in Benton, Tennessee, United States",
         128.0,
         "5",
         249.31,
         null,
         42.0,
         0,
         0,
         0,
         0
        ],
        [
         "carrabassett valley",
         "maine",
         "Entire condo in Carrabassett Valley, Maine, United States",
         225.0,
         "4.96",
         249.31,
         1,
         null,
         0,
         0,
         0,
         0
        ],
        [
         "bonita springs",
         "florida",
         "Entire rental unit in Bonita Springs, Florida, United States",
         88.0,
         "4.91",
         249.31,
         4,
         42.0,
         0,
         0,
         0,
         0
        ],
        [
         "bonita springs",
         "florida",
         "Entire rental unit in Bonita Springs, Florida, United States",
         88.0,
         "4.91",
         249.31,
         null,
         37.0,
         0,
         0,
         0,
         0
        ],
        [
         "gordonville",
         "pennsylvania",
         "Entire home in Gordonville, Pennsylvania, United States",
         255.0,
         "4.89",
         249.31,
         null,
         null,
         0,
         0,
         0,
         0
        ],
        [
         "kittery",
         "maine",
         "Entire rental unit in Kittery, Maine, United States",
         225.0,
         "5",
         249.31,
         1,
         36.0,
         0,
         0,
         0,
         0
        ],
        [
         "innsbrook",
         "missouri",
         "Entire chalet in Innsbrook, Missouri, United States",
         264.0,
         "4.8",
         249.31,
         null,
         null,
         0,
         0,
         0,
         0
        ],
        [
         "eden",
         "utah",
         "Entire rental unit in Eden, Utah, United States",
         110.0,
         "4.83",
         249.31,
         1,
         47.0,
         0,
         0,
         0,
         0
        ],
        [
         "auburn",
         "alabama",
         "Room in Auburn, Alabama, United States",
         605.36,
         "4.64",
         249.31,
         8,
         null,
         0,
         0,
         0,
         0
        ],
        [
         "auburn",
         "alabama",
         "Room in Auburn, Alabama, United States",
         605.36,
         "4.64",
         249.31,
         null,
         null,
         0,
         0,
         0,
         0
        ],
        [
         "cosby",
         "tennessee",
         "Entire guest house in Cosby, Tennessee, United States",
         134.0,
         "5",
         249.31,
         1,
         null,
         0,
         0,
         0,
         0
        ],
        [
         "cosby",
         "tennessee",
         "Entire guest house in Cosby, Tennessee, United States",
         134.0,
         "5",
         249.31,
         null,
         42.0,
         0,
         0,
         0,
         0
        ],
        [
         "bonita springs",
         "florida",
         "Entire home in Bonita Springs, Florida, United States",
         467.0,
         "4.91",
         249.31,
         4,
         42.0,
         0,
         0,
         0,
         0
        ],
        [
         "bonita springs",
         "florida",
         "Entire home in Bonita Springs, Florida, United States",
         467.0,
         "4.91",
         249.31,
         null,
         37.0,
         0,
         0,
         0,
         0
        ],
        [
         "bonita springs",
         "florida",
         "Entire condo in Bonita Springs, Florida, United States",
         375.0,
         "4.94",
         249.31,
         4,
         42.0,
         0,
         0,
         0,
         0
        ],
        [
         "bonita springs",
         "florida",
         "Entire condo in Bonita Springs, Florida, United States",
         375.0,
         "4.94",
         249.31,
         null,
         37.0,
         0,
         0,
         0,
         0
        ],
        [
         "kittery",
         "maine",
         "Entire cottage in Kittery, Maine, United States",
         154.0,
         "4.93",
         249.31,
         1,
         36.0,
         0,
         0,
         0,
         0
        ],
        [
         "eden",
         "utah",
         "Entire condo in Eden, Utah, United States",
         314.0,
         "4.85",
         249.31,
         1,
         47.0,
         0,
         0,
         0,
         0
        ],
        [
         "auburn",
         "alabama",
         "Entire home in Auburn, Alabama, United States",
         390.0,
         "5",
         249.31,
         8,
         null,
         0,
         0,
         0,
         0
        ],
        [
         "auburn",
         "alabama",
         "Entire home in Auburn, Alabama, United States",
         390.0,
         "5",
         249.31,
         null,
         null,
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "listing_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ratings",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City_Crime_Rate_Per_100K",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Total_Fatalities",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Median_AQI",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_high_crime",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_high_fatalities",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_poor_aqi",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_high_risk_area",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSample of HIGH-RISK listings:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>state</th><th>listing_name</th><th>price</th><th>ratings</th><th>City_Crime_Rate_Per_100K</th><th>Total_Fatalities</th><th>Median_AQI</th><th>is_high_crime</th><th>is_high_fatalities</th><th>is_poor_aqi</th></tr></thead><tbody><tr><td>winslow</td><td>arizona</td><td>Entire home in Winslow, Arizona, United States</td><td>95.0</td><td>5</td><td>249.31</td><td>5</td><td>12.0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>winslow</td><td>arizona</td><td>Entire home in Winslow, Arizona, United States</td><td>95.0</td><td>5</td><td>249.31</td><td>null</td><td>14.0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>holbrook</td><td>arizona</td><td>Campsite in Holbrook, Arizona, United States</td><td>13.0</td><td>4.72</td><td>249.31</td><td>6</td><td>12.0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>holbrook</td><td>arizona</td><td>Campsite in Holbrook, Arizona, United States</td><td>13.0</td><td>4.72</td><td>249.31</td><td>null</td><td>14.0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>baker city</td><td>oregon</td><td>Entire home in Baker City, Oregon, United States</td><td>150.0</td><td>4.97</td><td>249.31</td><td>null</td><td>25.0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>butte</td><td>montana</td><td>Room in Butte, Montana, United States</td><td>732.0</td><td>0</td><td>249.31</td><td>null</td><td>12.0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>baker city</td><td>oregon</td><td>Entire cottage in Baker City, Oregon, United States</td><td>92.0</td><td>4.74</td><td>249.31</td><td>null</td><td>25.0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>houston</td><td>texas</td><td>Entire home in Houston, Texas, United States</td><td>6233.0</td><td>5</td><td>1148.17</td><td>296</td><td>63.0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Entire home in Houston, Texas, United States</td><td>6233.0</td><td>5</td><td>1148.17</td><td>null</td><td>null</td><td>1</td><td>0</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Entire home in Houston, Texas</td><td>200.0</td><td>4.88</td><td>1148.17</td><td>296</td><td>63.0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Entire home in Houston, Texas</td><td>200.0</td><td>4.88</td><td>1148.17</td><td>null</td><td>null</td><td>1</td><td>0</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Room in Houston, Texas, United States</td><td>1096.24</td><td>5</td><td>1148.17</td><td>296</td><td>63.0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Room in Houston, Texas, United States</td><td>1096.24</td><td>5</td><td>1148.17</td><td>null</td><td>null</td><td>1</td><td>0</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Private room in home in Houston, Texas, United States</td><td>164.0</td><td>4.85</td><td>1148.17</td><td>296</td><td>63.0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Private room in home in Houston, Texas, United States</td><td>164.0</td><td>4.85</td><td>1148.17</td><td>null</td><td>null</td><td>1</td><td>0</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Entire home in Houston, Texas, United States</td><td>134.0</td><td>4.88</td><td>1148.17</td><td>296</td><td>63.0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Entire home in Houston, Texas, United States</td><td>134.0</td><td>4.88</td><td>1148.17</td><td>null</td><td>null</td><td>1</td><td>0</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Entire rental unit in Houston, Texas, United States</td><td>110.0</td><td>4.83</td><td>1148.17</td><td>296</td><td>63.0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Entire rental unit in Houston, Texas, United States</td><td>110.0</td><td>4.83</td><td>1148.17</td><td>null</td><td>null</td><td>1</td><td>0</td><td>0</td></tr><tr><td>houston</td><td>texas</td><td>Room in Houston, Texas, United States</td><td>45.0</td><td>5</td><td>1148.17</td><td>296</td><td>63.0</td><td>1</td><td>1</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "winslow",
         "arizona",
         "Entire home in Winslow, Arizona, United States",
         95.0,
         "5",
         249.31,
         5,
         12.0,
         0,
         0,
         1
        ],
        [
         "winslow",
         "arizona",
         "Entire home in Winslow, Arizona, United States",
         95.0,
         "5",
         249.31,
         null,
         14.0,
         0,
         0,
         1
        ],
        [
         "holbrook",
         "arizona",
         "Campsite in Holbrook, Arizona, United States",
         13.0,
         "4.72",
         249.31,
         6,
         12.0,
         0,
         0,
         1
        ],
        [
         "holbrook",
         "arizona",
         "Campsite in Holbrook, Arizona, United States",
         13.0,
         "4.72",
         249.31,
         null,
         14.0,
         0,
         0,
         1
        ],
        [
         "baker city",
         "oregon",
         "Entire home in Baker City, Oregon, United States",
         150.0,
         "4.97",
         249.31,
         null,
         25.0,
         0,
         0,
         1
        ],
        [
         "butte",
         "montana",
         "Room in Butte, Montana, United States",
         732.0,
         "0",
         249.31,
         null,
         12.0,
         0,
         0,
         1
        ],
        [
         "baker city",
         "oregon",
         "Entire cottage in Baker City, Oregon, United States",
         92.0,
         "4.74",
         249.31,
         null,
         25.0,
         0,
         0,
         1
        ],
        [
         "houston",
         "texas",
         "Entire home in Houston, Texas, United States",
         6233.0,
         "5",
         1148.17,
         296,
         63.0,
         1,
         1,
         0
        ],
        [
         "houston",
         "texas",
         "Entire home in Houston, Texas, United States",
         6233.0,
         "5",
         1148.17,
         null,
         null,
         1,
         0,
         0
        ],
        [
         "houston",
         "texas",
         "Entire home in Houston, Texas",
         200.0,
         "4.88",
         1148.17,
         296,
         63.0,
         1,
         1,
         0
        ],
        [
         "houston",
         "texas",
         "Entire home in Houston, Texas",
         200.0,
         "4.88",
         1148.17,
         null,
         null,
         1,
         0,
         0
        ],
        [
         "houston",
         "texas",
         "Room in Houston, Texas, United States",
         1096.24,
         "5",
         1148.17,
         296,
         63.0,
         1,
         1,
         0
        ],
        [
         "houston",
         "texas",
         "Room in Houston, Texas, United States",
         1096.24,
         "5",
         1148.17,
         null,
         null,
         1,
         0,
         0
        ],
        [
         "houston",
         "texas",
         "Private room in home in Houston, Texas, United States",
         164.0,
         "4.85",
         1148.17,
         296,
         63.0,
         1,
         1,
         0
        ],
        [
         "houston",
         "texas",
         "Private room in home in Houston, Texas, United States",
         164.0,
         "4.85",
         1148.17,
         null,
         null,
         1,
         0,
         0
        ],
        [
         "houston",
         "texas",
         "Entire home in Houston, Texas, United States",
         134.0,
         "4.88",
         1148.17,
         296,
         63.0,
         1,
         1,
         0
        ],
        [
         "houston",
         "texas",
         "Entire home in Houston, Texas, United States",
         134.0,
         "4.88",
         1148.17,
         null,
         null,
         1,
         0,
         0
        ],
        [
         "houston",
         "texas",
         "Entire rental unit in Houston, Texas, United States",
         110.0,
         "4.83",
         1148.17,
         296,
         63.0,
         1,
         1,
         0
        ],
        [
         "houston",
         "texas",
         "Entire rental unit in Houston, Texas, United States",
         110.0,
         "4.83",
         1148.17,
         null,
         null,
         1,
         0,
         0
        ],
        [
         "houston",
         "texas",
         "Room in Houston, Texas, United States",
         45.0,
         "5",
         1148.17,
         296,
         63.0,
         1,
         1,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "listing_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ratings",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City_Crime_Rate_Per_100K",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Total_Fatalities",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Median_AQI",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_high_crime",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_high_fatalities",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_poor_aqi",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✓ Risk analysis complete!\n\nFinal dataset: airbnb_final (730,949 rows, 65 columns)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: Show sample data with risk indicators\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nSample of data with risk indicators:\")\n",
    "display(\n",
    "    airbnb_final.select(\n",
    "        \"city\", \"state\", \"listing_name\", \"price\", \"ratings\",\n",
    "        \"City_Crime_Rate_Per_100K\",\n",
    "        \"Total_Fatalities\", \"Median_AQI\",\n",
    "        \"is_high_crime\", \"is_high_fatalities\", \"is_poor_aqi\", \"is_high_risk_area\"\n",
    "    ).limit(20)\n",
    ")\n",
    "\n",
    "# Show high-risk listings\n",
    "print(\"\\nSample of HIGH-RISK listings:\")\n",
    "display(\n",
    "    airbnb_final.filter(col(\"is_high_risk_area\") == 1)\n",
    "    .select(\n",
    "        \"city\", \"state\", \"listing_name\", \"price\", \"ratings\",\n",
    "        \"City_Crime_Rate_Per_100K\", \"Total_Fatalities\", \"Median_AQI\",\n",
    "        \"is_high_crime\", \"is_high_fatalities\", \"is_poor_aqi\"\n",
    "    )\n",
    "    .limit(20)\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Risk analysis complete!\")\n",
    "print(f\"\\nFinal dataset: airbnb_final ({airbnb_final.count():,} rows, {len(airbnb_final.columns)} columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c44e08-f079-4ce3-b55f-520855ec16a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports and Setup"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nCOMPREHENSIVE ANALYSIS: USA ENRICHED AIRBNB DATASET\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, isnull, mean, stddev, min as spark_min, max as spark_max, countDistinct\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS: USA ENRICHED AIRBNB DATASET\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06bba80d-437b-4eac-8dfc-c32f618196f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic Dataset Information"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n1. BASIC DATASET INFORMATION\n================================================================================\n\nDataset: airbnb_enriched_df\n  Total rows: 730,949\n  Total columns: 61\n\nColumn names:\n   1. city\n   2. state\n   3. name\n   4. price\n   5. image\n   6. description\n   7. category\n   8. availability\n   9. discount\n  10. reviews\n  11. ratings\n  12. seller_info\n  13. breadcrumbs\n  14. location\n  15. lat\n  16. long\n  17. guests\n  18. pets_allowed\n  19. description_items\n  20. category_rating\n  21. house_rules\n  22. details\n  23. highlights\n  24. arrangement_details\n  25. amenities\n  26. images\n  27. available_dates\n  28. url\n  29. final_url\n  30. listing_title\n  31. property_id\n  32. listing_name\n  33. location_details\n  34. description_by_sections\n  35. description_html\n  36. location_details_html\n  37. is_supperhost\n  38. host_number_of_reviews\n  39. host_rating\n  40. hosts_year\n  41. host_response_rate\n  42. is_guest_favorite\n  43. travel_details\n  44. pricing_details\n  45. total_price\n  46. currency\n  47. cancellation_policy\n  48. property_number_of_reviews\n  49. country\n  50. postcode_map_url\n  51. host_image\n  52. host_details\n  53. price_numeric\n  54. price_sanity_flag\n  55. country_parsed\n  56. county\n  57. City_Crime_Rate_Per_100K\n  58. Total_Fatalities\n  59. Median_Income\n  60. Disability_Rate\n  61. Median_AQI\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. BASIC DATASET INFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. BASIC DATASET INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_rows = airbnb_enriched_df.count()\n",
    "total_cols = len(airbnb_enriched_df.columns)\n",
    "\n",
    "print(f\"\\nDataset: airbnb_enriched_df\")\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  Total columns: {total_cols}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col_name in enumerate(airbnb_enriched_df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "035e1e2b-c84f-4428-89c8-d3424e1ecd3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Completeness Analysis"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n2. DATA COMPLETENESS ANALYSIS\n================================================================================\n\nNull/Missing value counts per column:\n\nColumn                              Non-Null     Null         Null %    \n----------------------------------------------------------------------\ncity                                730,949      0            0.0       %\nstate                               730,805      144          0.0       %\nname                                730,949      0            0.0       %\nprice                               730,949      0            0.0       %\nimage                               730,949      0            0.0       %\ndescription                         729,846      1,103        0.2       %\ncategory                            730,949      0            0.0       %\navailability                        730,947      2            0.0       %\ndiscount                            0            730,949      100.0     %\nreviews                             680,117      50,832       7.0       %\nratings                             730,948      1            0.0       %\nseller_info                         730,949      0            0.0       %\nbreadcrumbs                         730,949      0            0.0       %\nlocation                            730,949      0            0.0       %\nlat                                 730,931      18           0.0       %\nlong                                730,931      18           0.0       %\nguests                              730,949      0            0.0       %\npets_allowed                        699,998      30,951       4.2       %\ndescription_items                   730,949      0            0.0       %\ncategory_rating                     730,949      0            0.0       %\nhouse_rules                         728,480      2,469        0.3       %\ndetails                             730,949      0            0.0       %\nhighlights                          730,949      0            0.0       %\narrangement_details                 571,422      159,527      21.8      %\namenities                           730,949      0            0.0       %\nimages                              728,340      2,609        0.4       %\navailable_dates                     730,949      0            0.0       %\nurl                                 730,949      0            0.0       %\nfinal_url                           730,949      0            0.0       %\nlisting_title                       730,949      0            0.0       %\nproperty_id                         730,949      0            0.0       %\nlisting_name                        730,949      0            0.0       %\nlocation_details                    730,949      0            0.0       %\ndescription_by_sections             730,949      0            0.0       %\ndescription_html                    729,846      1,103        0.2       %\nlocation_details_html               437,655      293,294      40.1      %\nis_supperhost                       728,340      2,609        0.4       %\nhost_number_of_reviews              728,340      2,609        0.4       %\nhost_rating                         728,340      2,609        0.4       %\nhosts_year                          707,079      23,870       3.3       %\nhost_response_rate                  678,916      52,033       7.1       %\nis_guest_favorite                   730,949      0            0.0       %\ntravel_details                      730,949      0            0.0       %\npricing_details                     730,949      0            0.0       %\ntotal_price                         716,326      14,623       2.0       %\ncurrency                            730,949      0            0.0       %\ncancellation_policy                 719,839      11,110       1.5       %\nproperty_number_of_reviews          401,289      329,660      45.1      %\ncountry                             0            730,949      100.0     %\npostcode_map_url                    0            730,949      100.0     %\nhost_image                          172,586      558,363      76.4      %\nhost_details                        0            730,949      100.0     %\nprice_numeric                       730,949      0            0.0       %\nprice_sanity_flag                   730,949      0            0.0       %\ncountry_parsed                      727,508      3,441        0.5       %\ncounty                              635,938      95,011       13.0      %\nCity_Crime_Rate_Per_100K            730,949      0            0.0       %\nTotal_Fatalities                    273,329      457,620      62.6      %\nMedian_Income                       442,580      288,369      39.5      %\nDisability_Rate                     605,770      125,179      17.1      %\nMedian_AQI                          423,063      307,886      42.1      %\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. DATA COMPLETENESS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. DATA COMPLETENESS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nNull/Missing value counts per column:\")\n",
    "print(f\"\\n{'Column':<35} {'Non-Null':<12} {'Null':<12} {'Null %':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for col_name in airbnb_enriched_df.columns:\n",
    "    null_count = airbnb_enriched_df.filter(col(col_name).isNull()).count()\n",
    "    non_null_count = total_rows - null_count\n",
    "    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "    print(f\"{col_name:<35} {non_null_count:<12,} {null_count:<12,} {null_pct:<10.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0929e9b-2388-412e-88f4-9ebd7f5d2dbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numeric Column Statistics"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n3. NUMERIC COLUMN STATISTICS\n================================================================================\n\nSummary statistics for numeric columns:\n\nColumn                              Count      Mean         Std          Min          Max         \n-----------------------------------------------------------------------------------------------\nprice                               730,949    715.42       3896.74      10.00        751966.00   \nratings                             730,948    4.48         1.26         0.00         5.00        \nproperty_number_of_reviews          401,289    64.84        94.89        0.00         3439.00     \ntotal_price                         716,326    1527.52      5563.37      9.00         1234959.00  \nlat                                 730,931    35.74        6.51         -90.00       71.29       \nlong                                730,931    -94.84       19.18        -176.64      155.22      \nCity_Crime_Rate_Per_100K            730,949    333.16       217.44       249.31       2501.28     \nTotal_Fatalities                    273,329    31.47        60.30        1.00         329.00      \nMedian_Income                       442,580    85565.31     60829.50     0.00         300000.00   \nDisability_Rate                     605,770    30.86        4.60         19.30        48.50       \nMedian_AQI                          423,063    41.78        12.21        3.00         90.00       \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. NUMERIC COLUMN STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. NUMERIC COLUMN STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = [\n",
    "    'price', 'ratings', 'property_number_of_reviews', 'total_price',\n",
    "    'lat', 'long', 'City_Crime_Rate_Per_100K', 'Total_Fatalities',\n",
    "    'Median_Income', 'Disability_Rate', 'Median_AQI'\n",
    "]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "numeric_cols = [c for c in numeric_cols if c in airbnb_enriched_df.columns]\n",
    "\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "print(f\"\\n{'Column':<35} {'Count':<10} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    # Cast to double for statistics\n",
    "    stats = airbnb_enriched_df.select(\n",
    "        count(when(col(col_name).isNotNull(), 1)).alias('count'),\n",
    "        mean(col(col_name).cast('double')).alias('mean'),\n",
    "        stddev(col(col_name).cast('double')).alias('std'),\n",
    "        spark_min(col(col_name).cast('double')).alias('min'),\n",
    "        spark_max(col(col_name).cast('double')).alias('max')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"{col_name:<35} {stats['count']:<10,} {stats['mean']:<12.2f} {stats['std']:<12.2f} {stats['min']:<12.2f} {stats['max']:<12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60cf782-bcc3-48c2-9897-b236354c9f9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Categorical Column Distributions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n4. CATEGORICAL COLUMN DISTRIBUTIONS\n================================================================================\n\nTop 20 cities by listing count:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>count</th></tr></thead><tbody><tr><td>kissimmee</td><td>20589</td></tr><tr><td>austin</td><td>8669</td></tr><tr><td>davenport</td><td>7843</td></tr><tr><td>miami</td><td>6666</td></tr><tr><td>dallas</td><td>6578</td></tr><tr><td>san francisco</td><td>6522</td></tr><tr><td>panama city beach</td><td>6212</td></tr><tr><td>las vegas</td><td>6200</td></tr><tr><td>atlanta</td><td>6004</td></tr><tr><td>houston</td><td>5769</td></tr><tr><td>kihei</td><td>5655</td></tr><tr><td>orlando</td><td>5557</td></tr><tr><td>chicago</td><td>5485</td></tr><tr><td>los angeles</td><td>5475</td></tr><tr><td>nashville</td><td>5348</td></tr><tr><td>seattle</td><td>5154</td></tr><tr><td>myrtle beach</td><td>4918</td></tr><tr><td>destin</td><td>4761</td></tr><tr><td>san diego</td><td>4620</td></tr><tr><td>denver</td><td>4214</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "kissimmee",
         20589
        ],
        [
         "austin",
         8669
        ],
        [
         "davenport",
         7843
        ],
        [
         "miami",
         6666
        ],
        [
         "dallas",
         6578
        ],
        [
         "san francisco",
         6522
        ],
        [
         "panama city beach",
         6212
        ],
        [
         "las vegas",
         6200
        ],
        [
         "atlanta",
         6004
        ],
        [
         "houston",
         5769
        ],
        [
         "kihei",
         5655
        ],
        [
         "orlando",
         5557
        ],
        [
         "chicago",
         5485
        ],
        [
         "los angeles",
         5475
        ],
        [
         "nashville",
         5348
        ],
        [
         "seattle",
         5154
        ],
        [
         "myrtle beach",
         4918
        ],
        [
         "destin",
         4761
        ],
        [
         "san diego",
         4620
        ],
        [
         "denver",
         4214
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTop 20 states by listing count:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>state</th><th>count</th></tr></thead><tbody><tr><td>florida</td><td>132498</td></tr><tr><td>california</td><td>75625</td></tr><tr><td>texas</td><td>61167</td></tr><tr><td>new york</td><td>32985</td></tr><tr><td>colorado</td><td>30462</td></tr><tr><td>north carolina</td><td>26801</td></tr><tr><td>arizona</td><td>23215</td></tr><tr><td>georgia</td><td>22283</td></tr><tr><td>washington</td><td>21269</td></tr><tr><td>tennessee</td><td>21159</td></tr><tr><td>hawaii</td><td>17888</td></tr><tr><td>south carolina</td><td>16360</td></tr><tr><td>michigan</td><td>13035</td></tr><tr><td>pennsylvania</td><td>12894</td></tr><tr><td>virginia</td><td>12490</td></tr><tr><td>ohio</td><td>11521</td></tr><tr><td>utah</td><td>11285</td></tr><tr><td>missouri</td><td>11233</td></tr><tr><td>new jersey</td><td>10929</td></tr><tr><td>oregon</td><td>10909</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "florida",
         132498
        ],
        [
         "california",
         75625
        ],
        [
         "texas",
         61167
        ],
        [
         "new york",
         32985
        ],
        [
         "colorado",
         30462
        ],
        [
         "north carolina",
         26801
        ],
        [
         "arizona",
         23215
        ],
        [
         "georgia",
         22283
        ],
        [
         "washington",
         21269
        ],
        [
         "tennessee",
         21159
        ],
        [
         "hawaii",
         17888
        ],
        [
         "south carolina",
         16360
        ],
        [
         "michigan",
         13035
        ],
        [
         "pennsylvania",
         12894
        ],
        [
         "virginia",
         12490
        ],
        [
         "ohio",
         11521
        ],
        [
         "utah",
         11285
        ],
        [
         "missouri",
         11233
        ],
        [
         "new jersey",
         10929
        ],
        [
         "oregon",
         10909
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSuperhost distribution:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>is_supperhost</th><th>count</th></tr></thead><tbody><tr><td>null</td><td>2609</td></tr><tr><td>false</td><td>327098</td></tr><tr><td>true</td><td>401242</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         2609
        ],
        [
         "false",
         327098
        ],
        [
         "true",
         401242
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "is_supperhost",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nGuest favorite distribution:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>is_guest_favorite</th><th>count</th></tr></thead><tbody><tr><td>false</td><td>435785</td></tr><tr><td>true</td><td>295164</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "false",
         435785
        ],
        [
         "true",
         295164
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "is_guest_favorite",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. CATEGORICAL COLUMN DISTRIBUTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. CATEGORICAL COLUMN DISTRIBUTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# City distribution\n",
    "print(\"\\nTop 20 cities by listing count:\")\n",
    "city_dist = airbnb_enriched_df.groupBy('city').count().orderBy(col('count').desc()).limit(20)\n",
    "display(city_dist)\n",
    "\n",
    "# State distribution\n",
    "print(\"\\nTop 20 states by listing count:\")\n",
    "state_dist = airbnb_enriched_df.groupBy('state').count().orderBy(col('count').desc()).limit(20)\n",
    "display(state_dist)\n",
    "\n",
    "# Superhost distribution\n",
    "print(\"\\nSuperhost distribution:\")\n",
    "superhost_dist = airbnb_enriched_df.groupBy('is_supperhost').count().orderBy('is_supperhost')\n",
    "display(superhost_dist)\n",
    "\n",
    "# Guest favorite distribution\n",
    "print(\"\\nGuest favorite distribution:\")\n",
    "guest_fav_dist = airbnb_enriched_df.groupBy('is_guest_favorite').count().orderBy('is_guest_favorite')\n",
    "display(guest_fav_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12de45e1-4e4b-4da2-8276-37c96c029908",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Price Analysis"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n5. PRICE ANALYSIS\n================================================================================\n\nPrice distribution:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>price</th></tr></thead><tbody><tr><td>count</td><td>730949</td></tr><tr><td>mean</td><td>715.4150576442402</td></tr><tr><td>stddev</td><td>3896.7367493797647</td></tr><tr><td>min</td><td>10.0</td></tr><tr><td>25%</td><td>122.0</td></tr><tr><td>50%</td><td>197.0</td></tr><tr><td>75%</td><td>350.0</td></tr><tr><td>max</td><td>751966.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "730949"
        ],
        [
         "mean",
         "715.4150576442402"
        ],
        [
         "stddev",
         "3896.7367493797647"
        ],
        [
         "min",
         "10.0"
        ],
        [
         "25%",
         "122.0"
        ],
        [
         "50%",
         "197.0"
        ],
        [
         "75%",
         "350.0"
        ],
        [
         "max",
         "751966.0"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAverage price by state (top 10 most expensive):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>state</th><th>avg_price</th><th>listing_count</th></tr></thead><tbody><tr><td>los osos</td><td>9317.0</td><td>1</td></tr><tr><td>clearwater</td><td>7837.0</td><td>1</td></tr><tr><td>queens</td><td>5968.0</td><td>1</td></tr><tr><td>united states</td><td>4905.724221212121</td><td>3300</td></tr><tr><td>los angeles</td><td>3388.75</td><td>4</td></tr><tr><td>az</td><td>2751.5</td><td>2</td></tr><tr><td>la</td><td>2000.0</td><td>1</td></tr><tr><td>henderson</td><td>1651.26</td><td>2</td></tr><tr><td>seacrest,</td><td>1648.0</td><td>1</td></tr><tr><td>auberge resorts collection</td><td>1646.0</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "los osos",
         9317.0,
         1
        ],
        [
         "clearwater",
         7837.0,
         1
        ],
        [
         "queens",
         5968.0,
         1
        ],
        [
         "united states",
         4905.724221212121,
         3300
        ],
        [
         "los angeles",
         3388.75,
         4
        ],
        [
         "az",
         2751.5,
         2
        ],
        [
         "la",
         2000.0,
         1
        ],
        [
         "henderson",
         1651.26,
         2
        ],
        [
         "seacrest,",
         1648.0,
         1
        ],
        [
         "auberge resorts collection",
         1646.0,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "listing_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. PRICE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. PRICE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Price statistics\n",
    "price_stats = airbnb_enriched_df.filter(col('price').isNotNull()).select(\n",
    "    col('price').cast('double')\n",
    ").summary('count', 'mean', 'stddev', 'min', '25%', '50%', '75%', 'max')\n",
    "\n",
    "print(\"\\nPrice distribution:\")\n",
    "display(price_stats)\n",
    "\n",
    "# Price by state (top 10)\n",
    "print(\"\\nAverage price by state (top 10 most expensive):\")\n",
    "price_by_state = airbnb_enriched_df.filter(col('price').isNotNull()).groupBy('state').agg(\n",
    "    mean(col('price').cast('double')).alias('avg_price'),\n",
    "    count('*').alias('listing_count')\n",
    ").orderBy(col('avg_price').desc()).limit(10)\n",
    "display(price_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71d5991-9932-4077-8d60-9ac6a99479b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Environmental Data Analysis"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n6. ENVIRONMENTAL DATA ANALYSIS\n================================================================================\n\nCrime rate statistics (listings with data):\n  Listings with crime data: 730,949\n  Mean crime rate: 333.16 per 100K\n  Std deviation: 217.44\n  Min: 249.31\n  Max: 2501.28\n\nMedian income statistics (listings with data):\n  Listings with income data: 442,580\n  Mean median income: $85,565.31\n  Std deviation: $60,829.50\n  Min: $0.00\n  Max: $300,000.00\n\nAir Quality Index (AQI) statistics (listings with data):\n  Listings with AQI data: 423,063\n  Mean AQI: 41.78\n  Std deviation: 12.21\n  Min: 3.00\n  Max: 90.00\n\nTraffic fatalities statistics (listings with data):\n  Listings with fatalities data: 273,329\n  Mean fatalities: 31.47\n  Std deviation: 60.30\n  Min: 1\n  Max: 329\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. ENVIRONMENTAL DATA ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. ENVIRONMENTAL DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crime rate analysis\n",
    "print(\"\\nCrime rate statistics (listings with data):\")\n",
    "crime_with_data = airbnb_enriched_df.filter(col('City_Crime_Rate_Per_100K').isNotNull())\n",
    "print(f\"  Listings with crime data: {crime_with_data.count():,}\")\n",
    "\n",
    "crime_stats = crime_with_data.select(\n",
    "    mean('City_Crime_Rate_Per_100K').alias('mean'),\n",
    "    stddev('City_Crime_Rate_Per_100K').alias('std'),\n",
    "    spark_min('City_Crime_Rate_Per_100K').alias('min'),\n",
    "    spark_max('City_Crime_Rate_Per_100K').alias('max')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Mean crime rate: {crime_stats['mean']:.2f} per 100K\")\n",
    "print(f\"  Std deviation: {crime_stats['std']:.2f}\")\n",
    "print(f\"  Min: {crime_stats['min']:.2f}\")\n",
    "print(f\"  Max: {crime_stats['max']:.2f}\")\n",
    "\n",
    "# Income analysis\n",
    "print(\"\\nMedian income statistics (listings with data):\")\n",
    "income_with_data = airbnb_enriched_df.filter(col('Median_Income').isNotNull())\n",
    "print(f\"  Listings with income data: {income_with_data.count():,}\")\n",
    "\n",
    "income_stats = income_with_data.select(\n",
    "    mean('Median_Income').alias('mean'),\n",
    "    stddev('Median_Income').alias('std'),\n",
    "    spark_min('Median_Income').alias('min'),\n",
    "    spark_max('Median_Income').alias('max')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Mean median income: ${income_stats['mean']:,.2f}\")\n",
    "print(f\"  Std deviation: ${income_stats['std']:,.2f}\")\n",
    "print(f\"  Min: ${income_stats['min']:,.2f}\")\n",
    "print(f\"  Max: ${income_stats['max']:,.2f}\")\n",
    "\n",
    "# AQI analysis\n",
    "print(\"\\nAir Quality Index (AQI) statistics (listings with data):\")\n",
    "aqi_with_data = airbnb_enriched_df.filter(col('Median_AQI').isNotNull())\n",
    "print(f\"  Listings with AQI data: {aqi_with_data.count():,}\")\n",
    "\n",
    "aqi_stats = aqi_with_data.select(\n",
    "    mean('Median_AQI').alias('mean'),\n",
    "    stddev('Median_AQI').alias('std'),\n",
    "    spark_min('Median_AQI').alias('min'),\n",
    "    spark_max('Median_AQI').alias('max')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Mean AQI: {aqi_stats['mean']:.2f}\")\n",
    "print(f\"  Std deviation: {aqi_stats['std']:.2f}\")\n",
    "print(f\"  Min: {aqi_stats['min']:.2f}\")\n",
    "print(f\"  Max: {aqi_stats['max']:.2f}\")\n",
    "\n",
    "# Fatalities analysis\n",
    "print(\"\\nTraffic fatalities statistics (listings with data):\")\n",
    "fatalities_with_data = airbnb_enriched_df.filter(col('Total_Fatalities').isNotNull())\n",
    "print(f\"  Listings with fatalities data: {fatalities_with_data.count():,}\")\n",
    "\n",
    "fatalities_stats = fatalities_with_data.select(\n",
    "    mean('Total_Fatalities').alias('mean'),\n",
    "    stddev('Total_Fatalities').alias('std'),\n",
    "    spark_min('Total_Fatalities').alias('min'),\n",
    "    spark_max('Total_Fatalities').alias('max')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Mean fatalities: {fatalities_stats['mean']:.2f}\")\n",
    "print(f\"  Std deviation: {fatalities_stats['std']:.2f}\")\n",
    "print(f\"  Min: {fatalities_stats['min']:.0f}\")\n",
    "print(f\"  Max: {fatalities_stats['max']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208e2a91-c6b7-46cb-a0c0-f853159c2740",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Geographic Coverage & Final Summary"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n7. GEOGRAPHIC COVERAGE\n================================================================================\n\nUnique locations:\n  Unique cities: 12,695\n  Unique states: 184\n\nData completeness:\n  Listings with ALL environmental data: 70,487 (9.6%)\n\n================================================================================\nANALYSIS COMPLETE\n================================================================================\n\nKey Insights:\n  • Dataset contains 730,949 USA Airbnb listings across 184 states\n  • 21.0% of listings have environmental data (crime, income, AQI, fatalities)\n  • Average price: $715.42\n  • 730,949 listings have crime rate data\n  • Geographic coverage spans 12,695 unique cities\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. GEOGRAPHIC COVERAGE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. GEOGRAPHIC COVERAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_unique_cities = airbnb_enriched_df.select(countDistinct('city')).collect()[0][0]\n",
    "num_unique_states = airbnb_enriched_df.select(countDistinct('state')).collect()[0][0]\n",
    "\n",
    "print(f\"\\nUnique locations:\")\n",
    "print(f\"  Unique cities: {num_unique_cities:,}\")\n",
    "print(f\"  Unique states: {num_unique_states:,}\")\n",
    "\n",
    "# Listings with complete environmental data\n",
    "complete_env_data = airbnb_enriched_df.filter(\n",
    "    col('City_Crime_Rate_Per_100K').isNotNull() &\n",
    "    col('Median_Income').isNotNull() &\n",
    "    col('Median_AQI').isNotNull() &\n",
    "    col('Total_Fatalities').isNotNull()\n",
    ").count()\n",
    "\n",
    "print(f\"\\nData completeness:\")\n",
    "print(f\"  Listings with ALL environmental data: {complete_env_data:,} ({complete_env_data/total_rows*100:.1f}%)\")\n",
    "# print(f\"  Listings with ANY environmental data: {matched_listings:,} ({match_rate:.1f}%)\")\n",
    "# print(f\"  Listings with NO environmental data: {total_rows - matched_listings:,} ({100-match_rate:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  • Dataset contains {total_rows:,} USA Airbnb listings across {num_unique_states} states\")\n",
    "print(f\"  • {match_rate:.1f}% of listings have environmental data (crime, income, AQI, fatalities)\")\n",
    "print(f\"  • Average price: ${float(price_stats.collect()[1][1]):.2f}\")\n",
    "print(f\"  • {crime_with_data.count():,} listings have crime rate data\")\n",
    "print(f\"  • Geographic coverage spans {num_unique_cities:,} unique cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e79a31-2fd6-4bd6-b3a6-24a9d3402282",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports & Setup"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSCALABLE PRICE ANOMALY DETECTION - OPTION A: BUCKETED LOCAL QUANTILES\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, log as spark_log, when, size, split, length, lit, \n",
    "    regexp_replace, lower, trim, expr\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SCALABLE PRICE ANOMALY DETECTION - OPTION A: BUCKETED LOCAL QUANTILES\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491469d4-5bcd-4616-aa9f-0348e399dd6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter Valid Prices"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1. Filtering for valid prices...\n   Original rows: 730,949\n   Valid price rows: 730,949 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Filtering for valid prices...\")\n",
    "\n",
    "# Filter for non-null, positive prices\n",
    "df_valid = airbnb_enriched_df.filter(\n",
    "    (col(\"price\").isNotNull()) & \n",
    "    (col(\"price\").cast(DoubleType()) > 0)\n",
    ")\n",
    "\n",
    "original_count = airbnb_enriched_df.count()\n",
    "valid_count = df_valid.count()\n",
    "\n",
    "print(f\"   Original rows: {original_count:,}\")\n",
    "print(f\"   Valid price rows: {valid_count:,} ({valid_count/original_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2141ba1-1825-4870-a7e4-50193cc32905",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Log Price"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Creating target variable: log(price)...\n   ✓ log_price created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Creating target variable: log(price)...\")\n",
    "\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"log_price\",\n",
    "    spark_log(col(\"price\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "print(\"   ✓ log_price created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701fad54-076a-45c2-b17a-4d43a3fe3f67",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Features"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. Preparing features...\n   ✓ Features prepared\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. Preparing features...\")\n",
    "\n",
    "# Cast coordinates to double\n",
    "df_valid = df_valid.withColumn(\"lat\", col(\"lat\").cast(DoubleType()))\n",
    "df_valid = df_valid.withColumn(\"long\", col(\"long\").cast(DoubleType()))\n",
    "\n",
    "# Create amenities_count\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"amenities_count\",\n",
    "    when(\n",
    "        col(\"amenities\").isNotNull(),\n",
    "        size(split(col(\"amenities\"), \",\"))\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Extract room_type from listing_name\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"room_type\",\n",
    "    when(col(\"listing_name\").contains(\"Entire\"), \"entire\")\n",
    "    .when(col(\"listing_name\").contains(\"Private room\"), \"private\")\n",
    "    .when(col(\"listing_name\").contains(\"Shared room\"), \"shared\")\n",
    "    .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "# Extract property_type from listing_name (simplified buckets)\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"property_type\",\n",
    "    when(col(\"listing_name\").contains(\"home\"), \"home\")\n",
    "    .when(col(\"listing_name\").contains(\"apartment\") | col(\"listing_name\").contains(\"rental unit\"), \"apartment\")\n",
    "    .when(col(\"listing_name\").contains(\"condo\"), \"condo\")\n",
    "    .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "# Cast and fill reputation features\n",
    "df_valid = df_valid.withColumn(\"ratings\", col(\"ratings\").cast(DoubleType()))\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"property_number_of_reviews\",\n",
    "    when(col(\"property_number_of_reviews\").isNull(), 0)\n",
    "    .otherwise(col(\"property_number_of_reviews\").cast(IntegerType()))\n",
    ")\n",
    "df_valid = df_valid.withColumn(\"is_supperhost\", col(\"is_supperhost\").cast(IntegerType()))\n",
    "\n",
    "df_valid = df_valid.fillna({\n",
    "    'ratings': 0.0,\n",
    "    'property_number_of_reviews': 0,\n",
    "    'is_supperhost': 0\n",
    "})\n",
    "\n",
    "print(\"   ✓ Features prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa642c3-3326-43a3-8881-911acc7da38c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Similarity Buckets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n4. Creating similarity buckets...\n   ✓ Geographic buckets created (precision ~1km)\n   ✓ Amenities buckets created (0-5, 6-10, 11-20, 20+)\n   ✓ Composite similarity bucket created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Creating similarity buckets...\")\n",
    "\n",
    "# Geohash bucket (precision 6 for ~1.2km x 0.6km cells)\n",
    "# Using a simple lat/long bucketing approach since geohash function may not be available\n",
    "# Bucket lat/long into ~0.01 degree bins (roughly 1km)\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"lat_bucket\",\n",
    "    (col(\"lat\") * 100).cast(IntegerType())\n",
    ")\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"long_bucket\",\n",
    "    (col(\"long\") * 100).cast(IntegerType())\n",
    ")\n",
    "\n",
    "# Combine into geohash-like bucket\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"geo_bucket\",\n",
    "    expr(\"concat(lat_bucket, '_', long_bucket)\")\n",
    ")\n",
    "\n",
    "print(\"   ✓ Geographic buckets created (precision ~1km)\")\n",
    "\n",
    "# Amenities bucket (0-5, 6-10, 11-20, 20+)\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"amenities_bucket\",\n",
    "    when(col(\"amenities_count\") <= 5, \"0-5\")\n",
    "    .when(col(\"amenities_count\") <= 10, \"6-10\")\n",
    "    .when(col(\"amenities_count\") <= 20, \"11-20\")\n",
    "    .otherwise(\"20+\")\n",
    ")\n",
    "\n",
    "print(\"   ✓ Amenities buckets created (0-5, 6-10, 11-20, 20+)\")\n",
    "\n",
    "# Create composite similarity bucket\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"similarity_bucket\",\n",
    "    expr(\"concat(geo_bucket, '|', room_type, '|', property_type, '|', amenities_bucket)\")\n",
    ")\n",
    "\n",
    "print(\"   ✓ Composite similarity bucket created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773c9d87-b2e4-41e1-aeff-92de77cf1127",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Summary and Display"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nDATA PREPARATION COMPLETE\n================================================================================\n\nDataset: df_valid\n  Rows: 730,949\n\nSimilarity bucketing:\n  - Geographic: lat/long bucketed to ~1km cells\n  - Room type: entire, private, shared, other\n  - Property type: home, apartment, condo, other\n  - Amenities: 0-5, 6-10, 11-20, 20+\n\nTotal unique similarity buckets: 189,121\nAverage listings per bucket: 3.9\n\nSample data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>property_id</th><th>city</th><th>state</th><th>price</th><th>log_price</th><th>geo_bucket</th><th>room_type</th><th>property_type</th><th>amenities_bucket</th><th>similarity_bucket</th></tr></thead><tbody><tr><td>902367758953614183</td><td>benton</td><td>tennessee</td><td>128.0</td><td>4.852030263919617</td><td>3517_-8463</td><td>other</td><td>other</td><td>20+</td><td>3517_-8463|other|other|20+</td></tr><tr><td>48672316</td><td>carrabassett valley</td><td>maine</td><td>225.0</td><td>5.41610040220442</td><td>4505_-7031</td><td>entire</td><td>condo</td><td>20+</td><td>4505_-7031|entire|condo|20+</td></tr><tr><td>643423129076681645</td><td>bonita springs</td><td>florida</td><td>88.0</td><td>4.477336814478207</td><td>2633_-8180</td><td>entire</td><td>apartment</td><td>20+</td><td>2633_-8180|entire|apartment|20+</td></tr><tr><td>643423129076681645</td><td>bonita springs</td><td>florida</td><td>88.0</td><td>4.477336814478207</td><td>2633_-8180</td><td>entire</td><td>apartment</td><td>20+</td><td>2633_-8180|entire|apartment|20+</td></tr><tr><td>20800206</td><td>gordonville</td><td>pennsylvania</td><td>255.0</td><td>5.541263545158426</td><td>4003_-7610</td><td>entire</td><td>home</td><td>20+</td><td>4003_-7610|entire|home|20+</td></tr><tr><td>867572983915994824</td><td>kittery</td><td>maine</td><td>225.0</td><td>5.41610040220442</td><td>4308_-7074</td><td>entire</td><td>apartment</td><td>20+</td><td>4308_-7074|entire|apartment|20+</td></tr><tr><td>875581428033264239</td><td>innsbrook</td><td>missouri</td><td>264.0</td><td>5.575949103146316</td><td>3877_-9103</td><td>entire</td><td>other</td><td>20+</td><td>3877_-9103|entire|other|20+</td></tr><tr><td>791404048769330143</td><td>eden</td><td>utah</td><td>110.0</td><td>4.700480365792417</td><td>4132_-11182</td><td>entire</td><td>apartment</td><td>20+</td><td>4132_-11182|entire|apartment|20+</td></tr><tr><td>42024153</td><td>auburn</td><td>alabama</td><td>605.36</td><td>6.405823322386266</td><td>3264_-8551</td><td>other</td><td>other</td><td>20+</td><td>3264_-8551|other|other|20+</td></tr><tr><td>42024153</td><td>auburn</td><td>alabama</td><td>605.36</td><td>6.405823322386266</td><td>3264_-8551</td><td>other</td><td>other</td><td>20+</td><td>3264_-8551|other|other|20+</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "902367758953614183",
         "benton",
         "tennessee",
         128.0,
         4.852030263919617,
         "3517_-8463",
         "other",
         "other",
         "20+",
         "3517_-8463|other|other|20+"
        ],
        [
         "48672316",
         "carrabassett valley",
         "maine",
         225.0,
         5.41610040220442,
         "4505_-7031",
         "entire",
         "condo",
         "20+",
         "4505_-7031|entire|condo|20+"
        ],
        [
         "643423129076681645",
         "bonita springs",
         "florida",
         88.0,
         4.477336814478207,
         "2633_-8180",
         "entire",
         "apartment",
         "20+",
         "2633_-8180|entire|apartment|20+"
        ],
        [
         "643423129076681645",
         "bonita springs",
         "florida",
         88.0,
         4.477336814478207,
         "2633_-8180",
         "entire",
         "apartment",
         "20+",
         "2633_-8180|entire|apartment|20+"
        ],
        [
         "20800206",
         "gordonville",
         "pennsylvania",
         255.0,
         5.541263545158426,
         "4003_-7610",
         "entire",
         "home",
         "20+",
         "4003_-7610|entire|home|20+"
        ],
        [
         "867572983915994824",
         "kittery",
         "maine",
         225.0,
         5.41610040220442,
         "4308_-7074",
         "entire",
         "apartment",
         "20+",
         "4308_-7074|entire|apartment|20+"
        ],
        [
         "875581428033264239",
         "innsbrook",
         "missouri",
         264.0,
         5.575949103146316,
         "3877_-9103",
         "entire",
         "other",
         "20+",
         "3877_-9103|entire|other|20+"
        ],
        [
         "791404048769330143",
         "eden",
         "utah",
         110.0,
         4.700480365792417,
         "4132_-11182",
         "entire",
         "apartment",
         "20+",
         "4132_-11182|entire|apartment|20+"
        ],
        [
         "42024153",
         "auburn",
         "alabama",
         605.36,
         6.405823322386266,
         "3264_-8551",
         "other",
         "other",
         "20+",
         "3264_-8551|other|other|20+"
        ],
        [
         "42024153",
         "auburn",
         "alabama",
         605.36,
         6.405823322386266,
         "3264_-8551",
         "other",
         "other",
         "20+",
         "3264_-8551|other|other|20+"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "property_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "log_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "geo_bucket",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "room_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "property_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amenities_bucket",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "similarity_bucket",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✓ Ready for local quantile computation!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset: df_valid\")\n",
    "print(f\"  Rows: {df_valid.count():,}\")\n",
    "\n",
    "print(f\"\\nSimilarity bucketing:\")\n",
    "print(f\"  - Geographic: lat/long bucketed to ~1km cells\")\n",
    "print(f\"  - Room type: entire, private, shared, other\")\n",
    "print(f\"  - Property type: home, apartment, condo, other\")\n",
    "print(f\"  - Amenities: 0-5, 6-10, 11-20, 20+\")\n",
    "\n",
    "num_buckets = df_valid.select(\"similarity_bucket\").distinct().count()\n",
    "print(f\"\\nTotal unique similarity buckets: {num_buckets:,}\")\n",
    "\n",
    "avg_bucket_size = valid_count / num_buckets if num_buckets > 0 else 0\n",
    "print(f\"Average listings per bucket: {avg_bucket_size:.1f}\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "display(df_valid.select(\n",
    "    \"property_id\", \"city\", \"state\", \"price\", \"log_price\",\n",
    "    \"geo_bucket\", \"room_type\", \"property_type\", \"amenities_bucket\", \"similarity_bucket\"\n",
    ").limit(10))\n",
    "\n",
    "print(\"\\n✓ Ready for local quantile computation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e832ec25-4d75-4941-9c5c-60962fc36393",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports & Setup"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSTEP 2: COMPUTE LOCAL QUANTILES PER SIMILARITY BUCKET\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, count as spark_count, when\n",
    "from pyspark.sql.functions import mean, stddev, min as spark_min, max as spark_max\n",
    "from pyspark.sql import Window\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: COMPUTE LOCAL QUANTILES PER SIMILARITY BUCKET\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c701276d-4216-4610-b501-76936e02202b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute 25th Percentile per Bucket"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1. Computing 25th percentile of log_price per bucket...\n   (Using Spark SQL approx_percentile - fully distributed)\n   ✓ Computed quantiles for 189,121 buckets\n\nBucket size statistics:\n   Min bucket size: 1\n   Median bucket size: 2\n   Mean bucket size: 3.9\n   Max bucket size: 1069\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. Compute 25th percentile of log_price per bucket\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n1. Computing 25th percentile of log_price per bucket...\")\n",
    "print(\"   (Using Spark SQL approx_percentile - fully distributed)\")\n",
    "\n",
    "# Group by similarity bucket and compute 25th percentile\n",
    "bucket_quantiles = df_valid.groupBy(\"similarity_bucket\").agg(\n",
    "    expr(\"approx_percentile(log_price, 0.25)\").alias(\"bucket_q25_log_price\"),\n",
    "    spark_count(\"*\").alias(\"bucket_size\")\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Computed quantiles for {bucket_quantiles.count():,} buckets\")\n",
    "\n",
    "# Show bucket size distribution\n",
    "print(\"\\nBucket size statistics:\")\n",
    "bucket_stats = bucket_quantiles.select(\n",
    "    expr(\"min(bucket_size)\").alias(\"min\"),\n",
    "    expr(\"approx_percentile(bucket_size, 0.5)\").alias(\"median\"),\n",
    "    expr(\"avg(bucket_size)\").alias(\"mean\"),\n",
    "    expr(\"max(bucket_size)\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"   Min bucket size: {bucket_stats['min']}\")\n",
    "print(f\"   Median bucket size: {bucket_stats['median']:.0f}\")\n",
    "print(f\"   Mean bucket size: {bucket_stats['mean']:.1f}\")\n",
    "print(f\"   Max bucket size: {bucket_stats['max']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f7cfe4-bde4-45a8-8c5d-e36a7129d505",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join Quantiles Back to Listings"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Joining local quantiles back to listings...\n   ✓ Quantiles joined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. Join quantiles back to listings\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. Joining local quantiles back to listings...\")\n",
    "\n",
    "df_with_quantiles = df_valid.join(\n",
    "    bucket_quantiles,\n",
    "    on=\"similarity_bucket\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"   ✓ Quantiles joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ffe656-6556-4547-bb24-9ad41fe08f50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Raw Price Anomaly Score"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. Computing raw price anomaly score...\n   ✓ Raw price anomaly computed\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. Compute raw price anomaly score\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3. Computing raw price anomaly score...\")\n",
    "\n",
    "# s_price_raw = max(0, q25_bucket - log_price)\n",
    "df_with_anomaly = df_with_quantiles.withColumn(\n",
    "    \"s_price_raw\",\n",
    "    when(\n",
    "        (col(\"bucket_q25_log_price\").isNotNull()) &\n",
    "        (col(\"bucket_q25_log_price\") > col(\"log_price\")),\n",
    "        col(\"bucket_q25_log_price\") - col(\"log_price\")\n",
    "    ).otherwise(0.0)\n",
    ")\n",
    "\n",
    "print(\"   ✓ Raw price anomaly computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2caffe78-5322-4697-8827-a8c5aaf898d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyze Price Anomalies"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n4. Analyzing price anomalies...\n   Listings with positive anomaly: 62,788 (8.6%)\n   Mean raw anomaly: 0.0247\n   Std raw anomaly: 0.1286\n   Max raw anomaly: 5.5738\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. Statistics on anomalies\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n4. Analyzing price anomalies...\")\n",
    "\n",
    "anomalies_positive = df_with_anomaly.filter(col(\"s_price_raw\") > 0)\n",
    "num_anomalies = anomalies_positive.count()\n",
    "total = df_with_anomaly.count()\n",
    "pct_anomalies = (num_anomalies / total * 100) if total > 0 else 0\n",
    "\n",
    "stats = df_with_anomaly.select(\n",
    "    mean(\"s_price_raw\").alias(\"mean\"),\n",
    "    stddev(\"s_price_raw\").alias(\"std\"),\n",
    "    spark_min(\"s_price_raw\").alias(\"min\"),\n",
    "    spark_max(\"s_price_raw\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"   Listings with positive anomaly: {num_anomalies:,} ({pct_anomalies:.1f}%)\")\n",
    "print(f\"   Mean raw anomaly: {stats['mean']:.4f}\")\n",
    "print(f\"   Std raw anomaly: {stats['std']:.4f}\")\n",
    "print(f\"   Max raw anomaly: {stats['max']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406bf428-4ffa-4e8b-9fda-b770044bf4ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Summary & Display Results"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nLOCAL QUANTILE COMPUTATION COMPLETE\n================================================================================\n\nDataset: df_with_anomaly\n  Rows: 730,949\n  New columns: bucket_q25_log_price, bucket_size, s_price_raw\n\nSample data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>property_id</th><th>city</th><th>state</th><th>price</th><th>log_price</th><th>similarity_bucket</th><th>bucket_q25_log_price</th><th>bucket_size</th><th>s_price_raw</th></tr></thead><tbody><tr><td>902367758953614183</td><td>benton</td><td>tennessee</td><td>128.0</td><td>4.852030263919617</td><td>3517_-8463|other|other|20+</td><td>4.852030263919617</td><td>2</td><td>0.0</td></tr><tr><td>48672316</td><td>carrabassett valley</td><td>maine</td><td>225.0</td><td>5.41610040220442</td><td>4505_-7031|entire|condo|20+</td><td>5.0238805208462765</td><td>12</td><td>0.0</td></tr><tr><td>42024153</td><td>auburn</td><td>alabama</td><td>605.36</td><td>6.405823322386266</td><td>3264_-8551|other|other|20+</td><td>6.405823322386266</td><td>2</td><td>0.0</td></tr><tr><td>42024153</td><td>auburn</td><td>alabama</td><td>605.36</td><td>6.405823322386266</td><td>3264_-8551|other|other|20+</td><td>6.405823322386266</td><td>2</td><td>0.0</td></tr><tr><td>643423129076681645</td><td>bonita springs</td><td>florida</td><td>88.0</td><td>4.477336814478207</td><td>2633_-8180|entire|apartment|20+</td><td>4.204692619390966</td><td>4</td><td>0.0</td></tr><tr><td>643423129076681645</td><td>bonita springs</td><td>florida</td><td>88.0</td><td>4.477336814478207</td><td>2633_-8180|entire|apartment|20+</td><td>4.204692619390966</td><td>4</td><td>0.0</td></tr><tr><td>867572983915994824</td><td>kittery</td><td>maine</td><td>225.0</td><td>5.41610040220442</td><td>4308_-7074|entire|apartment|20+</td><td>5.41610040220442</td><td>1</td><td>0.0</td></tr><tr><td>791404048769330143</td><td>eden</td><td>utah</td><td>110.0</td><td>4.700480365792417</td><td>4132_-11182|entire|apartment|20+</td><td>4.859812404361672</td><td>13</td><td>0.15933203856925537</td></tr><tr><td>875581428033264239</td><td>innsbrook</td><td>missouri</td><td>264.0</td><td>5.575949103146316</td><td>3877_-9103|entire|other|20+</td><td>5.575949103146316</td><td>3</td><td>0.0</td></tr><tr><td>20800206</td><td>gordonville</td><td>pennsylvania</td><td>255.0</td><td>5.541263545158426</td><td>4003_-7610|entire|home|20+</td><td>5.236441962829949</td><td>5</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "902367758953614183",
         "benton",
         "tennessee",
         128.0,
         4.852030263919617,
         "3517_-8463|other|other|20+",
         4.852030263919617,
         2,
         0.0
        ],
        [
         "48672316",
         "carrabassett valley",
         "maine",
         225.0,
         5.41610040220442,
         "4505_-7031|entire|condo|20+",
         5.0238805208462765,
         12,
         0.0
        ],
        [
         "42024153",
         "auburn",
         "alabama",
         605.36,
         6.405823322386266,
         "3264_-8551|other|other|20+",
         6.405823322386266,
         2,
         0.0
        ],
        [
         "42024153",
         "auburn",
         "alabama",
         605.36,
         6.405823322386266,
         "3264_-8551|other|other|20+",
         6.405823322386266,
         2,
         0.0
        ],
        [
         "643423129076681645",
         "bonita springs",
         "florida",
         88.0,
         4.477336814478207,
         "2633_-8180|entire|apartment|20+",
         4.204692619390966,
         4,
         0.0
        ],
        [
         "643423129076681645",
         "bonita springs",
         "florida",
         88.0,
         4.477336814478207,
         "2633_-8180|entire|apartment|20+",
         4.204692619390966,
         4,
         0.0
        ],
        [
         "867572983915994824",
         "kittery",
         "maine",
         225.0,
         5.41610040220442,
         "4308_-7074|entire|apartment|20+",
         5.41610040220442,
         1,
         0.0
        ],
        [
         "791404048769330143",
         "eden",
         "utah",
         110.0,
         4.700480365792417,
         "4132_-11182|entire|apartment|20+",
         4.859812404361672,
         13,
         0.15933203856925537
        ],
        [
         "875581428033264239",
         "innsbrook",
         "missouri",
         264.0,
         5.575949103146316,
         "3877_-9103|entire|other|20+",
         5.575949103146316,
         3,
         0.0
        ],
        [
         "20800206",
         "gordonville",
         "pennsylvania",
         255.0,
         5.541263545158426,
         "4003_-7610|entire|home|20+",
         5.236441962829949,
         5,
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "property_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "log_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "similarity_bucket",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "bucket_q25_log_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "bucket_size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "s_price_raw",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTop 10 anomalies (most underpriced):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>property_id</th><th>city</th><th>state</th><th>price</th><th>log_price</th><th>bucket_q25_log_price</th><th>s_price_raw</th><th>bucket_size</th></tr></thead><tbody><tr><td>47890017</td><td>new york</td><td>united states</td><td>18.0</td><td>2.8903717578961645</td><td>8.464214266625351</td><td>5.573842508729187</td><td>59</td></tr><tr><td>34749828</td><td>beverly hills</td><td>california</td><td>30.0</td><td>3.4011973816621555</td><td>8.797095076549056</td><td>5.3958976948869</td><td>6</td></tr><tr><td>711469173030352391</td><td>orlando</td><td>florida</td><td>90.0</td><td>4.499809670330265</td><td>8.879611609982035</td><td>4.37980193965177</td><td>38</td></tr><tr><td>711469173030352391</td><td>orlando</td><td>florida</td><td>90.0</td><td>4.499809670330265</td><td>8.879611609982035</td><td>4.37980193965177</td><td>38</td></tr><tr><td>1233533517487378591</td><td>jersey city</td><td>new jersey</td><td>115.0</td><td>4.74493212836325</td><td>8.989943046329998</td><td>4.245010917966748</td><td>50</td></tr><tr><td>1233533517487378591</td><td>jersey city</td><td>new jersey</td><td>115.0</td><td>4.74493212836325</td><td>8.989943046329998</td><td>4.245010917966748</td><td>50</td></tr><tr><td>2977386</td><td>beverly hills</td><td>california</td><td>10.0</td><td>2.302585092994046</td><td>6.415096959171596</td><td>4.11251186617755</td><td>6</td></tr><tr><td>1177414235621227668</td><td>jersey city</td><td>new jersey</td><td>150.0</td><td>5.0106352940962555</td><td>8.989943046329998</td><td>3.9793077522337423</td><td>50</td></tr><tr><td>1177414235621227668</td><td>jersey city</td><td>new jersey</td><td>150.0</td><td>5.0106352940962555</td><td>8.989943046329998</td><td>3.9793077522337423</td><td>50</td></tr><tr><td>1156886612042523980</td><td>new york</td><td>united states</td><td>95.0</td><td>4.553876891600541</td><td>8.504107951867582</td><td>3.9502310602670407</td><td>37</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "47890017",
         "new york",
         "united states",
         18.0,
         2.8903717578961645,
         8.464214266625351,
         5.573842508729187,
         59
        ],
        [
         "34749828",
         "beverly hills",
         "california",
         30.0,
         3.4011973816621555,
         8.797095076549056,
         5.3958976948869,
         6
        ],
        [
         "711469173030352391",
         "orlando",
         "florida",
         90.0,
         4.499809670330265,
         8.879611609982035,
         4.37980193965177,
         38
        ],
        [
         "711469173030352391",
         "orlando",
         "florida",
         90.0,
         4.499809670330265,
         8.879611609982035,
         4.37980193965177,
         38
        ],
        [
         "1233533517487378591",
         "jersey city",
         "new jersey",
         115.0,
         4.74493212836325,
         8.989943046329998,
         4.245010917966748,
         50
        ],
        [
         "1233533517487378591",
         "jersey city",
         "new jersey",
         115.0,
         4.74493212836325,
         8.989943046329998,
         4.245010917966748,
         50
        ],
        [
         "2977386",
         "beverly hills",
         "california",
         10.0,
         2.302585092994046,
         6.415096959171596,
         4.11251186617755,
         6
        ],
        [
         "1177414235621227668",
         "jersey city",
         "new jersey",
         150.0,
         5.0106352940962555,
         8.989943046329998,
         3.9793077522337423,
         50
        ],
        [
         "1177414235621227668",
         "jersey city",
         "new jersey",
         150.0,
         5.0106352940962555,
         8.989943046329998,
         3.9793077522337423,
         50
        ],
        [
         "1156886612042523980",
         "new york",
         "united states",
         95.0,
         4.553876891600541,
         8.504107951867582,
         3.9502310602670407,
         37
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "property_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "log_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "bucket_q25_log_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_price_raw",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "bucket_size",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✓ Ready for normalization!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOCAL QUANTILE COMPUTATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset: df_with_anomaly\")\n",
    "print(f\"  Rows: {df_with_anomaly.count():,}\")\n",
    "print(f\"  New columns: bucket_q25_log_price, bucket_size, s_price_raw\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "display(df_with_anomaly.select(\n",
    "    \"property_id\", \"city\", \"state\", \"price\", \"log_price\",\n",
    "    \"similarity_bucket\", \"bucket_q25_log_price\", \"bucket_size\", \"s_price_raw\"\n",
    ").limit(10))\n",
    "\n",
    "print(\"\\nTop 10 anomalies (most underpriced):\")\n",
    "display(df_with_anomaly.orderBy(col(\"s_price_raw\").desc()).select(\n",
    "    \"property_id\", \"city\", \"state\", \"price\", \"log_price\",\n",
    "    \"bucket_q25_log_price\", \"s_price_raw\", \"bucket_size\"\n",
    ").limit(10))\n",
    "\n",
    "print(\"\\n✓ Ready for normalization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e039b78-5606-4d8b-9128-f06256481e6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports & Header"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSTEP 3: NORMALIZE ANOMALY & COMPUTE REPUTATION SIGNALS\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, exp, log as spark_log, when, least, greatest, percent_rank\n",
    "from pyspark.sql.functions import mean, stddev, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 3: NORMALIZE ANOMALY & COMPUTE REPUTATION SIGNALS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb35841-4a64-4ea8-b443-03b590b04205",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalize Price Anomaly Score"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1. Normalizing price anomaly score...\n   ✓ Price anomaly normalized to [0, 1] using percent_rank()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Normalizing price anomaly score...\")\n",
    "\n",
    "# Create window for percentile rank\n",
    "window_spec = Window.orderBy(col(\"s_price_raw\"))\n",
    "\n",
    "# Compute percentile rank (0 to 1)\n",
    "df_with_price_score = df_with_anomaly.withColumn(\n",
    "    \"s_price\",\n",
    "    percent_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "print(\"   ✓ Price anomaly normalized to [0, 1] using percent_rank()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69bcab5c-0da0-46a1-9bdb-41b7b1062ede",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Review Count Signal"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Computing review count signal...\n   ✓ Review signal computed (tau=12.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Computing review count signal...\")\n",
    "\n",
    "# s_reviews = exp(-reviews / tau)\n",
    "tau = 12.0\n",
    "\n",
    "df_with_signals = df_with_price_score.withColumn(\n",
    "    \"s_reviews\",\n",
    "    exp(-col(\"property_number_of_reviews\").cast(DoubleType()) / lit(tau))\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Review signal computed (tau={tau})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a99c58d-185f-4746-a57a-867cbf670486",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Rating Signal"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. Computing rating signal...\n   ✓ Rating signal computed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. Computing rating signal...\")\n",
    "\n",
    "# effective_rating = rating × min(1, log(reviews+1) / log(20))\n",
    "df_with_signals = df_with_signals.withColumn(\n",
    "    \"effective_rating\",\n",
    "    col(\"ratings\") * least(\n",
    "        lit(1.0),\n",
    "        spark_log(col(\"property_number_of_reviews\") + 1) / spark_log(lit(20.0))\n",
    "    )\n",
    ")\n",
    "\n",
    "# s_rating = max(0, 4.2 - effective_rating) / 4.2\n",
    "df_with_signals = df_with_signals.withColumn(\n",
    "    \"s_rating\",\n",
    "    greatest(lit(0.0), lit(4.2) - col(\"effective_rating\")) / lit(4.2)\n",
    ")\n",
    "\n",
    "print(\"   ✓ Rating signal computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be5a05c3-24ab-4b0a-b3a7-4e18e9061d78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Superhost Signal"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n4. Computing superhost signal...\n   ✓ Superhost signal computed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Computing superhost signal...\")\n",
    "\n",
    "# s_superhost = 1 if not superhost, 0 if superhost\n",
    "df_with_signals = df_with_signals.withColumn(\n",
    "    \"s_superhost\",\n",
    "    when(col(\"is_supperhost\") == 0, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "print(\"   ✓ Superhost signal computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9fe993-fdf1-499f-9244-53ebd0751a49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Signal Statistics"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n5. Analyzing risk signals...\n\nRisk signal statistics:\n\n  s_price (price anomaly):\n    Mean: 0.0822\n    Std:  0.2683\n\n  s_reviews (review count):\n    Mean: 0.5952\n    Std:  0.4421\n\n  s_rating (rating quality):\n    Mean: 0.5298\n    Std:  0.4731\n\n  s_superhost:\n    Mean: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Analyzing risk signals...\")\n",
    "\n",
    "signal_stats = df_with_signals.select(\n",
    "    mean(\"s_price\").alias(\"s_price_mean\"),\n",
    "    stddev(\"s_price\").alias(\"s_price_std\"),\n",
    "    mean(\"s_reviews\").alias(\"s_reviews_mean\"),\n",
    "    stddev(\"s_reviews\").alias(\"s_reviews_std\"),\n",
    "    mean(\"s_rating\").alias(\"s_rating_mean\"),\n",
    "    stddev(\"s_rating\").alias(\"s_rating_std\"),\n",
    "    mean(\"s_superhost\").alias(\"s_superhost_mean\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nRisk signal statistics:\")\n",
    "print(f\"\\n  s_price (price anomaly):\")\n",
    "print(f\"    Mean: {signal_stats['s_price_mean']:.4f}\")\n",
    "print(f\"    Std:  {signal_stats['s_price_std']:.4f}\")\n",
    "\n",
    "print(f\"\\n  s_reviews (review count):\")\n",
    "print(f\"    Mean: {signal_stats['s_reviews_mean']:.4f}\")\n",
    "print(f\"    Std:  {signal_stats['s_reviews_std']:.4f}\")\n",
    "\n",
    "print(f\"\\n  s_rating (rating quality):\")\n",
    "print(f\"    Mean: {signal_stats['s_rating_mean']:.4f}\")\n",
    "print(f\"    Std:  {signal_stats['s_rating_std']:.4f}\")\n",
    "\n",
    "print(f\"\\n  s_superhost:\")\n",
    "print(f\"    Mean: {signal_stats['s_superhost_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd92897-034a-42f7-81a7-0d8acc390491",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports Section"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSTEP 4: FINAL RISK SCORE, RANKING, AND FLAGGING\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, percent_rank, when, mean, min as spark_min, max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 4: FINAL RISK SCORE, RANKING, AND FLAGGING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716cc619-b10e-40ae-9685-fad6fb650f15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Weighted Final Risk Score"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1. Computing weighted final risk score...\n   Weights: price=0.4, reviews=0.25, rating=0.25, superhost=0.1\n   ✓ Raw risk score computed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Computing weighted final risk score...\")\n",
    "\n",
    "# Weights\n",
    "w1 = 0.40  # price anomaly\n",
    "w2 = 0.25  # review count\n",
    "w3 = 0.25  # rating quality\n",
    "w4 = 0.10  # superhost\n",
    "\n",
    "print(f\"   Weights: price={w1}, reviews={w2}, rating={w3}, superhost={w4}\")\n",
    "\n",
    "# Compute weighted risk score\n",
    "df_with_risk = df_with_signals.withColumn(\n",
    "    \"risk_score_raw\",\n",
    "    (lit(w1) * col(\"s_price\")) +\n",
    "    (lit(w2) * col(\"s_reviews\")) +\n",
    "    (lit(w3) * col(\"s_rating\")) +\n",
    "    (lit(w4) * col(\"s_superhost\"))\n",
    ")\n",
    "\n",
    "print(\"   ✓ Raw risk score computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce76f7d-c2ae-4c2a-bbff-8594f6d0ef25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalize Risk Score to [0, 1]"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Normalizing risk score to [0, 1]...\n   Risk score range: [0.1000, 1.0000]\n   ✓ Risk score normalized\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Normalizing risk score to [0, 1]...\")\n",
    "\n",
    "# Get min and max for normalization\n",
    "risk_stats = df_with_risk.select(\n",
    "    spark_min(\"risk_score_raw\").alias(\"min\"),\n",
    "    spark_max(\"risk_score_raw\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "min_risk = risk_stats['min']\n",
    "max_risk = risk_stats['max']\n",
    "\n",
    "print(f\"   Risk score range: [{min_risk:.4f}, {max_risk:.4f}]\")\n",
    "\n",
    "# Normalize\n",
    "df_with_risk = df_with_risk.withColumn(\n",
    "    \"final_risk_score\",\n",
    "    (col(\"risk_score_raw\") - lit(min_risk)) / lit(max_risk - min_risk)\n",
    ")\n",
    "\n",
    "print(\"   ✓ Risk score normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7554fdd-82d4-4b3c-9bc4-4d94c82da2c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Risk Percentile Rank"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. Computing risk percentile rank...\n   ✓ Risk percentile computed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. Computing risk percentile rank...\")\n",
    "\n",
    "# Create window for percentile rank\n",
    "risk_window = Window.orderBy(col(\"final_risk_score\"))\n",
    "\n",
    "df_with_risk = df_with_risk.withColumn(\n",
    "    \"risk_rank_percentile\",\n",
    "    percent_rank().over(risk_window)\n",
    ")\n",
    "\n",
    "print(\"   ✓ Risk percentile computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41967de0-7a92-47db-8c78-69e5dfd07929",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flag High-Risk Listings"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n4. Flagging high-risk listings...\n   ✓ Flagged 37,859 listings as high-risk (5.2%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Flagging high-risk listings...\")\n",
    "\n",
    "# Flag top 10%\n",
    "df_final = df_with_risk.withColumn(\n",
    "    \"high_risk\",\n",
    "    when(col(\"risk_rank_percentile\") >= 0.90, True).otherwise(False)\n",
    ")\n",
    "\n",
    "# Count high-risk listings\n",
    "high_risk_count = df_final.filter(col(\"high_risk\") == True).count()\n",
    "total_count = df_final.count()\n",
    "pct_high_risk = (high_risk_count / total_count * 100) if total_count > 0 else 0\n",
    "\n",
    "print(f\"   ✓ Flagged {high_risk_count:,} listings as high-risk ({pct_high_risk:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511b8647-7bba-4cd4-8d6b-cf533c1163fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify All Columns Preserved"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nFINAL COLUMN VERIFICATION\n================================================================================\n\nColumn count progression:\n  airbnb_df (original):        52 columns\n  usa_df (after parsing):      55 columns\n  airbnb_enriched_df:          61 columns\n  df_valid (filtered):         70 columns\n  df_final (with anomalies):   82 columns\n\nAll columns in df_final:\n   1. similarity_bucket\n   2. city\n   3. state\n   4. name\n   5. price\n   6. image\n   7. description\n   8. category\n   9. availability\n  10. discount\n  11. reviews\n  12. ratings\n  13. seller_info\n  14. breadcrumbs\n  15. location\n  16. lat\n  17. long\n  18. guests\n  19. pets_allowed\n  20. description_items\n  21. category_rating\n  22. house_rules\n  23. details\n  24. highlights\n  25. arrangement_details\n  26. amenities\n  27. images\n  28. available_dates\n  29. url\n  30. final_url\n  31. listing_title\n  32. property_id\n  33. listing_name\n  34. location_details\n  35. description_by_sections\n  36. description_html\n  37. location_details_html\n  38. is_supperhost\n  39. host_number_of_reviews\n  40. host_rating\n  41. hosts_year\n  42. host_response_rate\n  43. is_guest_favorite\n  44. travel_details\n  45. pricing_details\n  46. total_price\n  47. currency\n  48. cancellation_policy\n  49. property_number_of_reviews\n  50. country\n  51. postcode_map_url\n  52. host_image\n  53. host_details\n  54. price_numeric\n  55. price_sanity_flag\n  56. country_parsed\n  57. county\n  58. City_Crime_Rate_Per_100K\n  59. Total_Fatalities\n  60. Median_Income\n  61. Disability_Rate\n  62. Median_AQI\n  63. log_price\n  64. amenities_count\n  65. room_type\n  66. property_type\n  67. lat_bucket\n  68. long_bucket\n  69. geo_bucket\n  70. amenities_bucket\n  71. bucket_q25_log_price\n  72. bucket_size\n  73. s_price_raw\n  74. s_price\n  75. s_reviews\n  76. effective_rating\n  77. s_rating\n  78. s_superhost\n  79. risk_score_raw\n  80. final_risk_score\n  81. risk_rank_percentile\n  82. high_risk\n\n✅ SUCCESS: All 52 original columns are preserved in df_final!\n\nNew columns added during processing: 30\n  + City_Crime_Rate_Per_100K\n  + Disability_Rate\n  + Median_AQI\n  + Median_Income\n  + Total_Fatalities\n  + amenities_bucket\n  + amenities_count\n  + bucket_q25_log_price\n  + bucket_size\n  + city\n  + country_parsed\n  + county\n  + effective_rating\n  + final_risk_score\n  + geo_bucket\n  + high_risk\n  + lat_bucket\n  + log_price\n  + long_bucket\n  + property_type\n  + risk_rank_percentile\n  + risk_score_raw\n  + room_type\n  + s_price\n  + s_price_raw\n  + s_rating\n  + s_reviews\n  + s_superhost\n  + similarity_bucket\n  + state\n"
     ]
    }
   ],
   "source": [
    "# DBTITLE 1,Verify All Columns Preserved\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COLUMN VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nColumn count progression:\")\n",
    "print(f\"  airbnb_df (original):        {len(airbnb_df.columns)} columns\")\n",
    "print(f\"  usa_df (after parsing):      {len(usa_df.columns)} columns\")\n",
    "print(f\"  airbnb_enriched_df:          {len(airbnb_enriched_df.columns)} columns\")\n",
    "print(f\"  df_valid (filtered):         {len(df_valid.columns)} columns\")\n",
    "print(f\"  df_final (with anomalies):   {len(df_final.columns)} columns\")\n",
    "\n",
    "print(f\"\\nAll columns in df_final:\")\n",
    "for i, col_name in enumerate(df_final.columns, 1):\n",
    "    print(f\"  {i:2d}. {col_name}\")\n",
    "\n",
    "# Verify original columns are still there\n",
    "original_cols = set(airbnb_df.columns)\n",
    "final_cols = set(df_final.columns)\n",
    "missing_cols = original_cols - final_cols\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"\\n⚠️  WARNING: {len(missing_cols)} original columns were dropped!\")\n",
    "    for col_name in sorted(missing_cols):\n",
    "        print(f\"    - {col_name}\")\n",
    "else:\n",
    "    print(f\"\\n✅ SUCCESS: All {len(original_cols)} original columns are preserved in df_final!\")\n",
    "    \n",
    "added_cols = final_cols - original_cols\n",
    "print(f\"\\nNew columns added during processing: {len(added_cols)}\")\n",
    "for col_name in sorted(added_cols):\n",
    "    print(f\"  + {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf77fb0-dcf0-451f-9a5a-6c85bd870613",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Final Output DataFrame"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n5. Creating final output DataFrame...\n   ✓ Output DataFrame created: 730,949 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Creating final output DataFrame...\")\n",
    "\n",
    "# Select output columns\n",
    "output_df = df_final.select(\n",
    "    col(\"property_id\").alias(\"listing_id\"),\n",
    "    col(\"city\"),\n",
    "    col(\"state\"),\n",
    "    col(\"price\"),\n",
    "    col(\"log_price\"),\n",
    "    col(\"bucket_q25_log_price\").alias(\"predicted_q25_price\"),\n",
    "    col(\"s_price\"),\n",
    "    col(\"s_reviews\"),\n",
    "    col(\"s_rating\"),\n",
    "    col(\"s_superhost\"),\n",
    "    col(\"final_risk_score\"),\n",
    "    col(\"risk_rank_percentile\"),\n",
    "    col(\"high_risk\").alias(\"high_risk_flag\"),\n",
    "    col(\"similarity_bucket\"),\n",
    "    col(\"bucket_size\")\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Output DataFrame created: {output_df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163881b0-e933-4786-81fe-c048a747d571",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Results Summary"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nANOMALY DETECTION COMPLETE\n================================================================================\n\nFinal dataset: output_df\n  Total listings: 730,949\n  High-risk listings: 37,859 (5.2%)\n\nRisk score distribution:\n  Min:  0.0000\n  Mean: 0.3490\n  Max:  1.0000\n\nTop 20 highest-risk listings:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>listing_id</th><th>city</th><th>state</th><th>price</th><th>log_price</th><th>predicted_q25_price</th><th>s_price</th><th>s_reviews</th><th>s_rating</th><th>s_superhost</th><th>final_risk_score</th><th>risk_rank_percentile</th><th>high_risk_flag</th><th>similarity_bucket</th><th>bucket_size</th></tr></thead><tbody><tr><td>47890017</td><td>new york</td><td>united states</td><td>18.0</td><td>2.8903717578961645</td><td>8.464214266625351</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>true</td><td>4076_-7398|entire|apartment|20+</td><td>59</td></tr><tr><td>34749828</td><td>beverly hills</td><td>california</td><td>30.0</td><td>3.4011973816621555</td><td>8.797095076549056</td><td>0.9999986319136245</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999993919616108</td><td>0.9999986319136245</td><td>true</td><td>3414_-11841|entire|other|20+</td><td>6</td></tr><tr><td>711469173030352391</td><td>orlando</td><td>florida</td><td>90.0</td><td>4.499809670330265</td><td>8.879611609982035</td><td>0.9999958957408734</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999981758848325</td><td>0.9999958957408734</td><td>true</td><td>2838_-8147|private|other|20+</td><td>38</td></tr><tr><td>711469173030352391</td><td>orlando</td><td>florida</td><td>90.0</td><td>4.499809670330265</td><td>8.879611609982035</td><td>0.9999958957408734</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999981758848325</td><td>0.9999958957408734</td><td>true</td><td>2838_-8147|private|other|20+</td><td>38</td></tr><tr><td>1233533517487378591</td><td>jersey city</td><td>new jersey</td><td>115.0</td><td>4.74493212836325</td><td>8.989943046329998</td><td>0.9999931595681225</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999969598080545</td><td>0.9999931595681225</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>1233533517487378591</td><td>jersey city</td><td>new jersey</td><td>115.0</td><td>4.74493212836325</td><td>8.989943046329998</td><td>0.9999931595681225</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999969598080545</td><td>0.9999931595681225</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>2977386</td><td>beverly hills</td><td>california</td><td>10.0</td><td>2.302585092994046</td><td>6.415096959171596</td><td>0.999991791481747</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999963517696653</td><td>0.999991791481747</td><td>true</td><td>3407_-11838|entire|condo|20+</td><td>6</td></tr><tr><td>1177414235621227668</td><td>jersey city</td><td>new jersey</td><td>150.0</td><td>5.0106352940962555</td><td>8.989943046329998</td><td>0.999989055308996</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999951356928871</td><td>0.999989055308996</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>1177414235621227668</td><td>jersey city</td><td>new jersey</td><td>150.0</td><td>5.0106352940962555</td><td>8.989943046329998</td><td>0.999989055308996</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999951356928871</td><td>0.999989055308996</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>1156886612042523980</td><td>new york</td><td>united states</td><td>95.0</td><td>4.553876891600541</td><td>8.504107951867582</td><td>0.9999876872226204</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999945276544979</td><td>0.9999876872226204</td><td>true</td><td>4073_-7398|entire|apartment|20+</td><td>37</td></tr><tr><td>1137761450128469107</td><td>arlington</td><td>virginia</td><td>138.0</td><td>4.927253685157205</td><td>8.744966011114109</td><td>0.9999822148771185</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999920955009416</td><td>0.999986319136245</td><td>true</td><td>3889_-7706|entire|apartment|20+</td><td>10</td></tr><tr><td>1142674800027105450</td><td>hermosa beach</td><td>california</td><td>10.0</td><td>2.302585092994046</td><td>6.1092475827643655</td><td>0.9999794787043674</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999908794241633</td><td>0.999983582963494</td><td>true</td><td>3387_-11840|entire|home|20+</td><td>17</td></tr><tr><td>1142674800027105450</td><td>hermosa beach</td><td>california</td><td>10.0</td><td>2.302585092994046</td><td>6.1092475827643655</td><td>0.9999794787043674</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999908794241633</td><td>0.999983582963494</td><td>true</td><td>3387_-11840|entire|home|20+</td><td>17</td></tr><tr><td>38893792</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>38893792</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>13994516</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>13994516</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>981243016821521104</td><td>grand prairie</td><td>texas</td><td>84.0</td><td>4.430816798843313</td><td>8.177796683277778</td><td>0.9999657978406125</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999847990402722</td><td>0.9999740063588655</td><td>true</td><td>3276_-9706|entire|condo|20+</td><td>18</td></tr><tr><td>981243016821521104</td><td>grand prairie</td><td>texas</td><td>84.0</td><td>4.430816798843313</td><td>8.177796683277778</td><td>0.9999657978406125</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999847990402722</td><td>0.9999740063588655</td><td>true</td><td>3276_-9706|entire|condo|20+</td><td>18</td></tr><tr><td>981243016821521104</td><td>grand prairie</td><td>texas</td><td>84.0</td><td>4.430816798843313</td><td>8.177796683277778</td><td>0.9999657978406125</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999847990402722</td><td>0.9999740063588655</td><td>true</td><td>3276_-9706|entire|condo|20+</td><td>18</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "47890017",
         "new york",
         "united states",
         18.0,
         2.8903717578961645,
         8.464214266625351,
         1.0,
         1.0,
         1.0,
         1.0,
         1.0,
         1.0,
         true,
         "4076_-7398|entire|apartment|20+",
         59
        ],
        [
         "34749828",
         "beverly hills",
         "california",
         30.0,
         3.4011973816621555,
         8.797095076549056,
         0.9999986319136245,
         1.0,
         1.0,
         1.0,
         0.9999993919616108,
         0.9999986319136245,
         true,
         "3414_-11841|entire|other|20+",
         6
        ],
        [
         "711469173030352391",
         "orlando",
         "florida",
         90.0,
         4.499809670330265,
         8.879611609982035,
         0.9999958957408734,
         1.0,
         1.0,
         1.0,
         0.9999981758848325,
         0.9999958957408734,
         true,
         "2838_-8147|private|other|20+",
         38
        ],
        [
         "711469173030352391",
         "orlando",
         "florida",
         90.0,
         4.499809670330265,
         8.879611609982035,
         0.9999958957408734,
         1.0,
         1.0,
         1.0,
         0.9999981758848325,
         0.9999958957408734,
         true,
         "2838_-8147|private|other|20+",
         38
        ],
        [
         "1233533517487378591",
         "jersey city",
         "new jersey",
         115.0,
         4.74493212836325,
         8.989943046329998,
         0.9999931595681225,
         1.0,
         1.0,
         1.0,
         0.9999969598080545,
         0.9999931595681225,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "1233533517487378591",
         "jersey city",
         "new jersey",
         115.0,
         4.74493212836325,
         8.989943046329998,
         0.9999931595681225,
         1.0,
         1.0,
         1.0,
         0.9999969598080545,
         0.9999931595681225,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "2977386",
         "beverly hills",
         "california",
         10.0,
         2.302585092994046,
         6.415096959171596,
         0.999991791481747,
         1.0,
         1.0,
         1.0,
         0.9999963517696653,
         0.999991791481747,
         true,
         "3407_-11838|entire|condo|20+",
         6
        ],
        [
         "1177414235621227668",
         "jersey city",
         "new jersey",
         150.0,
         5.0106352940962555,
         8.989943046329998,
         0.999989055308996,
         1.0,
         1.0,
         1.0,
         0.9999951356928871,
         0.999989055308996,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "1177414235621227668",
         "jersey city",
         "new jersey",
         150.0,
         5.0106352940962555,
         8.989943046329998,
         0.999989055308996,
         1.0,
         1.0,
         1.0,
         0.9999951356928871,
         0.999989055308996,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "1156886612042523980",
         "new york",
         "united states",
         95.0,
         4.553876891600541,
         8.504107951867582,
         0.9999876872226204,
         1.0,
         1.0,
         1.0,
         0.9999945276544979,
         0.9999876872226204,
         true,
         "4073_-7398|entire|apartment|20+",
         37
        ],
        [
         "1137761450128469107",
         "arlington",
         "virginia",
         138.0,
         4.927253685157205,
         8.744966011114109,
         0.9999822148771185,
         1.0,
         1.0,
         1.0,
         0.9999920955009416,
         0.999986319136245,
         true,
         "3889_-7706|entire|apartment|20+",
         10
        ],
        [
         "1142674800027105450",
         "hermosa beach",
         "california",
         10.0,
         2.302585092994046,
         6.1092475827643655,
         0.9999794787043674,
         1.0,
         1.0,
         1.0,
         0.9999908794241633,
         0.999983582963494,
         true,
         "3387_-11840|entire|home|20+",
         17
        ],
        [
         "1142674800027105450",
         "hermosa beach",
         "california",
         10.0,
         2.302585092994046,
         6.1092475827643655,
         0.9999794787043674,
         1.0,
         1.0,
         1.0,
         0.9999908794241633,
         0.999983582963494,
         true,
         "3387_-11840|entire|home|20+",
         17
        ],
        [
         "38893792",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "38893792",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "13994516",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "13994516",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "981243016821521104",
         "grand prairie",
         "texas",
         84.0,
         4.430816798843313,
         8.177796683277778,
         0.9999657978406125,
         1.0,
         1.0,
         1.0,
         0.9999847990402722,
         0.9999740063588655,
         true,
         "3276_-9706|entire|condo|20+",
         18
        ],
        [
         "981243016821521104",
         "grand prairie",
         "texas",
         84.0,
         4.430816798843313,
         8.177796683277778,
         0.9999657978406125,
         1.0,
         1.0,
         1.0,
         0.9999847990402722,
         0.9999740063588655,
         true,
         "3276_-9706|entire|condo|20+",
         18
        ],
        [
         "981243016821521104",
         "grand prairie",
         "texas",
         84.0,
         4.430816798843313,
         8.177796683277778,
         0.9999657978406125,
         1.0,
         1.0,
         1.0,
         0.9999847990402722,
         0.9999740063588655,
         true,
         "3276_-9706|entire|condo|20+",
         18
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "listing_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "log_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "predicted_q25_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_reviews",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_rating",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_superhost",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "final_risk_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "risk_rank_percentile",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "high_risk_flag",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "similarity_bucket",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "bucket_size",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nHigh-risk listings only (sample):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>listing_id</th><th>city</th><th>state</th><th>price</th><th>log_price</th><th>predicted_q25_price</th><th>s_price</th><th>s_reviews</th><th>s_rating</th><th>s_superhost</th><th>final_risk_score</th><th>risk_rank_percentile</th><th>high_risk_flag</th><th>similarity_bucket</th><th>bucket_size</th></tr></thead><tbody><tr><td>47890017</td><td>new york</td><td>united states</td><td>18.0</td><td>2.8903717578961645</td><td>8.464214266625351</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>true</td><td>4076_-7398|entire|apartment|20+</td><td>59</td></tr><tr><td>34749828</td><td>beverly hills</td><td>california</td><td>30.0</td><td>3.4011973816621555</td><td>8.797095076549056</td><td>0.9999986319136245</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999993919616108</td><td>0.9999986319136245</td><td>true</td><td>3414_-11841|entire|other|20+</td><td>6</td></tr><tr><td>711469173030352391</td><td>orlando</td><td>florida</td><td>90.0</td><td>4.499809670330265</td><td>8.879611609982035</td><td>0.9999958957408734</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999981758848325</td><td>0.9999958957408734</td><td>true</td><td>2838_-8147|private|other|20+</td><td>38</td></tr><tr><td>711469173030352391</td><td>orlando</td><td>florida</td><td>90.0</td><td>4.499809670330265</td><td>8.879611609982035</td><td>0.9999958957408734</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999981758848325</td><td>0.9999958957408734</td><td>true</td><td>2838_-8147|private|other|20+</td><td>38</td></tr><tr><td>1233533517487378591</td><td>jersey city</td><td>new jersey</td><td>115.0</td><td>4.74493212836325</td><td>8.989943046329998</td><td>0.9999931595681225</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999969598080545</td><td>0.9999931595681225</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>1233533517487378591</td><td>jersey city</td><td>new jersey</td><td>115.0</td><td>4.74493212836325</td><td>8.989943046329998</td><td>0.9999931595681225</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999969598080545</td><td>0.9999931595681225</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>2977386</td><td>beverly hills</td><td>california</td><td>10.0</td><td>2.302585092994046</td><td>6.415096959171596</td><td>0.999991791481747</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999963517696653</td><td>0.999991791481747</td><td>true</td><td>3407_-11838|entire|condo|20+</td><td>6</td></tr><tr><td>1177414235621227668</td><td>jersey city</td><td>new jersey</td><td>150.0</td><td>5.0106352940962555</td><td>8.989943046329998</td><td>0.999989055308996</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999951356928871</td><td>0.999989055308996</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>1177414235621227668</td><td>jersey city</td><td>new jersey</td><td>150.0</td><td>5.0106352940962555</td><td>8.989943046329998</td><td>0.999989055308996</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999951356928871</td><td>0.999989055308996</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>1156886612042523980</td><td>new york</td><td>united states</td><td>95.0</td><td>4.553876891600541</td><td>8.504107951867582</td><td>0.9999876872226204</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999945276544979</td><td>0.9999876872226204</td><td>true</td><td>4073_-7398|entire|apartment|20+</td><td>37</td></tr><tr><td>1137761450128469107</td><td>arlington</td><td>virginia</td><td>138.0</td><td>4.927253685157205</td><td>8.744966011114109</td><td>0.9999822148771185</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999920955009416</td><td>0.999986319136245</td><td>true</td><td>3889_-7706|entire|apartment|20+</td><td>10</td></tr><tr><td>1142674800027105450</td><td>hermosa beach</td><td>california</td><td>10.0</td><td>2.302585092994046</td><td>6.1092475827643655</td><td>0.9999794787043674</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999908794241633</td><td>0.999983582963494</td><td>true</td><td>3387_-11840|entire|home|20+</td><td>17</td></tr><tr><td>1142674800027105450</td><td>hermosa beach</td><td>california</td><td>10.0</td><td>2.302585092994046</td><td>6.1092475827643655</td><td>0.9999794787043674</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999908794241633</td><td>0.999983582963494</td><td>true</td><td>3387_-11840|entire|home|20+</td><td>17</td></tr><tr><td>38893792</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>38893792</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>13994516</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>13994516</td><td>jersey city</td><td>new jersey</td><td>179.0</td><td>5.187385805840755</td><td>8.989943046329998</td><td>0.9999740063588655</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999884472706069</td><td>0.999978110617992</td><td>true</td><td>4072_-7403|entire|apartment|20+</td><td>50</td></tr><tr><td>981243016821521104</td><td>grand prairie</td><td>texas</td><td>84.0</td><td>4.430816798843313</td><td>8.177796683277778</td><td>0.9999657978406125</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999847990402722</td><td>0.9999740063588655</td><td>true</td><td>3276_-9706|entire|condo|20+</td><td>18</td></tr><tr><td>981243016821521104</td><td>grand prairie</td><td>texas</td><td>84.0</td><td>4.430816798843313</td><td>8.177796683277778</td><td>0.9999657978406125</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999847990402722</td><td>0.9999740063588655</td><td>true</td><td>3276_-9706|entire|condo|20+</td><td>18</td></tr><tr><td>981243016821521104</td><td>grand prairie</td><td>texas</td><td>84.0</td><td>4.430816798843313</td><td>8.177796683277778</td><td>0.9999657978406125</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.9999847990402722</td><td>0.9999740063588655</td><td>true</td><td>3276_-9706|entire|condo|20+</td><td>18</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "47890017",
         "new york",
         "united states",
         18.0,
         2.8903717578961645,
         8.464214266625351,
         1.0,
         1.0,
         1.0,
         1.0,
         1.0,
         1.0,
         true,
         "4076_-7398|entire|apartment|20+",
         59
        ],
        [
         "34749828",
         "beverly hills",
         "california",
         30.0,
         3.4011973816621555,
         8.797095076549056,
         0.9999986319136245,
         1.0,
         1.0,
         1.0,
         0.9999993919616108,
         0.9999986319136245,
         true,
         "3414_-11841|entire|other|20+",
         6
        ],
        [
         "711469173030352391",
         "orlando",
         "florida",
         90.0,
         4.499809670330265,
         8.879611609982035,
         0.9999958957408734,
         1.0,
         1.0,
         1.0,
         0.9999981758848325,
         0.9999958957408734,
         true,
         "2838_-8147|private|other|20+",
         38
        ],
        [
         "711469173030352391",
         "orlando",
         "florida",
         90.0,
         4.499809670330265,
         8.879611609982035,
         0.9999958957408734,
         1.0,
         1.0,
         1.0,
         0.9999981758848325,
         0.9999958957408734,
         true,
         "2838_-8147|private|other|20+",
         38
        ],
        [
         "1233533517487378591",
         "jersey city",
         "new jersey",
         115.0,
         4.74493212836325,
         8.989943046329998,
         0.9999931595681225,
         1.0,
         1.0,
         1.0,
         0.9999969598080545,
         0.9999931595681225,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "1233533517487378591",
         "jersey city",
         "new jersey",
         115.0,
         4.74493212836325,
         8.989943046329998,
         0.9999931595681225,
         1.0,
         1.0,
         1.0,
         0.9999969598080545,
         0.9999931595681225,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "2977386",
         "beverly hills",
         "california",
         10.0,
         2.302585092994046,
         6.415096959171596,
         0.999991791481747,
         1.0,
         1.0,
         1.0,
         0.9999963517696653,
         0.999991791481747,
         true,
         "3407_-11838|entire|condo|20+",
         6
        ],
        [
         "1177414235621227668",
         "jersey city",
         "new jersey",
         150.0,
         5.0106352940962555,
         8.989943046329998,
         0.999989055308996,
         1.0,
         1.0,
         1.0,
         0.9999951356928871,
         0.999989055308996,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "1177414235621227668",
         "jersey city",
         "new jersey",
         150.0,
         5.0106352940962555,
         8.989943046329998,
         0.999989055308996,
         1.0,
         1.0,
         1.0,
         0.9999951356928871,
         0.999989055308996,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "1156886612042523980",
         "new york",
         "united states",
         95.0,
         4.553876891600541,
         8.504107951867582,
         0.9999876872226204,
         1.0,
         1.0,
         1.0,
         0.9999945276544979,
         0.9999876872226204,
         true,
         "4073_-7398|entire|apartment|20+",
         37
        ],
        [
         "1137761450128469107",
         "arlington",
         "virginia",
         138.0,
         4.927253685157205,
         8.744966011114109,
         0.9999822148771185,
         1.0,
         1.0,
         1.0,
         0.9999920955009416,
         0.999986319136245,
         true,
         "3889_-7706|entire|apartment|20+",
         10
        ],
        [
         "1142674800027105450",
         "hermosa beach",
         "california",
         10.0,
         2.302585092994046,
         6.1092475827643655,
         0.9999794787043674,
         1.0,
         1.0,
         1.0,
         0.9999908794241633,
         0.999983582963494,
         true,
         "3387_-11840|entire|home|20+",
         17
        ],
        [
         "1142674800027105450",
         "hermosa beach",
         "california",
         10.0,
         2.302585092994046,
         6.1092475827643655,
         0.9999794787043674,
         1.0,
         1.0,
         1.0,
         0.9999908794241633,
         0.999983582963494,
         true,
         "3387_-11840|entire|home|20+",
         17
        ],
        [
         "38893792",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "38893792",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "13994516",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "13994516",
         "jersey city",
         "new jersey",
         179.0,
         5.187385805840755,
         8.989943046329998,
         0.9999740063588655,
         1.0,
         1.0,
         1.0,
         0.9999884472706069,
         0.999978110617992,
         true,
         "4072_-7403|entire|apartment|20+",
         50
        ],
        [
         "981243016821521104",
         "grand prairie",
         "texas",
         84.0,
         4.430816798843313,
         8.177796683277778,
         0.9999657978406125,
         1.0,
         1.0,
         1.0,
         0.9999847990402722,
         0.9999740063588655,
         true,
         "3276_-9706|entire|condo|20+",
         18
        ],
        [
         "981243016821521104",
         "grand prairie",
         "texas",
         84.0,
         4.430816798843313,
         8.177796683277778,
         0.9999657978406125,
         1.0,
         1.0,
         1.0,
         0.9999847990402722,
         0.9999740063588655,
         true,
         "3276_-9706|entire|condo|20+",
         18
        ],
        [
         "981243016821521104",
         "grand prairie",
         "texas",
         84.0,
         4.430816798843313,
         8.177796683277778,
         0.9999657978406125,
         1.0,
         1.0,
         1.0,
         0.9999847990402722,
         0.9999740063588655,
         true,
         "3276_-9706|entire|condo|20+",
         18
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "listing_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "log_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "predicted_q25_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_reviews",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_rating",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "s_superhost",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "final_risk_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "risk_rank_percentile",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "high_risk_flag",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "similarity_bucket",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "bucket_size",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✓ Analysis complete!\n\nOutput DataFrame 'output_df' contains:\n  - listing_id, city, state, price\n  - predicted_q25_price (local 25th percentile from bucket)\n  - s_price, s_reviews, s_rating, s_superhost\n  - final_risk_score, risk_rank_percentile, high_risk_flag\n  - similarity_bucket, bucket_size (for explainability)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANOMALY DETECTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFinal dataset: output_df\")\n",
    "print(f\"  Total listings: {total_count:,}\")\n",
    "print(f\"  High-risk listings: {high_risk_count:,} ({pct_high_risk:.1f}%)\")\n",
    "\n",
    "print(\"\\nRisk score distribution:\")\n",
    "risk_dist = output_df.select(\n",
    "    spark_min(\"final_risk_score\").alias(\"min\"),\n",
    "    mean(\"final_risk_score\").alias(\"mean\"),\n",
    "    spark_max(\"final_risk_score\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Min:  {risk_dist['min']:.4f}\")\n",
    "print(f\"  Mean: {risk_dist['mean']:.4f}\")\n",
    "print(f\"  Max:  {risk_dist['max']:.4f}\")\n",
    "\n",
    "print(\"\\nTop 20 highest-risk listings:\")\n",
    "display(output_df.orderBy(col(\"final_risk_score\").desc()).limit(20))\n",
    "\n",
    "print(\"\\nHigh-risk listings only (sample):\")\n",
    "display(output_df.filter(col(\"high_risk_flag\") == True).orderBy(col(\"final_risk_score\").desc()).limit(20))\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")\n",
    "print(\"\\nOutput DataFrame 'output_df' contains:\")\n",
    "print(\"  - listing_id, city, state, price\")\n",
    "print(\"  - predicted_q25_price (local 25th percentile from bucket)\")\n",
    "print(\"  - s_price, s_reviews, s_rating, s_superhost\")\n",
    "print(\"  - final_risk_score, risk_rank_percentile, high_risk_flag\")\n",
    "print(\"  - similarity_bucket, bucket_size (for explainability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0373fb85-724f-4bb2-8630-79d894f510ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Features for Machine Learning and Recommendation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d71357-8226-41ea-8629-8055c8a7ba44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load the relevant data"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    load_path = \"dbfs:/Workspace/Users/eliezerm@campus.technion.ac.il/eliezer_data\"\n",
    "    df_reloaded = spark.read.parquet(load_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f224c147-7cd4-448d-b062-4806eb1d1d36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define All Feature Categories"
    }
   },
   "outputs": [],
   "source": [
    "try:  \n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Imports and Configuration\n",
    "\n",
    "    from pyspark.sql.functions import (\n",
    "        col, lit, when, coalesce, create_map, array,\n",
    "        isnan, isnull, lower, trim, to_json, size,\n",
    "        concat_ws, struct, regexp_replace,\n",
    "        # ADD THESE NEW IMPORTS:\n",
    "        length, split, regexp_extract, \n",
    "        expr, udf, array_contains\n",
    "    )\n",
    "    from pyspark.sql.types import (\n",
    "        DoubleType, StringType, IntegerType,\n",
    "        # ADD THIS:\n",
    "        FloatType\n",
    "    )\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE FEATURES VECTOR PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Extracts ALL features into a single dictionary per listing\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Define All Feature Categories\n",
    "\n",
    "    # =============================================================================\n",
    "    # TEXT/DESCRIPTION FEATURES (9)\n",
    "    # =============================================================================\n",
    "    TEXT_DESCRIPTION_FEATURES = [\n",
    "        \"description\",\n",
    "        \"description_html\",\n",
    "        \"description_by_sections\",\n",
    "        \"description_items\",\n",
    "        \"highlights\",\n",
    "        \"house_rules\",\n",
    "        \"listing_name\",\n",
    "        \"listing_title\",\n",
    "        \"details\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # LOCATION FEATURES (6)\n",
    "    # =============================================================================\n",
    "    LOCATION_FEATURES = [\n",
    "        \"country\",\n",
    "        \"country_parsed\",\n",
    "        \"city\",\n",
    "        \"state\",\n",
    "        \"county\",\n",
    "        \"location\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # CATEGORICAL/BUCKETING RAW FEATURES (2)\n",
    "    # =============================================================================\n",
    "    CATEGORICAL_RAW_FEATURES = [\n",
    "        \"category\",\n",
    "        \"category_rating\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # PRICE/DISCOUNT FEATURES (2)\n",
    "    # =============================================================================\n",
    "    PRICE_DISCOUNT_FEATURES = [\n",
    "        \"price\",\n",
    "        \"discount\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # REVIEW FEATURES (1)\n",
    "    # =============================================================================\n",
    "    REVIEW_FEATURES = [\n",
    "        \"reviews\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # URL/METADATA FEATURES (5)\n",
    "    # =============================================================================\n",
    "    URL_METADATA_FEATURES = [\n",
    "        \"url\",\n",
    "        \"final_url\",\n",
    "        \"postcode_map_url\",\n",
    "        \"breadcrumbs\",\n",
    "        \"arrangement_details\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # MEDIA FEATURES (3)\n",
    "    # =============================================================================\n",
    "    MEDIA_FEATURES = [\n",
    "        \"image\",\n",
    "        \"images\",\n",
    "        \"host_image\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # HOST/SELLER FEATURES (3)\n",
    "    # =============================================================================\n",
    "    HOST_SELLER_FEATURES = [\n",
    "        \"host_details\",\n",
    "        \"seller_info\",\n",
    "        \"name\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # OTHER FEATURES (3)\n",
    "    # =============================================================================\n",
    "    OTHER_FEATURES = [\n",
    "        \"currency\",\n",
    "        \"amenities\",\n",
    "        \"travel_details\",\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # NUMERICAL FEATURES (30) - From Original Pipeline\n",
    "    # =============================================================================\n",
    "    NUMERICAL_FEATURES = [\n",
    "        (\"price_numeric\", 0.0),\n",
    "        (\"total_price\", 0.0),\n",
    "        (\"log_price\", 0.0),\n",
    "        (\"bucket_q25_log_price\", 0.0),\n",
    "        (\"lat\", 0.0),\n",
    "        (\"long\", 0.0),\n",
    "        (\"guests\", 0.0),\n",
    "        (\"amenities_count\", 0.0),\n",
    "        (\"bucket_size\", 0.0),\n",
    "        (\"ratings\", 0.0),\n",
    "        (\"property_number_of_reviews\", 0.0),\n",
    "        (\"effective_rating\", 0.0),\n",
    "        (\"host_number_of_reviews\", 0.0),\n",
    "        (\"host_rating\", 0.0),\n",
    "        (\"hosts_year\", 0.0),\n",
    "        (\"host_response_rate\", 0.0),\n",
    "        (\"is_supperhost\", 0.0),\n",
    "        (\"City_Crime_Rate_Per_100K\", 0.0),\n",
    "        (\"Total_Fatalities\", 0.0),\n",
    "        (\"Median_Income\", 0.0),\n",
    "        (\"Disability_Rate\", 0.0),\n",
    "        (\"Median_AQI\", 0.0),\n",
    "        (\"s_price_raw\", 0.0),\n",
    "        (\"s_price\", 0.0),\n",
    "        (\"s_reviews\", 0.0),\n",
    "        (\"s_rating\", 0.0),\n",
    "        (\"s_superhost\", 0.0),\n",
    "        (\"risk_score_raw\", 0.0),\n",
    "        (\"final_risk_score\", 0.0),\n",
    "        (\"risk_rank_percentile\", 0.0),\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # CATEGORICAL ONE-HOT FEATURES (8 categories -> 24 one-hot features)\n",
    "    # =============================================================================\n",
    "    CATEGORICAL_ONEHOT_FEATURES = [\n",
    "        (\"room_type\", [\"entire\", \"private\", \"shared\", \"other\"]),\n",
    "        (\"property_type\", [\"home\", \"apartment\", \"condo\", \"other\"]),\n",
    "        (\"amenities_bucket\", [\"0-5\", \"6-10\", \"11-20\", \"20+\"]),\n",
    "        (\"pets_allowed\", [\"true\", \"false\"]),\n",
    "        (\"is_guest_favorite\", [\"true\", \"false\"]),\n",
    "        (\"availability\", [\"true\", \"false\"]),\n",
    "        (\"high_risk\", [\"true\", \"false\"]),\n",
    "        (\"price_sanity_flag\", [\"valid\", \"unrealistic_low\", \"null_price\", \"unknown\"]),\n",
    "    ]\n",
    "\n",
    "    # Count totals\n",
    "    total_text = len(TEXT_DESCRIPTION_FEATURES)\n",
    "    total_location = len(LOCATION_FEATURES)\n",
    "    total_cat_raw = len(CATEGORICAL_RAW_FEATURES)\n",
    "    total_price_discount = len(PRICE_DISCOUNT_FEATURES)\n",
    "    total_review = len(REVIEW_FEATURES)\n",
    "    total_url = len(URL_METADATA_FEATURES)\n",
    "    total_media = len(MEDIA_FEATURES)\n",
    "    total_host = len(HOST_SELLER_FEATURES)\n",
    "    total_other = len(OTHER_FEATURES)\n",
    "    total_numerical = len(NUMERICAL_FEATURES)\n",
    "    total_onehot = sum(len(values) for _, values in CATEGORICAL_ONEHOT_FEATURES)\n",
    "\n",
    "    print(f\"\\nFeature Categories Defined:\")\n",
    "    print(f\"  Text/Description: {total_text}\")\n",
    "    print(f\"  Location: {total_location}\")\n",
    "    print(f\"  Categorical Raw: {total_cat_raw}\")\n",
    "    print(f\"  Price/Discount: {total_price_discount}\")\n",
    "    print(f\"  Review: {total_review}\")\n",
    "    print(f\"  URL/Metadata: {total_url}\")\n",
    "    print(f\"  Media: {total_media}\")\n",
    "    print(f\"  Host/Seller: {total_host}\")\n",
    "    print(f\"  Other: {total_other}\")\n",
    "    print(f\"  Numerical: {total_numerical}\")\n",
    "    print(f\"  Categorical One-Hot: {total_onehot}\")\n",
    "    print(f\"\\n  TOTAL FEATURES: {total_text + total_location + total_cat_raw + total_price_discount + total_review + total_url + total_media + total_host + total_other + total_numerical + total_onehot}\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Text-Derived Recommender Features Definition\n",
    "\n",
    "    # =============================================================================\n",
    "    # TEXT-DERIVED RECOMMENDER FEATURES (15 NEW)\n",
    "    # =============================================================================\n",
    "    # These features are derived from textual analysis of reviews and descriptions\n",
    "    # to help the recommender system provide better apartment suggestions to users\n",
    "\n",
    "    TEXT_DERIVED_FEATURES = [\n",
    "        (\"description_word_count\", 0.0),\n",
    "        (\"description_quality_score\", 0.0),\n",
    "        (\"review_sentiment_positive\", 0.0),\n",
    "        (\"review_sentiment_negative\", 0.0),\n",
    "        (\"cleanliness_score_text\", 0.0),\n",
    "        (\"location_quality_text\", 0.0),\n",
    "        (\"host_responsiveness_text\", 0.0),\n",
    "        (\"value_for_money_text\", 0.0),\n",
    "        (\"family_friendly_score\", 0.0),\n",
    "        (\"business_travel_score\", 0.0),\n",
    "        (\"luxury_indicator_score\", 0.0),\n",
    "        (\"outdoor_space_score\", 0.0),\n",
    "        (\"quietness_score\", 0.0),\n",
    "        (\"accessibility_score\", 0.0),\n",
    "        (\"pet_friendliness_text\", 0.0),\n",
    "    ]\n",
    "\n",
    "    # Keyword dictionaries for text analysis\n",
    "    POSITIVE_SENTIMENT_WORDS = [\n",
    "        \"great\", \"excellent\", \"amazing\", \"wonderful\", \"fantastic\", \"perfect\", \n",
    "        \"lovely\", \"beautiful\", \"clean\", \"comfortable\", \"cozy\", \"spacious\",\n",
    "        \"friendly\", \"helpful\", \"responsive\", \"recommend\", \"loved\", \"enjoyed\",\n",
    "        \"best\", \"outstanding\", \"superb\", \"brilliant\", \"awesome\", \"pleasant\"\n",
    "    ]\n",
    "\n",
    "    NEGATIVE_SENTIMENT_WORDS = [\n",
    "        \"dirty\", \"noisy\", \"loud\", \"smell\", \"smelly\", \"broken\", \"old\", \"worn\",\n",
    "        \"uncomfortable\", \"disappointing\", \"terrible\", \"horrible\", \"awful\", \"bad\",\n",
    "        \"worst\", \"rude\", \"unresponsive\", \"problem\", \"issue\", \"complaint\", \"bugs\",\n",
    "        \"cockroach\", \"mice\", \"mold\", \"stain\", \"unsafe\", \"dangerous\"\n",
    "    ]\n",
    "\n",
    "    CLEANLINESS_WORDS = [\n",
    "        \"clean\", \"spotless\", \"immaculate\", \"tidy\", \"pristine\", \"fresh\",\n",
    "        \"sanitized\", \"hygienic\", \"neat\", \"dirty\", \"dusty\", \"stain\", \"mold\"\n",
    "    ]\n",
    "\n",
    "    LOCATION_QUALITY_WORDS = [\n",
    "        \"central\", \"convenient\", \"walkable\", \"close\", \"nearby\", \"accessible\",\n",
    "        \"quiet neighborhood\", \"safe area\", \"great location\", \"perfect location\",\n",
    "        \"metro\", \"subway\", \"bus stop\", \"restaurant\", \"shop\", \"downtown\"\n",
    "    ]\n",
    "\n",
    "    HOST_RESPONSIVENESS_WORDS = [\n",
    "        \"responsive\", \"quick\", \"fast reply\", \"helpful host\", \"great host\",\n",
    "        \"communicative\", \"attentive\", \"welcoming\", \"accommodating\", \"friendly host\",\n",
    "        \"easy check-in\", \"smooth check-in\", \"flexible\"\n",
    "    ]\n",
    "\n",
    "    VALUE_WORDS = [\n",
    "        \"value\", \"worth\", \"affordable\", \"reasonable\", \"good price\", \"great price\",\n",
    "        \"bargain\", \"deal\", \"expensive\", \"overpriced\", \"cheap\", \"budget\"\n",
    "    ]\n",
    "\n",
    "    FAMILY_FRIENDLY_WORDS = [\n",
    "        \"family\", \"kid\", \"kids\", \"child\", \"children\", \"baby\", \"infant\", \"crib\",\n",
    "        \"high chair\", \"family-friendly\", \"child-friendly\", \"safe for kids\",\n",
    "        \"playground\", \"toys\", \"games\"\n",
    "    ]\n",
    "\n",
    "    BUSINESS_TRAVEL_WORDS = [\n",
    "        \"wifi\", \"wi-fi\", \"workspace\", \"desk\", \"work from home\", \"business\",\n",
    "        \"laptop\", \"office\", \"quiet\", \"work-friendly\", \"fast internet\",\n",
    "        \"dedicated workspace\", \"professional\"\n",
    "    ]\n",
    "\n",
    "    LUXURY_WORDS = [\n",
    "        \"luxury\", \"luxurious\", \"premium\", \"high-end\", \"elegant\", \"designer\",\n",
    "        \"upscale\", \"exclusive\", \"sophisticated\", \"stunning\", \"gorgeous\",\n",
    "        \"marble\", \"modern\", \"stylish\", \"chic\", \"boutique\"\n",
    "    ]\n",
    "\n",
    "    OUTDOOR_SPACE_WORDS = [\n",
    "        \"balcony\", \"terrace\", \"patio\", \"garden\", \"yard\", \"outdoor\", \"deck\",\n",
    "        \"rooftop\", \"pool\", \"bbq\", \"barbecue\", \"grill\", \"courtyard\", \"veranda\"\n",
    "    ]\n",
    "\n",
    "    QUIETNESS_WORDS = [\n",
    "        \"quiet\", \"peaceful\", \"tranquil\", \"serene\", \"silent\", \"calm\",\n",
    "        \"noisy\", \"loud\", \"noise\", \"street noise\", \"traffic\", \"party\"\n",
    "    ]\n",
    "\n",
    "    ACCESSIBILITY_WORDS = [\n",
    "        \"accessible\", \"wheelchair\", \"elevator\", \"lift\", \"ground floor\",\n",
    "        \"step-free\", \"accessible bathroom\", \"grab bars\", \"ramp\", \"disability\",\n",
    "        \"mobility\", \"handicap\"\n",
    "    ]\n",
    "\n",
    "    PET_FRIENDLY_WORDS = [\n",
    "        \"pet\", \"pets\", \"dog\", \"dogs\", \"cat\", \"cats\", \"pet-friendly\",\n",
    "        \"pet friendly\", \"animals\", \"furry\", \"dog park\", \"pet fee\"\n",
    "    ]\n",
    "\n",
    "    total_text_derived = len(TEXT_DERIVED_FEATURES)\n",
    "\n",
    "    print(f\"\\nText-Derived Recommender Features Defined: {total_text_derived}\")\n",
    "    print(\"  These features extract signals from reviews and descriptions\")\n",
    "    print(\"  to improve recommendation quality for users\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Text/Description Feature Extractors\n",
    "\n",
    "    def extract_description(df_col):\n",
    "        \"\"\"Extract full property description as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_description_html(df_col):\n",
    "        \"\"\"Extract HTML version of description as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_description_by_sections(df_col):\n",
    "        \"\"\"Extract sectioned description as JSON string\"\"\"\n",
    "        # Column is already a string, so just coalesce with default\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    def extract_description_items(df_col):\n",
    "        \"\"\"Extract parsed description items as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    def extract_highlights(df_col):\n",
    "        \"\"\"Extract key property highlights as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    def extract_house_rules(df_col):\n",
    "        \"\"\"Extract property house rules as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    def extract_listing_name(df_col):\n",
    "        \"\"\"Extract property listing name as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_listing_title(df_col):\n",
    "        \"\"\"Extract property title as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_details(df_col):\n",
    "        \"\"\"Extract additional property details as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    # Registry for text extractors\n",
    "    TEXT_EXTRACTORS = {\n",
    "        \"description\": extract_description,\n",
    "        \"description_html\": extract_description_html,\n",
    "        \"description_by_sections\": extract_description_by_sections,\n",
    "        \"description_items\": extract_description_items,\n",
    "        \"highlights\": extract_highlights,\n",
    "        \"house_rules\": extract_house_rules,\n",
    "        \"listing_name\": extract_listing_name,\n",
    "        \"listing_title\": extract_listing_title,\n",
    "        \"details\": extract_details,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Text/Description feature extractors defined (9 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Location Feature Extractors\n",
    "\n",
    "    def extract_country(df_col):\n",
    "        \"\"\"Extract country name as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_country_parsed(df_col):\n",
    "        \"\"\"Extract parsed country as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_city(df_col):\n",
    "        \"\"\"Extract city name as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_state(df_col):\n",
    "        \"\"\"Extract state name as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_county(df_col):\n",
    "        \"\"\"Extract county name as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_location(df_col):\n",
    "        \"\"\"Extract location string as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    # Registry for location extractors\n",
    "    LOCATION_EXTRACTORS = {\n",
    "        \"country\": extract_country,\n",
    "        \"country_parsed\": extract_country_parsed,\n",
    "        \"city\": extract_city,\n",
    "        \"state\": extract_state,\n",
    "        \"county\": extract_county,\n",
    "        \"location\": extract_location,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Location feature extractors defined (6 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Categorical Raw Feature Extractors\n",
    "\n",
    "    def extract_category(df_col):\n",
    "        \"\"\"Extract property category as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_category_rating(df_col):\n",
    "        \"\"\"Extract category-level rating as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    # Registry for categorical raw extractors\n",
    "    CATEGORICAL_RAW_EXTRACTORS = {\n",
    "        \"category\": extract_category,\n",
    "        \"category_rating\": extract_category_rating,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Categorical raw feature extractors defined (2 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Price/Discount Feature Extractors\n",
    "\n",
    "    def extract_price_raw(df_col):\n",
    "        \"\"\"Extract raw price as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_discount(df_col):\n",
    "        \"\"\"Extract discount amount/percentage as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    # Registry for price/discount extractors\n",
    "    PRICE_DISCOUNT_EXTRACTORS = {\n",
    "        \"price\": extract_price_raw,\n",
    "        \"discount\": extract_discount,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Price/Discount feature extractors defined (2 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Review Feature Extractors\n",
    "\n",
    "    def extract_reviews(df_col):\n",
    "        \"\"\"Extract full review text/data as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    # Registry for review extractors\n",
    "    REVIEW_EXTRACTORS = {\n",
    "        \"reviews\": extract_reviews,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Review feature extractors defined (1 function)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,URL/Metadata Feature Extractors\n",
    "\n",
    "    def extract_url(df_col):\n",
    "        \"\"\"Extract original listing URL as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_final_url(df_col):\n",
    "        \"\"\"Extract final/processed URL as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_postcode_map_url(df_col):\n",
    "        \"\"\"Extract map URL for postcode as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_breadcrumbs(df_col):\n",
    "        \"\"\"Extract navigation breadcrumbs as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_arrangement_details(df_col):\n",
    "        \"\"\"Extract arrangement/booking details as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    # Registry for URL/metadata extractors\n",
    "    URL_METADATA_EXTRACTORS = {\n",
    "        \"url\": extract_url,\n",
    "        \"final_url\": extract_final_url,\n",
    "        \"postcode_map_url\": extract_postcode_map_url,\n",
    "        \"breadcrumbs\": extract_breadcrumbs,\n",
    "        \"arrangement_details\": extract_arrangement_details,\n",
    "    }\n",
    "\n",
    "    print(\"✓ URL/Metadata feature extractors defined (5 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Media Feature Extractors\n",
    "\n",
    "    def extract_image(df_col):\n",
    "        \"\"\"Extract property image URL as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_images(df_col):\n",
    "        \"\"\"Extract multiple images as JSON array string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    def extract_host_image(df_col):\n",
    "        \"\"\"Extract host profile image URL as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    # Registry for media extractors\n",
    "    MEDIA_EXTRACTORS = {\n",
    "        \"image\": extract_image,\n",
    "        \"images\": extract_images,\n",
    "        \"host_image\": extract_host_image,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Media feature extractors defined (3 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Host/Seller Feature Extractors\n",
    "\n",
    "    def extract_host_details(df_col):\n",
    "        \"\"\"Extract detailed host information as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"{}\"))\n",
    "\n",
    "    def extract_seller_info(df_col):\n",
    "        \"\"\"Extract seller information as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"{}\"))\n",
    "\n",
    "    def extract_name(df_col):\n",
    "        \"\"\"Extract host/listing name as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    # Registry for host/seller extractors\n",
    "    HOST_SELLER_EXTRACTORS = {\n",
    "        \"host_details\": extract_host_details,\n",
    "        \"seller_info\": extract_seller_info,\n",
    "        \"name\": extract_name,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Host/Seller feature extractors defined (3 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Other Feature Extractors\n",
    "\n",
    "    def extract_currency(df_col):\n",
    "        \"\"\"Extract currency code as string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"\"))\n",
    "\n",
    "    def extract_amenities(df_col):\n",
    "        \"\"\"Extract raw amenities data as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"[]\"))\n",
    "\n",
    "    def extract_travel_details(df_col):\n",
    "        \"\"\"Extract travel details as JSON string\"\"\"\n",
    "        return coalesce(col(df_col).cast(StringType()), lit(\"{}\"))\n",
    "\n",
    "    # Registry for other extractors\n",
    "    OTHER_EXTRACTORS = {\n",
    "        \"currency\": extract_currency,\n",
    "        \"amenities\": extract_amenities,\n",
    "        \"travel_details\": extract_travel_details,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Other feature extractors defined (3 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Text-Derived Recommender Feature Extractors\n",
    "\n",
    "    def count_keyword_matches(text_col, keywords_list):\n",
    "        \"\"\"\n",
    "        Count how many keywords from the list appear in the text.\n",
    "        Returns a column expression.\n",
    "        \"\"\"\n",
    "        # Build a regex pattern for all keywords (case-insensitive)\n",
    "        pattern = \"|\".join([f\"\\\\b{word}\\\\b\" for word in keywords_list])\n",
    "        # Use regexp_replace to mark matches, then count\n",
    "        # Alternative: count occurrences using size and split\n",
    "        lowered = lower(coalesce(text_col, lit(\"\")))\n",
    "        # Count matches by checking each keyword\n",
    "        match_count = lit(0)\n",
    "        for keyword in keywords_list:\n",
    "            match_count = match_count + when(\n",
    "                lowered.contains(keyword.lower()), lit(1)\n",
    "            ).otherwise(lit(0))\n",
    "        return match_count\n",
    "\n",
    "    def extract_description_word_count(df):\n",
    "        \"\"\"\n",
    "        Extract word count from description.\n",
    "        Longer descriptions often provide more useful information to users.\n",
    "        \"\"\"\n",
    "        desc_col = coalesce(col(\"description\").cast(StringType()), lit(\"\"))\n",
    "        # Count words by splitting on whitespace\n",
    "        word_count = size(split(trim(desc_col), \"\\\\s+\"))\n",
    "        # Handle empty strings\n",
    "        return when(trim(desc_col) == \"\", lit(\"0.0\")).otherwise(word_count.cast(StringType()))\n",
    "\n",
    "    def extract_description_quality_score(df):\n",
    "        \"\"\"\n",
    "        Score description quality based on length and presence of key info.\n",
    "        Combines description, highlights, and details for comprehensive score.\n",
    "        \"\"\"\n",
    "        desc = coalesce(col(\"description\").cast(StringType()), lit(\"\"))\n",
    "        highlights = coalesce(col(\"highlights\").cast(StringType()), lit(\"\"))\n",
    "        details = coalesce(col(\"details\").cast(StringType()), lit(\"\"))\n",
    "        \n",
    "        combined = concat_ws(\" \", desc, highlights, details)\n",
    "        combined_lower = lower(combined)\n",
    "        \n",
    "        # Base score from length (0-5 points based on word count)\n",
    "        word_count = size(split(trim(combined), \"\\\\s+\"))\n",
    "        length_score = when(word_count > 200, lit(5.0)\n",
    "            ).when(word_count > 100, lit(4.0)\n",
    "            ).when(word_count > 50, lit(3.0)\n",
    "            ).when(word_count > 20, lit(2.0)\n",
    "            ).when(word_count > 0, lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        # Bonus for mentioning key aspects (0-5 points)\n",
    "        info_score = lit(0.0)\n",
    "        info_keywords = [\"bedroom\", \"bathroom\", \"kitchen\", \"wifi\", \"parking\", \n",
    "                        \"check-in\", \"checkout\", \"location\", \"amenities\", \"neighborhood\"]\n",
    "        for kw in info_keywords:\n",
    "            info_score = info_score + when(combined_lower.contains(kw), lit(0.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        total_score = length_score + info_score\n",
    "        # Normalize to 0-10 scale\n",
    "        normalized = when(total_score > 10, lit(10.0)).otherwise(total_score)\n",
    "        \n",
    "        return normalized.cast(StringType())\n",
    "\n",
    "    def extract_review_sentiment_positive(df):\n",
    "        \"\"\"\n",
    "        Count positive sentiment indicators in reviews.\n",
    "        Higher score = more positive guest experiences.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        positive_count = lit(0.0)\n",
    "        for word in POSITIVE_SENTIMENT_WORDS:\n",
    "            positive_count = positive_count + when(\n",
    "                reviews_col.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return positive_count.cast(StringType())\n",
    "\n",
    "    def extract_review_sentiment_negative(df):\n",
    "        \"\"\"\n",
    "        Count negative sentiment indicators in reviews.\n",
    "        Higher score = more complaints/issues reported.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        negative_count = lit(0.0)\n",
    "        for word in NEGATIVE_SENTIMENT_WORDS:\n",
    "            negative_count = negative_count + when(\n",
    "                reviews_col.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return negative_count.cast(StringType())\n",
    "\n",
    "    def extract_cleanliness_score_text(df):\n",
    "        \"\"\"\n",
    "        Extract cleanliness signals from reviews.\n",
    "        Key factor for user satisfaction.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        # Positive cleanliness words add points\n",
    "        clean_score = lit(0.0)\n",
    "        positive_clean = [\"clean\", \"spotless\", \"immaculate\", \"tidy\", \"pristine\", \"fresh\", \"sanitized\"]\n",
    "        negative_clean = [\"dirty\", \"dusty\", \"stain\", \"mold\", \"filthy\", \"grimy\"]\n",
    "        \n",
    "        for word in positive_clean:\n",
    "            clean_score = clean_score + when(reviews_col.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        for word in negative_clean:\n",
    "            clean_score = clean_score - when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        # Normalize: negative scores become 0\n",
    "        final_score = when(clean_score < 0, lit(0.0)).otherwise(clean_score)\n",
    "        \n",
    "        return final_score.cast(StringType())\n",
    "\n",
    "    def extract_location_quality_text(df):\n",
    "        \"\"\"\n",
    "        Extract location quality signals from reviews and description.\n",
    "        Helps users find well-located properties.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        location_score = lit(0.0)\n",
    "        for word in LOCATION_QUALITY_WORDS:\n",
    "            location_score = location_score + when(\n",
    "                combined.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return location_score.cast(StringType())\n",
    "\n",
    "    def extract_host_responsiveness_text(df):\n",
    "        \"\"\"\n",
    "        Extract host responsiveness signals from reviews.\n",
    "        Good hosts = better guest experience.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        host_score = lit(0.0)\n",
    "        for word in HOST_RESPONSIVENESS_WORDS:\n",
    "            host_score = host_score + when(\n",
    "                reviews_col.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return host_score.cast(StringType())\n",
    "\n",
    "    def extract_value_for_money_text(df):\n",
    "        \"\"\"\n",
    "        Extract value perception from reviews.\n",
    "        Helps users find good deals.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        value_score = lit(0.0)\n",
    "        positive_value = [\"value\", \"worth it\", \"affordable\", \"reasonable\", \"good price\", \"great price\", \"bargain\", \"deal\"]\n",
    "        negative_value = [\"expensive\", \"overpriced\", \"not worth\"]\n",
    "        \n",
    "        for word in positive_value:\n",
    "            value_score = value_score + when(reviews_col.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        for word in negative_value:\n",
    "            value_score = value_score - when(reviews_col.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        # Keep as-is (can be negative to indicate poor value)\n",
    "        return value_score.cast(StringType())\n",
    "\n",
    "    def extract_family_friendly_score(df):\n",
    "        \"\"\"\n",
    "        Extract family-friendliness signals.\n",
    "        Helps families find suitable properties.\n",
    "        \"\"\"\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", desc_col, amenities_col, reviews_col)\n",
    "        \n",
    "        family_score = lit(0.0)\n",
    "        for word in FAMILY_FRIENDLY_WORDS:\n",
    "            family_score = family_score + when(\n",
    "                combined.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return family_score.cast(StringType())\n",
    "\n",
    "    def extract_business_travel_score(df):\n",
    "        \"\"\"\n",
    "        Extract business travel suitability signals.\n",
    "        Helps business travelers find work-friendly spaces.\n",
    "        \"\"\"\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        highlights_col = lower(coalesce(col(\"highlights\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", desc_col, amenities_col, highlights_col)\n",
    "        \n",
    "        business_score = lit(0.0)\n",
    "        for word in BUSINESS_TRAVEL_WORDS:\n",
    "            business_score = business_score + when(\n",
    "                combined.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return business_score.cast(StringType())\n",
    "\n",
    "    def extract_luxury_indicator_score(df):\n",
    "        \"\"\"\n",
    "        Extract luxury/premium property signals.\n",
    "        Helps users seeking upscale accommodations.\n",
    "        \"\"\"\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        highlights_col = lower(coalesce(col(\"highlights\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", desc_col, highlights_col)\n",
    "        \n",
    "        luxury_score = lit(0.0)\n",
    "        for word in LUXURY_WORDS:\n",
    "            luxury_score = luxury_score + when(\n",
    "                combined.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return luxury_score.cast(StringType())\n",
    "\n",
    "    def extract_outdoor_space_score(df):\n",
    "        \"\"\"\n",
    "        Extract outdoor space availability signals.\n",
    "        Important for many travelers.\n",
    "        \"\"\"\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", desc_col, amenities_col)\n",
    "        \n",
    "        outdoor_score = lit(0.0)\n",
    "        for word in OUTDOOR_SPACE_WORDS:\n",
    "            outdoor_score = outdoor_score + when(\n",
    "                combined.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return outdoor_score.cast(StringType())\n",
    "\n",
    "    def extract_quietness_score(df):\n",
    "        \"\"\"\n",
    "        Extract quietness/noise level signals.\n",
    "        Net positive = quiet property, negative = noise issues.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        quiet_score = lit(0.0)\n",
    "        positive_quiet = [\"quiet\", \"peaceful\", \"tranquil\", \"serene\", \"silent\", \"calm\"]\n",
    "        negative_quiet = [\"noisy\", \"loud\", \"noise\", \"street noise\", \"traffic noise\", \"party\"]\n",
    "        \n",
    "        for word in positive_quiet:\n",
    "            quiet_score = quiet_score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        for word in negative_quiet:\n",
    "            quiet_score = quiet_score - when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return quiet_score.cast(StringType())\n",
    "\n",
    "    def extract_accessibility_score(df):\n",
    "        \"\"\"\n",
    "        Extract accessibility feature signals.\n",
    "        Helps users with mobility needs.\n",
    "        \"\"\"\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", desc_col, amenities_col)\n",
    "        \n",
    "        accessibility_score = lit(0.0)\n",
    "        for word in ACCESSIBILITY_WORDS:\n",
    "            accessibility_score = accessibility_score + when(\n",
    "                combined.contains(word.lower()), lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        \n",
    "        return accessibility_score.cast(StringType())\n",
    "\n",
    "    def extract_pet_friendliness_text(df):\n",
    "        \"\"\"\n",
    "        Extract pet-friendliness signals beyond the boolean flag.\n",
    "        Helps pet owners find truly welcoming properties.\n",
    "        \"\"\"\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        house_rules_col = lower(coalesce(col(\"house_rules\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", desc_col, house_rules_col, amenities_col)\n",
    "        \n",
    "        pet_score = lit(0.0)\n",
    "        positive_pet = [\"pet friendly\", \"pet-friendly\", \"pets welcome\", \"pets allowed\", \"dog friendly\", \"dog park\"]\n",
    "        negative_pet = [\"no pets\", \"no animals\", \"pet free\"]\n",
    "        \n",
    "        for word in positive_pet:\n",
    "            pet_score = pet_score + when(combined.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        # General pet mentions (weaker signal)\n",
    "        general_pet = [\"pet\", \"dog\", \"cat\"]\n",
    "        for word in general_pet:\n",
    "            pet_score = pet_score + when(combined.contains(word), lit(0.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        for word in negative_pet:\n",
    "            pet_score = pet_score - when(combined.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return pet_score.cast(StringType())\n",
    "\n",
    "    # Registry for text-derived feature extractors\n",
    "    # These extractors take the full dataframe and return column expressions\n",
    "    TEXT_DERIVED_EXTRACTORS = {\n",
    "        \"description_word_count\": extract_description_word_count,\n",
    "        \"description_quality_score\": extract_description_quality_score,\n",
    "        \"review_sentiment_positive\": extract_review_sentiment_positive,\n",
    "        \"review_sentiment_negative\": extract_review_sentiment_negative,\n",
    "        \"cleanliness_score_text\": extract_cleanliness_score_text,\n",
    "        \"location_quality_text\": extract_location_quality_text,\n",
    "        \"host_responsiveness_text\": extract_host_responsiveness_text,\n",
    "        \"value_for_money_text\": extract_value_for_money_text,\n",
    "        \"family_friendly_score\": extract_family_friendly_score,\n",
    "        \"business_travel_score\": extract_business_travel_score,\n",
    "        \"luxury_indicator_score\": extract_luxury_indicator_score,\n",
    "        \"outdoor_space_score\": extract_outdoor_space_score,\n",
    "        \"quietness_score\": extract_quietness_score,\n",
    "        \"accessibility_score\": extract_accessibility_score,\n",
    "        \"pet_friendliness_text\": extract_pet_friendliness_text,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Text-Derived Recommender feature extractors defined (15 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Numerical Feature Extractors (Existing)\n",
    "\n",
    "    def extract_numerical_feature(df_col, default_val=0.0):\n",
    "        \"\"\"Generic numerical feature extractor - returns string representation\"\"\"\n",
    "        return coalesce(col(df_col).cast(DoubleType()).cast(StringType()), lit(str(default_val)))\n",
    "\n",
    "    # Create individual extractors for each numerical feature\n",
    "    def extract_price_numeric(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_total_price(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_log_price(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_bucket_q25_log_price(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_lat(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_long(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_guests(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_amenities_count(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_bucket_size(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_ratings(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_property_number_of_reviews(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_effective_rating(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_host_number_of_reviews(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_host_rating(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_hosts_year(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_host_response_rate(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_is_supperhost(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_city_crime_rate(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_total_fatalities(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_median_income(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_disability_rate(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_median_aqi(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_s_price_raw(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_s_price(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_s_reviews(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_s_rating(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_s_superhost(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_risk_score_raw(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_final_risk_score(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    def extract_risk_rank_percentile(df_col):\n",
    "        return extract_numerical_feature(df_col, 0.0)\n",
    "\n",
    "    # Registry for numerical extractors\n",
    "    NUMERICAL_EXTRACTORS = {\n",
    "        \"price_numeric\": extract_price_numeric,\n",
    "        \"total_price\": extract_total_price,\n",
    "        \"log_price\": extract_log_price,\n",
    "        \"bucket_q25_log_price\": extract_bucket_q25_log_price,\n",
    "        \"lat\": extract_lat,\n",
    "        \"long\": extract_long,\n",
    "        \"guests\": extract_guests,\n",
    "        \"amenities_count\": extract_amenities_count,\n",
    "        \"bucket_size\": extract_bucket_size,\n",
    "        \"ratings\": extract_ratings,\n",
    "        \"property_number_of_reviews\": extract_property_number_of_reviews,\n",
    "        \"effective_rating\": extract_effective_rating,\n",
    "        \"host_number_of_reviews\": extract_host_number_of_reviews,\n",
    "        \"host_rating\": extract_host_rating,\n",
    "        \"hosts_year\": extract_hosts_year,\n",
    "        \"host_response_rate\": extract_host_response_rate,\n",
    "        \"is_supperhost\": extract_is_supperhost,\n",
    "        \"City_Crime_Rate_Per_100K\": extract_city_crime_rate,\n",
    "        \"Total_Fatalities\": extract_total_fatalities,\n",
    "        \"Median_Income\": extract_median_income,\n",
    "        \"Disability_Rate\": extract_disability_rate,\n",
    "        \"Median_AQI\": extract_median_aqi,\n",
    "        \"s_price_raw\": extract_s_price_raw,\n",
    "        \"s_price\": extract_s_price,\n",
    "        \"s_reviews\": extract_s_reviews,\n",
    "        \"s_rating\": extract_s_rating,\n",
    "        \"s_superhost\": extract_s_superhost,\n",
    "        \"risk_score_raw\": extract_risk_score_raw,\n",
    "        \"final_risk_score\": extract_final_risk_score,\n",
    "        \"risk_rank_percentile\": extract_risk_rank_percentile,\n",
    "    }\n",
    "\n",
    "    print(\"✓ Numerical feature extractors defined (30 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Categorical One-Hot Feature Extractors (Existing)\n",
    "\n",
    "    # Room Type\n",
    "    def extract_room_type_entire(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"entire\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_room_type_private(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"private\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_room_type_shared(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"shared\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_room_type_other(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"other\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    # Property Type\n",
    "    def extract_property_type_home(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"home\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_property_type_apartment(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"apartment\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_property_type_condo(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"condo\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_property_type_other(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"other\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    # Amenities Bucket\n",
    "    def extract_amenities_bucket_0_5(df_col):\n",
    "        return when(col(df_col) == \"0-5\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_amenities_bucket_6_10(df_col):\n",
    "        return when(col(df_col) == \"6-10\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_amenities_bucket_11_20(df_col):\n",
    "        return when(col(df_col) == \"11-20\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_amenities_bucket_20_plus(df_col):\n",
    "        return when(col(df_col) == \"20+\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    # Boolean Features\n",
    "    def extract_pets_allowed_true(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"true\") | (col(df_col) == True),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_pets_allowed_false(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"false\") | (col(df_col) == False) | col(df_col).isNull(),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_is_guest_favorite_true(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"true\") | (col(df_col) == True),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_is_guest_favorite_false(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"false\") | (col(df_col) == False) | col(df_col).isNull(),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_availability_true(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"true\") | (col(df_col) == True),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_availability_false(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"false\") | (col(df_col) == False) | col(df_col).isNull(),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_high_risk_true(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"true\") | (col(df_col) == True),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_high_risk_false(df_col):\n",
    "        return when(\n",
    "            (lower(col(df_col).cast(StringType())) == \"false\") | (col(df_col) == False) | col(df_col).isNull(),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    # Price Sanity Flag\n",
    "    def extract_price_sanity_valid(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"valid\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_price_sanity_unrealistic_low(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"unrealistic_low\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_price_sanity_null_price(df_col):\n",
    "        return when(lower(trim(col(df_col))) == \"null_price\", lit(\"1.0\")).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    def extract_price_sanity_unknown(df_col):\n",
    "        return when(\n",
    "            (lower(trim(col(df_col))) == \"unknown\") | col(df_col).isNull(),\n",
    "            lit(\"1.0\")\n",
    "        ).otherwise(lit(\"0.0\"))\n",
    "\n",
    "    # Registry for categorical one-hot extractors\n",
    "    # Format: {output_feature_name: (source_column, extractor_function)}\n",
    "    CATEGORICAL_ONEHOT_EXTRACTORS = {\n",
    "        \"room_type_entire\": (\"room_type\", extract_room_type_entire),\n",
    "        \"room_type_private\": (\"room_type\", extract_room_type_private),\n",
    "        \"room_type_shared\": (\"room_type\", extract_room_type_shared),\n",
    "        \"room_type_other\": (\"room_type\", extract_room_type_other),\n",
    "        \"property_type_home\": (\"property_type\", extract_property_type_home),\n",
    "        \"property_type_apartment\": (\"property_type\", extract_property_type_apartment),\n",
    "        \"property_type_condo\": (\"property_type\", extract_property_type_condo),\n",
    "        \"property_type_other\": (\"property_type\", extract_property_type_other),\n",
    "        \"amenities_bucket_0_5\": (\"amenities_bucket\", extract_amenities_bucket_0_5),\n",
    "        \"amenities_bucket_6_10\": (\"amenities_bucket\", extract_amenities_bucket_6_10),\n",
    "        \"amenities_bucket_11_20\": (\"amenities_bucket\", extract_amenities_bucket_11_20),\n",
    "        \"amenities_bucket_20_plus\": (\"amenities_bucket\", extract_amenities_bucket_20_plus),\n",
    "        \"pets_allowed_true\": (\"pets_allowed\", extract_pets_allowed_true),\n",
    "        \"pets_allowed_false\": (\"pets_allowed\", extract_pets_allowed_false),\n",
    "        \"is_guest_favorite_true\": (\"is_guest_favorite\", extract_is_guest_favorite_true),\n",
    "        \"is_guest_favorite_false\": (\"is_guest_favorite\", extract_is_guest_favorite_false),\n",
    "        \"availability_true\": (\"availability\", extract_availability_true),\n",
    "        \"availability_false\": (\"availability\", extract_availability_false),\n",
    "        \"high_risk_true\": (\"high_risk\", extract_high_risk_true),\n",
    "        \"high_risk_false\": (\"high_risk\", extract_high_risk_false),\n",
    "        \"price_sanity_valid\": (\"price_sanity_flag\", extract_price_sanity_valid),\n",
    "        \"price_sanity_unrealistic_low\": (\"price_sanity_flag\", extract_price_sanity_unrealistic_low),\n",
    "        \"price_sanity_null_price\": (\"price_sanity_flag\", extract_price_sanity_null_price),\n",
    "        \"price_sanity_unknown\": (\"price_sanity_flag\", extract_price_sanity_unknown),\n",
    "    }\n",
    "\n",
    "    print(\"✓ Categorical one-hot feature extractors defined (24 functions)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Validate Features Against DataFrame\n",
    "\n",
    "    def validate_features_against_df(df, extractors_dict, feature_type=\"\"):\n",
    "        \"\"\"\n",
    "        Validate that source columns exist in the dataframe.\n",
    "        Returns dict of valid extractors.\n",
    "        \"\"\"\n",
    "        df_columns = set(df.columns)\n",
    "        valid = {}\n",
    "        missing = []\n",
    "        \n",
    "        for feature_name, extractor in extractors_dict.items():\n",
    "            # For one-hot extractors, extractor is (source_col, func)\n",
    "            if isinstance(extractor, tuple):\n",
    "                source_col = extractor[0]\n",
    "            else:\n",
    "                source_col = feature_name\n",
    "            \n",
    "            if source_col in df_columns:\n",
    "                valid[feature_name] = extractor\n",
    "            else:\n",
    "                missing.append(source_col)\n",
    "        \n",
    "        if missing:\n",
    "            unique_missing = list(set(missing))\n",
    "            print(f\"  ⚠️  {feature_type}: {len(unique_missing)} source columns not found: {unique_missing[:5]}{'...' if len(unique_missing) > 5 else ''}\")\n",
    "        \n",
    "        print(f\"  ✓ {feature_type}: {len(valid)} valid features\")\n",
    "        return valid\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATING FEATURES AGAINST df_reloaded\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Validate all feature categories\n",
    "    VALID_TEXT = validate_features_against_df(df_reloaded, TEXT_EXTRACTORS, \"Text/Description\")\n",
    "    VALID_LOCATION = validate_features_against_df(df_reloaded, LOCATION_EXTRACTORS, \"Location\")\n",
    "    VALID_CATEGORICAL_RAW = validate_features_against_df(df_reloaded, CATEGORICAL_RAW_EXTRACTORS, \"Categorical Raw\")\n",
    "    VALID_PRICE_DISCOUNT = validate_features_against_df(df_reloaded, PRICE_DISCOUNT_EXTRACTORS, \"Price/Discount\")\n",
    "    VALID_REVIEW = validate_features_against_df(df_reloaded, REVIEW_EXTRACTORS, \"Review\")\n",
    "    VALID_URL_METADATA = validate_features_against_df(df_reloaded, URL_METADATA_EXTRACTORS, \"URL/Metadata\")\n",
    "    VALID_MEDIA = validate_features_against_df(df_reloaded, MEDIA_EXTRACTORS, \"Media\")\n",
    "    VALID_HOST_SELLER = validate_features_against_df(df_reloaded, HOST_SELLER_EXTRACTORS, \"Host/Seller\")\n",
    "    VALID_OTHER = validate_features_against_df(df_reloaded, OTHER_EXTRACTORS, \"Other\")\n",
    "    VALID_NUMERICAL = validate_features_against_df(df_reloaded, NUMERICAL_EXTRACTORS, \"Numerical\")\n",
    "    VALID_ONEHOT = validate_features_against_df(df_reloaded, CATEGORICAL_ONEHOT_EXTRACTORS, \"Categorical One-Hot\")\n",
    "\n",
    "    # Validate text-derived features (these use multiple source columns)\n",
    "    def validate_text_derived_features(df):\n",
    "        \"\"\"\n",
    "        Validate text-derived features. These use multiple source columns\n",
    "        so we check if the key source columns exist.\n",
    "        \"\"\"\n",
    "        df_columns = set(df.columns)\n",
    "        required_cols = [\"description\", \"reviews\", \"highlights\", \"details\", \"amenities\", \"house_rules\"]\n",
    "        \n",
    "        missing = [c for c in required_cols if c not in df_columns]\n",
    "        available = [c for c in required_cols if c in df_columns]\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"  ⚠️  Text-Derived: Some source columns not found: {missing}\")\n",
    "        \n",
    "        # All text-derived extractors are valid if at least description and reviews exist\n",
    "        valid_extractors = {}\n",
    "        if \"description\" in df_columns or \"reviews\" in df_columns:\n",
    "            valid_extractors = TEXT_DERIVED_EXTRACTORS.copy()\n",
    "        \n",
    "        print(f\"  ✓ Text-Derived: {len(valid_extractors)} features (using: {available})\")\n",
    "        return valid_extractors\n",
    "\n",
    "    VALID_TEXT_DERIVED = validate_text_derived_features(df_reloaded)\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Build Comprehensive Features Map\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Build Comprehensive Features Map\n",
    "\n",
    "    def build_comprehensive_features_map(df, \n",
    "                                        valid_text,\n",
    "                                        valid_location,\n",
    "                                        valid_categorical_raw,\n",
    "                                        valid_price_discount,\n",
    "                                        valid_review,\n",
    "                                        valid_url_metadata,\n",
    "                                        valid_media,\n",
    "                                        valid_host_seller,\n",
    "                                        valid_other,\n",
    "                                        valid_numerical,\n",
    "                                        valid_onehot,\n",
    "                                        valid_text_derived):\n",
    "        \"\"\"\n",
    "        Build a single map column containing ALL features.\n",
    "        All values are stored as StringType for uniformity.\n",
    "        \n",
    "        Returns:\n",
    "            Column expression for comprehensive features map\n",
    "        \"\"\"\n",
    "        map_args = []\n",
    "        \n",
    "        # 1. Text/Description Features\n",
    "        for feature_name, extractor_func in valid_text.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 2. Location Features\n",
    "        for feature_name, extractor_func in valid_location.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 3. Categorical Raw Features\n",
    "        for feature_name, extractor_func in valid_categorical_raw.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 4. Price/Discount Features\n",
    "        for feature_name, extractor_func in valid_price_discount.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 5. Review Features\n",
    "        for feature_name, extractor_func in valid_review.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 6. URL/Metadata Features\n",
    "        for feature_name, extractor_func in valid_url_metadata.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 7. Media Features\n",
    "        for feature_name, extractor_func in valid_media.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 8. Host/Seller Features\n",
    "        for feature_name, extractor_func in valid_host_seller.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 9. Other Features\n",
    "        for feature_name, extractor_func in valid_other.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 10. Numerical Features\n",
    "        for feature_name, extractor_func in valid_numerical.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(feature_name))\n",
    "        \n",
    "        # 11. Categorical One-Hot Features\n",
    "        for output_name, (source_col, extractor_func) in valid_onehot.items():\n",
    "            map_args.append(lit(output_name))\n",
    "            map_args.append(extractor_func(source_col))\n",
    "        \n",
    "        # 12. Text-Derived Recommender Features (NEW)\n",
    "        for feature_name, extractor_func in valid_text_derived.items():\n",
    "            map_args.append(lit(feature_name))\n",
    "            map_args.append(extractor_func(df))\n",
    "        \n",
    "        # Create the map column\n",
    "        features_map_col = create_map(*map_args)\n",
    "        \n",
    "        return features_map_col\n",
    "\n",
    "    print(\"✓ Comprehensive features map builder defined (updated with text-derived features)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Create Features Vector DataFrame\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING FEATURES VECTOR DATAFRAME\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Build the comprehensive features map expression\n",
    "    print(\"\\n1. Building comprehensive features map expression...\")\n",
    "    features_map_expr = build_comprehensive_features_map(\n",
    "        df_reloaded,\n",
    "        VALID_TEXT,\n",
    "        VALID_LOCATION,\n",
    "        VALID_CATEGORICAL_RAW,\n",
    "        VALID_PRICE_DISCOUNT,\n",
    "        VALID_REVIEW,\n",
    "        VALID_URL_METADATA,\n",
    "        VALID_MEDIA,\n",
    "        VALID_HOST_SELLER,\n",
    "        VALID_OTHER,\n",
    "        VALID_NUMERICAL,\n",
    "        VALID_ONEHOT,\n",
    "        VALID_TEXT_DERIVED  # NEW PARAMETER\n",
    "    )\n",
    "    print(\"   ✓ Features map expression created\")\n",
    "\n",
    "    # Create the features vector dataframe\n",
    "    print(\"\\n2. Creating features vector dataframe...\")\n",
    "    df_features_vector = df_reloaded.select(\n",
    "        col(\"property_id\").alias(\"listing_id\"),\n",
    "        features_map_expr.alias(\"features_dict\")\n",
    "    )\n",
    "\n",
    "    # Cache for performance if needed multiple times\n",
    "    df_features_vector = df_features_vector.persist()\n",
    "\n",
    "    print(\"   ✓ Features vector dataframe created and cached\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Create Feature Names Mapping\n",
    "\n",
    "    # Create comprehensive mapping of all feature names\n",
    "    FEATURE_NAMES_MAPPING = {}\n",
    "    idx = 0\n",
    "\n",
    "    # Track categories for reference\n",
    "    FEATURE_CATEGORIES = {\n",
    "        \"text_description\": [],\n",
    "        \"location\": [],\n",
    "        \"categorical_raw\": [],\n",
    "        \"price_discount\": [],\n",
    "        \"review\": [],\n",
    "        \"url_metadata\": [],\n",
    "        \"media\": [],\n",
    "        \"host_seller\": [],\n",
    "        \"other\": [],\n",
    "        \"numerical\": [],\n",
    "        \"categorical_onehot\": [],\n",
    "        \"text_derived\": [],  # NEW CATEGORY\n",
    "\n",
    "    }\n",
    "\n",
    "    # Add features by category\n",
    "    for feature_name in VALID_TEXT.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"text_description\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_LOCATION.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"location\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_CATEGORICAL_RAW.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"categorical_raw\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_PRICE_DISCOUNT.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"price_discount\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_REVIEW.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"review\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_URL_METADATA.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"url_metadata\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_MEDIA.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"media\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_HOST_SELLER.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"host_seller\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_OTHER.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"other\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_NUMERICAL.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"numerical\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    for feature_name in VALID_ONEHOT.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"categorical_onehot\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    # Add text-derived features to mapping\n",
    "    for feature_name in VALID_TEXT_DERIVED.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = idx\n",
    "        FEATURE_CATEGORIES[\"text_derived\"] = FEATURE_CATEGORIES.get(\"text_derived\", [])\n",
    "        FEATURE_CATEGORIES[\"text_derived\"].append(feature_name)\n",
    "        idx += 1\n",
    "\n",
    "    print(\"\\nFeature Names Mapping Created:\")\n",
    "    print(\"-\" * 60)\n",
    "    for category, features in FEATURE_CATEGORIES.items():\n",
    "        if features:\n",
    "            print(f\"\\n{category.upper()} ({len(features)} features):\")\n",
    "            for f in features[:5]:\n",
    "                print(f\"  {FEATURE_NAMES_MAPPING[f]:3d}: {f}\")\n",
    "            if len(features) > 5:\n",
    "                print(f\"  ... and {len(features) - 5} more\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Verify Output Schema\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OUTPUT VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nSchema of df_features_vector:\")\n",
    "    df_features_vector.printSchema()\n",
    "\n",
    "    total_features = len(FEATURE_NAMES_MAPPING)\n",
    "    print(f\"\\nTotal features in dictionary: {total_features}\")\n",
    "\n",
    "    print(\"\\nFeatures by category:\")\n",
    "    for category, features in FEATURE_CATEGORIES.items():\n",
    "        print(f\"  {category}: {len(features)}\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Display Sample Output\n",
    "\n",
    "    print(\"\\nSample output (first 3 rows):\")\n",
    "    display(df_features_vector.limit(3))\n",
    "\n",
    "    # Show a single record in detail\n",
    "    print(\"\\nDetailed view of single record features:\")\n",
    "    sample_row = df_features_vector.limit(1).collect()[0]\n",
    "    print(f\"\\nListing ID: {sample_row['listing_id']}\")\n",
    "    print(f\"\\nFeatures Dictionary ({len(sample_row['features_dict'])} features):\")\n",
    "\n",
    "    # Show sample of each category\n",
    "    for category, features in FEATURE_CATEGORIES.items():\n",
    "        if features:\n",
    "            print(f\"\\n--- {category.upper()} ---\")\n",
    "            for f in features[:3]:\n",
    "                value = sample_row['features_dict'].get(f, 'N/A')\n",
    "                # Truncate long values for display\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    value = value[:100] + \"...\"\n",
    "                print(f\"  {f}: {value}\")\n",
    "            if len(features) > 3:\n",
    "                print(f\"  ... and {len(features) - 3} more features\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Summary Statistics\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Count total rows (use action to trigger computation)\n",
    "    total_rows = df_features_vector.count()\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Pipeline Configuration:\n",
    "    - Source DataFrame: df_reloaded\n",
    "    - Output DataFrame: df_features_vector\n",
    "    \n",
    "    Output Schema:\n",
    "    - listing_id: string (primary key from property_id)\n",
    "    - features_dict: map<string, string> (dictionary of all feature values)\n",
    "\n",
    "    Features Extracted ({total_features} total):\n",
    "\n",
    "    TEXT/DESCRIPTION ({len(FEATURE_CATEGORIES['text_description'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['text_description'][:5])}{'...' if len(FEATURE_CATEGORIES['text_description']) > 5 else ''}\n",
    "\n",
    "    LOCATION ({len(FEATURE_CATEGORIES['location'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['location'])}\n",
    "\n",
    "    CATEGORICAL RAW ({len(FEATURE_CATEGORIES['categorical_raw'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['categorical_raw'])}\n",
    "\n",
    "    PRICE/DISCOUNT ({len(FEATURE_CATEGORIES['price_discount'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['price_discount'])}\n",
    "\n",
    "    REVIEW ({len(FEATURE_CATEGORIES['review'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['review'])}\n",
    "\n",
    "    URL/METADATA ({len(FEATURE_CATEGORIES['url_metadata'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['url_metadata'])}\n",
    "\n",
    "    MEDIA ({len(FEATURE_CATEGORIES['media'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['media'])}\n",
    "\n",
    "    HOST/SELLER ({len(FEATURE_CATEGORIES['host_seller'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['host_seller'])}\n",
    "\n",
    "    OTHER ({len(FEATURE_CATEGORIES['other'])} features):\n",
    "        {', '.join(FEATURE_CATEGORIES['other'])}\n",
    "\n",
    "    NUMERICAL ({len(FEATURE_CATEGORIES['numerical'])} features):\n",
    "        Price, Location, Property, Ratings, Host, Environmental, Risk scores\n",
    "\n",
    "    CATEGORICAL ONE-HOT ({len(FEATURE_CATEGORIES['categorical_onehot'])} features):\n",
    "        Room type, Property type, Amenities bucket, Boolean flags, Price sanity\n",
    "\n",
    "        TEXT-DERIVED RECOMMENDER ({len(FEATURE_CATEGORIES.get('text_derived', []))} features):\n",
    "        Sentiment analysis, cleanliness, location quality, host responsiveness,\n",
    "        value perception, family/business suitability, luxury indicators,\n",
    "        outdoor space, quietness, accessibility, pet-friendliness\n",
    "\n",
    "    Total Listings Processed: {total_rows:,}\n",
    "\n",
    "    Mappings Available:\n",
    "    - FEATURE_NAMES_MAPPING: feature_name -> index ({len(FEATURE_NAMES_MAPPING)} entries)\n",
    "    - FEATURE_CATEGORIES: category -> list of feature names\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"✓ Pipeline complete!\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Output Reference\n",
    "\n",
    "    # =============================================================================\n",
    "    # FINAL OUTPUT DATAFRAMES AND MAPPINGS\n",
    "    # =============================================================================\n",
    "\n",
    "    # 1. df_features_vector - Main output DataFrame\n",
    "    #    Schema: listing_id (string), features_dict (map<string, string>)\n",
    "    #    Contains ALL features: text, location, categorical, numerical, one-hot\n",
    "\n",
    "    # 2. FEATURE_NAMES_MAPPING - Dict mapping feature names to indices\n",
    "    #    Usage: FEATURE_NAMES_MAPPING['description'] -> 0\n",
    "    #           FEATURE_NAMES_MAPPING['price_numeric'] -> 45\n",
    "\n",
    "    # 3. FEATURE_CATEGORIES - Dict mapping category names to feature lists\n",
    "    #    Usage: FEATURE_CATEGORIES['numerical'] -> ['price_numeric', 'lat', ...]\n",
    "\n",
    "    # 4. Valid feature registries by category:\n",
    "    #    - VALID_TEXT, VALID_LOCATION, VALID_CATEGORICAL_RAW\n",
    "    #    - VALID_PRICE_DISCOUNT, VALID_REVIEW, VALID_URL_METADATA\n",
    "    #    - VALID_MEDIA, VALID_HOST_SELLER, VALID_OTHER\n",
    "    #    - VALID_NUMERICAL, VALID_ONEHOT\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"OUTPUT REFERENCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nMain Output:\")\n",
    "    print(\"  • df_features_vector - DataFrame with listing_id and features_dict\")\n",
    "    print(f\"    Rows: {total_rows:,}\")\n",
    "    print(f\"    Features per row: {total_features}\")\n",
    "\n",
    "    print(\"\\nMappings:\")\n",
    "    print(\"  • FEATURE_NAMES_MAPPING - feature_name -> index\")\n",
    "    print(\"  • FEATURE_CATEGORIES - category -> list of features\")\n",
    "\n",
    "    print(\"\\nData Types in features_dict:\")\n",
    "    print(\"  • Text fields: stored as strings\")\n",
    "    print(\"  • Numerical fields: stored as string representation of numbers\")\n",
    "    print(\"  • Arrays/Lists: stored as JSON strings\")\n",
    "    print(\"  • Nested objects: stored as JSON strings\")\n",
    "    print(\"  • One-hot encoded: stored as '1.0' or '0.0'\")\n",
    "\n",
    "    print(\"\\n✓ Ready for downstream processing!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9c83f9c1-f272-4a71-935f-8a36d0740440",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "User-Interest Features and Composite Scores - Definitions"
    }
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    from pyspark.sql.functions import col, lit, when, lower, coalesce, concat_ws, create_map, map_concat\n",
    "    from pyspark.sql.types import StringType\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,User-Interest Features and Composite Scores - Definitions\n",
    "\n",
    "    # =============================================================================\n",
    "    # 30 USER-INTEREST FEATURES\n",
    "    # =============================================================================\n",
    "    # These features are specifically designed to help users understand key aspects\n",
    "    # of the apartment they are considering renting on Airbnb\n",
    "\n",
    "    USER_INTEREST_FEATURES = [\n",
    "        # Comfort & Quality Features (1-6)\n",
    "        (\"bed_comfort_score\", 0.0),           # Mentions of bed/sleep quality\n",
    "        (\"bathroom_quality_score\", 0.0),      # Bathroom condition signals\n",
    "        (\"kitchen_quality_score\", 0.0),       # Kitchen usability signals\n",
    "        (\"temperature_control_score\", 0.0),   # AC/heating mentions\n",
    "        (\"natural_light_score\", 0.0),         # Light/brightness mentions\n",
    "        (\"space_efficiency_score\", 0.0),      # Space/room size perception\n",
    "        \n",
    "        # Location & Neighborhood Features (7-12)\n",
    "        (\"public_transport_score\", 0.0),      # Transit accessibility\n",
    "        (\"walkability_score\", 0.0),           # Walking distance to amenities\n",
    "        (\"nightlife_proximity_score\", 0.0),   # Bars/clubs/entertainment nearby\n",
    "        (\"restaurant_proximity_score\", 0.0),  # Dining options nearby\n",
    "        (\"grocery_proximity_score\", 0.0),     # Supermarket/store access\n",
    "        (\"neighborhood_safety_score\", 0.0),   # Safety perception from reviews\n",
    "        \n",
    "        # Experience & Atmosphere Features (13-18)\n",
    "        (\"view_quality_score\", 0.0),          # View mentions (city, ocean, etc.)\n",
    "        (\"privacy_level_score\", 0.0),         # Privacy vs shared space\n",
    "        (\"local_authenticity_score\", 0.0),    # Local/authentic experience\n",
    "        (\"modern_amenities_score\", 0.0),      # Modern/updated features\n",
    "        (\"aesthetic_appeal_score\", 0.0),      # Design/style/decor mentions\n",
    "        (\"noise_insulation_score\", 0.0),      # Soundproofing quality\n",
    "        \n",
    "        # Service & Process Features (19-24)\n",
    "        (\"checkin_ease_score\", 0.0),          # Check-in process smoothness\n",
    "        (\"listing_accuracy_score\", 0.0),      # Matches description/photos\n",
    "        (\"communication_quality_score\", 0.0), # Host communication quality\n",
    "        (\"response_speed_score\", 0.0),        # How fast host responds\n",
    "        (\"flexibility_score\", 0.0),           # Flexible policies\n",
    "        (\"local_tips_score\", 0.0),            # Host provides local recommendations\n",
    "        \n",
    "        # Trip Type Suitability Features (25-30)\n",
    "        (\"solo_traveler_score\", 0.0),         # Good for solo travelers\n",
    "        (\"couple_romantic_score\", 0.0),       # Good for couples/romantic trips\n",
    "        (\"group_gathering_score\", 0.0),       # Good for groups/parties\n",
    "        (\"long_stay_score\", 0.0),             # Suitable for extended stays\n",
    "        (\"first_time_visitor_score\", 0.0),    # Good for first-time visitors\n",
    "        (\"repeat_guest_indicator\", 0.0),      # Mentions of return visits\n",
    "    ]\n",
    "\n",
    "    # =============================================================================\n",
    "    # 10 COMPOSITE SCORES\n",
    "    # =============================================================================\n",
    "    # These aggregate scores provide high-level insights across multiple dimensions\n",
    "\n",
    "    COMPOSITE_SCORES = [\n",
    "        (\"overall_quality_score\", 0.0),       # Overall apartment quality\n",
    "        (\"location_convenience_score\", 0.0),  # Location & accessibility\n",
    "        (\"host_excellence_score\", 0.0),       # Host quality & service\n",
    "        (\"value_perception_score\", 0.0),      # Value for money\n",
    "        (\"comfort_index_score\", 0.0),         # Physical comfort\n",
    "        (\"family_suitability_score\", 0.0),    # Family-friendliness\n",
    "        (\"business_ready_score\", 0.0),        # Work/business suitability\n",
    "        (\"social_experience_score\", 0.0),     # Social/entertainment\n",
    "        (\"relaxation_wellness_score\", 0.0),   # Peace & relaxation\n",
    "        (\"local_immersion_score\", 0.0),       # Authentic local experience\n",
    "    ]\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"USER-INTEREST FEATURES AND COMPOSITE SCORES\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"User-Interest Features: {len(USER_INTEREST_FEATURES)}\")\n",
    "    print(f\"Composite Scores: {len(COMPOSITE_SCORES)}\")\n",
    "    print(f\"Total New Features: {len(USER_INTEREST_FEATURES) + len(COMPOSITE_SCORES)}\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Keyword Dictionaries for User-Interest Features\n",
    "\n",
    "    # =============================================================================\n",
    "    # KEYWORD DICTIONARIES FOR FEATURE EXTRACTION\n",
    "    # =============================================================================\n",
    "\n",
    "    # Comfort & Quality Keywords\n",
    "    BED_COMFORT_WORDS_POS = [\"comfortable bed\", \"great sleep\", \"slept well\", \"comfy bed\", \n",
    "                            \"soft mattress\", \"quality mattress\", \"cozy bed\", \"restful sleep\",\n",
    "                            \"good night sleep\", \"plush\", \"memory foam\"]\n",
    "    BED_COMFORT_WORDS_NEG = [\"uncomfortable bed\", \"hard mattress\", \"bad sleep\", \"couldn't sleep\",\n",
    "                            \"lumpy\", \"squeaky bed\", \"thin mattress\", \"back pain\"]\n",
    "\n",
    "    BATHROOM_WORDS_POS = [\"clean bathroom\", \"nice bathroom\", \"great shower\", \"good water pressure\",\n",
    "                        \"modern bathroom\", \"spacious bathroom\", \"hot water\", \"fresh towels\",\n",
    "                        \"bathroom amenities\", \"toiletries provided\"]\n",
    "    BATHROOM_WORDS_NEG = [\"dirty bathroom\", \"mold\", \"no hot water\", \"weak shower\", \"small bathroom\",\n",
    "                        \"bathroom issues\", \"plumbing problems\", \"broken toilet\"]\n",
    "\n",
    "    KITCHEN_WORDS_POS = [\"full kitchen\", \"well-equipped kitchen\", \"cooking\", \"great kitchen\",\n",
    "                        \"modern kitchen\", \"kitchen amenities\", \"pots and pans\", \"coffee maker\",\n",
    "                        \"fully stocked\", \"appliances\", \"dishwasher\"]\n",
    "    KITCHEN_WORDS_NEG = [\"no kitchen\", \"limited kitchen\", \"dirty kitchen\", \"missing utensils\",\n",
    "                        \"broken appliances\", \"no cooking\"]\n",
    "\n",
    "    TEMPERATURE_WORDS_POS = [\"great ac\", \"good air conditioning\", \"comfortable temperature\",\n",
    "                            \"central air\", \"heating works\", \"warm in winter\", \"cool in summer\",\n",
    "                            \"climate control\", \"thermostat\"]\n",
    "    TEMPERATURE_WORDS_NEG = [\"no ac\", \"broken ac\", \"too hot\", \"too cold\", \"no heating\",\n",
    "                            \"freezing\", \"sweltering\", \"stuffy\"]\n",
    "\n",
    "    NATURAL_LIGHT_WORDS = [\"bright\", \"natural light\", \"sunny\", \"sunlight\", \"windows\",\n",
    "                        \"light-filled\", \"airy\", \"well-lit\", \"morning light\", \"sunset view\"]\n",
    "\n",
    "    SPACE_WORDS_POS = [\"spacious\", \"roomy\", \"large\", \"plenty of space\", \"open floor plan\",\n",
    "                    \"generous space\", \"big rooms\", \"lots of room\"]\n",
    "    SPACE_WORDS_NEG = [\"cramped\", \"tiny\", \"small\", \"tight\", \"compact\", \"no space\", \"cluttered\"]\n",
    "\n",
    "    # Location & Neighborhood Keywords\n",
    "    TRANSPORT_WORDS = [\"metro\", \"subway\", \"bus stop\", \"train station\", \"public transport\",\n",
    "                    \"transit\", \"uber\", \"lyft\", \"tram\", \"bus line\", \"underground\",\n",
    "                    \"easy to get around\", \"well connected\"]\n",
    "\n",
    "    WALKABILITY_WORDS = [\"walkable\", \"walking distance\", \"steps away\", \"walk to\", \"on foot\",\n",
    "                        \"pedestrian\", \"stroll\", \"within walking\", \"easy walk\", \"short walk\"]\n",
    "\n",
    "    NIGHTLIFE_WORDS = [\"bars\", \"clubs\", \"nightlife\", \"nightclub\", \"pub\", \"live music\",\n",
    "                    \"entertainment\", \"party area\", \"vibrant\", \"lively area\", \"happening\"]\n",
    "\n",
    "    RESTAURANT_WORDS = [\"restaurants\", \"cafes\", \"dining\", \"eateries\", \"food options\",\n",
    "                        \"coffee shops\", \"brunch\", \"breakfast spot\", \"great food nearby\",\n",
    "                        \"foodie\", \"cuisine\", \"bistro\"]\n",
    "\n",
    "    GROCERY_WORDS = [\"grocery\", \"supermarket\", \"convenience store\", \"market\", \"shopping\",\n",
    "                    \"groceries nearby\", \"food store\", \"whole foods\", \"trader joe\"]\n",
    "\n",
    "    SAFETY_WORDS_POS = [\"safe\", \"secure\", \"safe area\", \"safe neighborhood\", \"well-lit streets\",\n",
    "                        \"security\", \"gated\", \"doorman\", \"secure building\", \"felt safe\"]\n",
    "    SAFETY_WORDS_NEG = [\"unsafe\", \"sketchy\", \"dangerous\", \"crime\", \"scary\", \"wouldn't walk\",\n",
    "                        \"shady\", \"not safe\", \"security concerns\"]\n",
    "\n",
    "    # Experience & Atmosphere Keywords\n",
    "    VIEW_WORDS = [\"view\", \"overlook\", \"skyline\", \"ocean view\", \"city view\", \"mountain view\",\n",
    "                \"water view\", \"scenic\", \"panoramic\", \"beautiful view\", \"stunning view\",\n",
    "                \"sunrise\", \"sunset\", \"balcony view\", \"window view\"]\n",
    "\n",
    "    PRIVACY_WORDS_POS = [\"private\", \"privacy\", \"secluded\", \"own space\", \"separate entrance\",\n",
    "                        \"private entrance\", \"no shared\", \"entire place\", \"all to yourself\"]\n",
    "    PRIVACY_WORDS_NEG = [\"shared\", \"thin walls\", \"noise from neighbors\", \"heard everything\",\n",
    "                        \"no privacy\", \"common area\"]\n",
    "\n",
    "    LOCAL_AUTHENTIC_WORDS = [\"local\", \"authentic\", \"neighborhood\", \"hidden gem\", \"off the beaten path\",\n",
    "                            \"local favorite\", \"like a local\", \"real experience\", \"community\",\n",
    "                            \"local spots\", \"neighborhood feel\", \"not touristy\"]\n",
    "\n",
    "    MODERN_WORDS = [\"modern\", \"updated\", \"renovated\", \"new\", \"contemporary\", \"sleek\",\n",
    "                    \"state of the art\", \"recently renovated\", \"brand new\", \"upgraded\"]\n",
    "\n",
    "    AESTHETIC_WORDS = [\"beautiful\", \"stylish\", \"decorated\", \"design\", \"aesthetic\", \"instagram\",\n",
    "                    \"cute\", \"charming\", \"artistic\", \"gorgeous\", \"lovely decor\", \"tasteful\"]\n",
    "\n",
    "    SOUNDPROOF_WORDS_POS = [\"quiet\", \"soundproof\", \"peaceful\", \"silent\", \"no noise\"]\n",
    "    SOUNDPROOF_WORDS_NEG = [\"noisy\", \"loud\", \"noise\", \"thin walls\", \"street noise\", \"heard neighbors\"]\n",
    "\n",
    "    # Service & Process Keywords\n",
    "    CHECKIN_WORDS_POS = [\"easy check-in\", \"smooth check-in\", \"self check-in\", \"keyless\",\n",
    "                        \"straightforward\", \"simple check-in\", \"lockbox\", \"smart lock\",\n",
    "                        \"24/7 check-in\", \"flexible check-in\"]\n",
    "    CHECKIN_WORDS_NEG = [\"check-in issues\", \"late check-in\", \"difficult check-in\",\n",
    "                        \"couldn't find\", \"problems getting in\", \"key issues\"]\n",
    "\n",
    "    ACCURACY_WORDS_POS = [\"as described\", \"accurate\", \"exactly as pictured\", \"matched photos\",\n",
    "                        \"what we expected\", \"true to listing\", \"no surprises\", \"as advertised\"]\n",
    "    ACCURACY_WORDS_NEG = [\"not as described\", \"misleading\", \"different from photos\",\n",
    "                        \"false advertising\", \"smaller than expected\", \"disappointment\"]\n",
    "\n",
    "    COMMUNICATION_WORDS_POS = [\"great communication\", \"responsive host\", \"quick response\",\n",
    "                            \"helpful host\", \"answered questions\", \"available\", \"attentive\",\n",
    "                            \"excellent communication\", \"always available\"]\n",
    "    COMMUNICATION_WORDS_NEG = [\"no response\", \"hard to reach\", \"unresponsive\", \"slow response\",\n",
    "                            \"poor communication\", \"never replied\", \"ghosted\"]\n",
    "\n",
    "    FLEXIBILITY_WORDS = [\"flexible\", \"accommodating\", \"understanding\", \"early check-in\",\n",
    "                        \"late checkout\", \"luggage storage\", \"worked with us\", \"adjusted\"]\n",
    "\n",
    "    LOCAL_TIPS_WORDS = [\"recommendations\", \"local tips\", \"guidebook\", \"suggested restaurants\",\n",
    "                        \"showed us around\", \"local knowledge\", \"insider tips\", \"helpful guide\",\n",
    "                        \"pointed us to\", \"told us about\"]\n",
    "\n",
    "    # Trip Type Keywords\n",
    "    SOLO_WORDS = [\"solo\", \"alone\", \"single traveler\", \"by myself\", \"solo trip\",\n",
    "                \"perfect for one\", \"single person\", \"independent\"]\n",
    "\n",
    "    COUPLE_WORDS = [\"romantic\", \"couples\", \"honeymoon\", \"anniversary\", \"date night\",\n",
    "                    \"romantic getaway\", \"partner\", \"loved one\", \"intimate\", \"cozy for two\"]\n",
    "\n",
    "    GROUP_WORDS = [\"group\", \"friends\", \"party\", \"gathering\", \"celebration\", \"bachelor\",\n",
    "                \"bachelorette\", \"reunion\", \"large group\", \"everyone\", \"hosted\"]\n",
    "\n",
    "    LONG_STAY_WORDS = [\"long stay\", \"extended stay\", \"monthly\", \"long term\", \"week\",\n",
    "                    \"stayed for\", \"lived here\", \"home away from home\", \"settled in\"]\n",
    "\n",
    "    FIRST_TIME_WORDS = [\"first time\", \"first visit\", \"new to\", \"tourist\", \"exploring\",\n",
    "                        \"sightseeing\", \"attractions\", \"landmarks\", \"must see\"]\n",
    "\n",
    "    REPEAT_GUEST_WORDS = [\"return\", \"come back\", \"again\", \"second time\", \"every time\",\n",
    "                        \"always stay\", \"regular\", \"will be back\", \"definitely return\"]\n",
    "\n",
    "    print(\"✓ Keyword dictionaries defined for all user-interest features\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,User-Interest Feature Extraction Functions (1-10)\n",
    "\n",
    "    def extract_bed_comfort_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating bed/sleep comfort quality from reviews.\n",
    "        Helps users who prioritize good sleep.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in BED_COMFORT_WORDS_POS:\n",
    "            score = score + when(reviews_col.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        for word in BED_COMFORT_WORDS_NEG:\n",
    "            score = score - when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_bathroom_quality_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating bathroom quality and cleanliness.\n",
    "        Important factor for guest satisfaction.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in BATHROOM_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        for word in BATHROOM_WORDS_NEG:\n",
    "            score = score - when(combined.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_kitchen_quality_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating kitchen usability and equipment.\n",
    "        Critical for guests planning to cook.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col, amenities_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in KITCHEN_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        for word in KITCHEN_WORDS_NEG:\n",
    "            score = score - when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_temperature_control_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating climate control quality (AC/heating).\n",
    "        Essential for comfort in various climates.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, amenities_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in TEMPERATURE_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        for word in TEMPERATURE_WORDS_NEG:\n",
    "            score = score - when(combined.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_natural_light_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating natural light and brightness.\n",
    "        Affects mood and livability.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in NATURAL_LIGHT_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_space_efficiency_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating perceived space and room size.\n",
    "        Helps users understand actual livable space.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in SPACE_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        for word in SPACE_WORDS_NEG:\n",
    "            score = score - when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_public_transport_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating public transportation accessibility.\n",
    "        Key for travelers without cars.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in TRANSPORT_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_walkability_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating walking accessibility to amenities.\n",
    "        Important for exploring on foot.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in WALKABILITY_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_nightlife_proximity_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating proximity to nightlife and entertainment.\n",
    "        For travelers seeking vibrant nightlife.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in NIGHTLIFE_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_restaurant_proximity_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating proximity to restaurants and cafes.\n",
    "        For food lovers and convenience seekers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in RESTAURANT_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "    print(\"✓ User-interest feature extractors defined (1-10)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,User-Interest Feature Extraction Functions (11-20)\n",
    "\n",
    "    def extract_grocery_proximity_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating proximity to grocery stores.\n",
    "        Essential for longer stays and cooking.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in GROCERY_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_neighborhood_safety_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating perceived neighborhood safety.\n",
    "        Critical factor for all travelers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in SAFETY_WORDS_POS:\n",
    "            score = score + when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        for word in SAFETY_WORDS_NEG:\n",
    "            score = score - when(reviews_col.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_view_quality_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating view quality from the property.\n",
    "        Premium feature for many travelers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        highlights_col = lower(coalesce(col(\"highlights\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col, highlights_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in VIEW_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_privacy_level_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating privacy level of the accommodation.\n",
    "        Important for guests valuing personal space.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in PRIVACY_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        for word in PRIVACY_WORDS_NEG:\n",
    "            score = score - when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_local_authenticity_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating authentic local experience potential.\n",
    "        For travelers seeking genuine cultural immersion.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in LOCAL_AUTHENTIC_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_modern_amenities_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating modern/updated amenities.\n",
    "        For guests preferring contemporary accommodations.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in MODERN_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_aesthetic_appeal_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating visual appeal and design quality.\n",
    "        For design-conscious travelers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in AESTHETIC_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_noise_insulation_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating noise/sound insulation quality.\n",
    "        Critical for light sleepers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in SOUNDPROOF_WORDS_POS:\n",
    "            score = score + when(reviews_col.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        for word in SOUNDPROOF_WORDS_NEG:\n",
    "            score = score - when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_checkin_ease_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating ease of check-in process.\n",
    "        Reduces arrival stress for guests.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in CHECKIN_WORDS_POS:\n",
    "            score = score + when(reviews_col.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        for word in CHECKIN_WORDS_NEG:\n",
    "            score = score - when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_listing_accuracy_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating how accurately listing matches reality.\n",
    "        Builds trust and sets correct expectations.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in ACCURACY_WORDS_POS:\n",
    "            score = score + when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        for word in ACCURACY_WORDS_NEG:\n",
    "            score = score - when(reviews_col.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "    print(\"✓ User-interest feature extractors defined (11-20)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,User-Interest Feature Extraction Functions (21-30)\n",
    "\n",
    "    def extract_communication_quality_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating host communication quality.\n",
    "        Key factor in guest experience.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in COMMUNICATION_WORDS_POS:\n",
    "            score = score + when(reviews_col.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        for word in COMMUNICATION_WORDS_NEG:\n",
    "            score = score - when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_response_speed_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating host response speed.\n",
    "        Uses host_response_rate if available, plus review mentions.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        # Base score from response-related mentions\n",
    "        score = lit(0.0)\n",
    "        quick_words = [\"quick response\", \"fast response\", \"responded immediately\", \n",
    "                    \"replied quickly\", \"prompt\", \"within minutes\"]\n",
    "        slow_words = [\"slow response\", \"took forever\", \"never responded\", \"waited\"]\n",
    "        \n",
    "        for word in quick_words:\n",
    "            score = score + when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        for word in slow_words:\n",
    "            score = score - when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_flexibility_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating host/policy flexibility.\n",
    "        Valuable for travelers with changing plans.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in FLEXIBILITY_WORDS:\n",
    "            score = score + when(reviews_col.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_local_tips_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating quality of local recommendations from host.\n",
    "        Enhances travel experience.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in LOCAL_TIPS_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_solo_traveler_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating suitability for solo travelers.\n",
    "        For independent travelers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in SOLO_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        # Bonus for safety and good location (important for solo travelers)\n",
    "        for word in SAFETY_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(0.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_couple_romantic_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating suitability for couples/romantic trips.\n",
    "        For romantic getaways.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in COUPLE_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        # Bonus for privacy and views\n",
    "        for word in PRIVACY_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(0.5)).otherwise(lit(0.0))\n",
    "        for word in VIEW_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(0.3)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_group_gathering_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating suitability for groups/gatherings.\n",
    "        For friend trips and celebrations.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in GROUP_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        # Bonus for space\n",
    "        for word in SPACE_WORDS_POS:\n",
    "            score = score + when(combined.contains(word), lit(0.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_long_stay_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating suitability for extended stays.\n",
    "        For digital nomads and long-term travelers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col, amenities_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in LONG_STAY_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        # Bonus for kitchen and laundry (important for long stays)\n",
    "        long_stay_amenities = [\"washer\", \"dryer\", \"laundry\", \"dishwasher\", \"full kitchen\"]\n",
    "        for word in long_stay_amenities:\n",
    "            score = score + when(combined.contains(word), lit(0.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_first_time_visitor_score(df):\n",
    "        \"\"\"\n",
    "        Score indicating suitability for first-time visitors.\n",
    "        For tourists new to the area.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in FIRST_TIME_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(1.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        # Bonus for good location and local tips\n",
    "        for word in LOCATION_QUALITY_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(0.3)).otherwise(lit(0.0))\n",
    "        for word in LOCAL_TIPS_WORDS:\n",
    "            score = score + when(combined.contains(word), lit(0.5)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "\n",
    "    def extract_repeat_guest_indicator(df):\n",
    "        \"\"\"\n",
    "        Score indicating mentions of return visits.\n",
    "        Strong quality signal.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        score = lit(0.0)\n",
    "        for word in REPEAT_GUEST_WORDS:\n",
    "            score = score + when(reviews_col.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        return score.cast(StringType())\n",
    "\n",
    "    print(\"✓ User-interest feature extractors defined (21-30)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Composite Score Calculation Functions\n",
    "\n",
    "    def calculate_overall_quality_score(df):\n",
    "        \"\"\"\n",
    "        OVERALL QUALITY SCORE (0-100 scale)\n",
    "        Combines: cleanliness, accuracy, amenities, and general positive sentiment.\n",
    "        Provides a single quality metric for quick assessment.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        # Component 1: Cleanliness (25 points max)\n",
    "        clean_score = lit(0.0)\n",
    "        for word in [\"clean\", \"spotless\", \"immaculate\", \"tidy\", \"pristine\"]:\n",
    "            clean_score = clean_score + when(reviews_col.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        clean_score = when(clean_score > 25, lit(25.0)).otherwise(clean_score)\n",
    "        \n",
    "        # Component 2: Accuracy (25 points max)\n",
    "        accuracy_score = lit(0.0)\n",
    "        for word in ACCURACY_WORDS_POS:\n",
    "            accuracy_score = accuracy_score + when(reviews_col.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        accuracy_score = when(accuracy_score > 25, lit(25.0)).otherwise(accuracy_score)\n",
    "        \n",
    "        # Component 3: Amenities quality (25 points max)\n",
    "        amenity_score = lit(0.0)\n",
    "        amenity_quality_words = [\"well-equipped\", \"everything we needed\", \"all amenities\", \n",
    "                                \"fully stocked\", \"great amenities\", \"modern appliances\"]\n",
    "        for word in amenity_quality_words:\n",
    "            amenity_score = amenity_score + when(reviews_col.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        amenity_score = when(amenity_score > 25, lit(25.0)).otherwise(amenity_score)\n",
    "        \n",
    "        # Component 4: General satisfaction (25 points max)\n",
    "        satisfaction_score = lit(0.0)\n",
    "        satisfaction_words = [\"highly recommend\", \"would stay again\", \"perfect\", \"exceeded expectations\",\n",
    "                            \"amazing stay\", \"wonderful experience\"]\n",
    "        for word in satisfaction_words:\n",
    "            satisfaction_score = satisfaction_score + when(reviews_col.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        satisfaction_score = when(satisfaction_score > 25, lit(25.0)).otherwise(satisfaction_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        negative_words = [\"disappointing\", \"dirty\", \"broken\", \"misleading\", \"avoid\", \"worst\"]\n",
    "        for word in negative_words:\n",
    "            deduction = deduction + when(reviews_col.contains(word), lit(10.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = clean_score + accuracy_score + amenity_score + satisfaction_score - deduction\n",
    "        # Ensure score is between 0 and 100\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_location_convenience_score(df):\n",
    "        \"\"\"\n",
    "        LOCATION CONVENIENCE SCORE (0-100 scale)\n",
    "        Combines: transport, walkability, proximity to amenities, and neighborhood quality.\n",
    "        Essential for travelers prioritizing accessibility.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        # Component 1: Public transport (25 points max)\n",
    "        transport_score = lit(0.0)\n",
    "        for word in TRANSPORT_WORDS:\n",
    "            transport_score = transport_score + when(combined.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        transport_score = when(transport_score > 25, lit(25.0)).otherwise(transport_score)\n",
    "        \n",
    "        # Component 2: Walkability (25 points max)\n",
    "        walk_score = lit(0.0)\n",
    "        for word in WALKABILITY_WORDS:\n",
    "            walk_score = walk_score + when(combined.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        walk_score = when(walk_score > 25, lit(25.0)).otherwise(walk_score)\n",
    "        \n",
    "        # Component 3: Nearby amenities (25 points max)\n",
    "        amenity_score = lit(0.0)\n",
    "        for word in RESTAURANT_WORDS + GROCERY_WORDS:\n",
    "            amenity_score = amenity_score + when(combined.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        amenity_score = when(amenity_score > 25, lit(25.0)).otherwise(amenity_score)\n",
    "        \n",
    "        # Component 4: Location quality mentions (25 points max)\n",
    "        location_score = lit(0.0)\n",
    "        for word in LOCATION_QUALITY_WORDS:\n",
    "            location_score = location_score + when(combined.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        location_score = when(location_score > 25, lit(25.0)).otherwise(location_score)\n",
    "        \n",
    "        total = transport_score + walk_score + amenity_score + location_score\n",
    "        final = when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_host_excellence_score(df):\n",
    "        \"\"\"\n",
    "        HOST EXCELLENCE SCORE (0-100 scale)\n",
    "        Combines: communication, responsiveness, check-in ease, and helpfulness.\n",
    "        Reflects the quality of host interaction.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        # Component 1: Communication (25 points max)\n",
    "        comm_score = lit(0.0)\n",
    "        for word in COMMUNICATION_WORDS_POS:\n",
    "            comm_score = comm_score + when(reviews_col.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        comm_score = when(comm_score > 25, lit(25.0)).otherwise(comm_score)\n",
    "        \n",
    "        # Component 2: Responsiveness (25 points max)\n",
    "        resp_score = lit(0.0)\n",
    "        for word in HOST_RESPONSIVENESS_WORDS:\n",
    "            resp_score = resp_score + when(reviews_col.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        resp_score = when(resp_score > 25, lit(25.0)).otherwise(resp_score)\n",
    "        \n",
    "        # Component 3: Check-in (25 points max)\n",
    "        checkin_score = lit(0.0)\n",
    "        for word in CHECKIN_WORDS_POS:\n",
    "            checkin_score = checkin_score + when(reviews_col.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        checkin_score = when(checkin_score > 25, lit(25.0)).otherwise(checkin_score)\n",
    "        \n",
    "        # Component 4: Local tips & flexibility (25 points max)\n",
    "        extra_score = lit(0.0)\n",
    "        for word in LOCAL_TIPS_WORDS + FLEXIBILITY_WORDS:\n",
    "            extra_score = extra_score + when(reviews_col.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        extra_score = when(extra_score > 25, lit(25.0)).otherwise(extra_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        for word in COMMUNICATION_WORDS_NEG:\n",
    "            deduction = deduction + when(reviews_col.contains(word), lit(15.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = comm_score + resp_score + checkin_score + extra_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_value_perception_score(df):\n",
    "        \"\"\"\n",
    "        VALUE PERCEPTION SCORE (0-100 scale)\n",
    "        Combines: value mentions, price-quality perception, and deal indicators.\n",
    "        Helps users understand bang for their buck.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        \n",
    "        # Component 1: Positive value mentions (50 points max)\n",
    "        value_score = lit(0.0)\n",
    "        positive_value = [\"great value\", \"worth every penny\", \"good value\", \"reasonable price\",\n",
    "                        \"affordable\", \"bargain\", \"good deal\", \"excellent value\", \"worth it\"]\n",
    "        for word in positive_value:\n",
    "            value_score = value_score + when(reviews_col.contains(word), lit(8.0)).otherwise(lit(0.0))\n",
    "        value_score = when(value_score > 50, lit(50.0)).otherwise(value_score)\n",
    "        \n",
    "        # Component 2: Quality for price (50 points max)\n",
    "        quality_score = lit(0.0)\n",
    "        quality_value = [\"exceeded expectations\", \"more than expected\", \"pleasantly surprised\",\n",
    "                        \"better than expected\", \"incredible for the price\"]\n",
    "        for word in quality_value:\n",
    "            quality_score = quality_score + when(reviews_col.contains(word), lit(10.0)).otherwise(lit(0.0))\n",
    "        quality_score = when(quality_score > 50, lit(50.0)).otherwise(quality_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        negative_value = [\"overpriced\", \"too expensive\", \"not worth\", \"rip off\", \"overcharged\"]\n",
    "        for word in negative_value:\n",
    "            deduction = deduction + when(reviews_col.contains(word), lit(20.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = value_score + quality_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_comfort_index_score(df):\n",
    "        \"\"\"\n",
    "        COMFORT INDEX SCORE (0-100 scale)\n",
    "        Combines: bed quality, temperature, space, noise, and overall comfort.\n",
    "        Essential for quality sleep and relaxation.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, amenities_col)\n",
    "        \n",
    "        # Component 1: Bed comfort (25 points max)\n",
    "        bed_score = lit(0.0)\n",
    "        for word in BED_COMFORT_WORDS_POS:\n",
    "            bed_score = bed_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        bed_score = when(bed_score > 25, lit(25.0)).otherwise(bed_score)\n",
    "        \n",
    "        # Component 2: Temperature control (25 points max)\n",
    "        temp_score = lit(0.0)\n",
    "        for word in TEMPERATURE_WORDS_POS:\n",
    "            temp_score = temp_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        temp_score = when(temp_score > 25, lit(25.0)).otherwise(temp_score)\n",
    "        \n",
    "        # Component 3: Space (25 points max)\n",
    "        space_score = lit(0.0)\n",
    "        for word in SPACE_WORDS_POS:\n",
    "            space_score = space_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        space_score = when(space_score > 25, lit(25.0)).otherwise(space_score)\n",
    "        \n",
    "        # Component 4: Quietness (25 points max)\n",
    "        quiet_score = lit(0.0)\n",
    "        for word in SOUNDPROOF_WORDS_POS:\n",
    "            quiet_score = quiet_score + when(combined.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        quiet_score = when(quiet_score > 25, lit(25.0)).otherwise(quiet_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        for word in BED_COMFORT_WORDS_NEG + TEMPERATURE_WORDS_NEG + SPACE_WORDS_NEG + SOUNDPROOF_WORDS_NEG:\n",
    "            deduction = deduction + when(combined.contains(word), lit(8.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = bed_score + temp_score + space_score + quiet_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "    print(\"✓ Composite score functions defined (1-5)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Composite Score Calculation Functions (6-10)\n",
    "\n",
    "    def calculate_family_suitability_score(df):\n",
    "        \"\"\"\n",
    "        FAMILY SUITABILITY SCORE (0-100 scale)\n",
    "        Combines: family mentions, safety, space, and child-friendly amenities.\n",
    "        Essential for traveling families.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col, amenities_col)\n",
    "        \n",
    "        # Component 1: Family mentions (30 points max)\n",
    "        family_score = lit(0.0)\n",
    "        for word in FAMILY_FRIENDLY_WORDS:\n",
    "            family_score = family_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        family_score = when(family_score > 30, lit(30.0)).otherwise(family_score)\n",
    "        \n",
    "        # Component 2: Safety (30 points max)\n",
    "        safety_score = lit(0.0)\n",
    "        for word in SAFETY_WORDS_POS:\n",
    "            safety_score = safety_score + when(combined.contains(word), lit(6.0)).otherwise(lit(0.0))\n",
    "        safety_score = when(safety_score > 30, lit(30.0)).otherwise(safety_score)\n",
    "        \n",
    "        # Component 3: Space (20 points max)\n",
    "        space_score = lit(0.0)\n",
    "        for word in SPACE_WORDS_POS:\n",
    "            space_score = space_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        space_score = when(space_score > 20, lit(20.0)).otherwise(space_score)\n",
    "        \n",
    "        # Component 4: Child amenities (20 points max)\n",
    "        child_amenity_score = lit(0.0)\n",
    "        child_amenities = [\"crib\", \"high chair\", \"toys\", \"games\", \"playground\", \"child-safe\",\n",
    "                        \"baby\", \"stroller\", \"pack n play\"]\n",
    "        for word in child_amenities:\n",
    "            child_amenity_score = child_amenity_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        child_amenity_score = when(child_amenity_score > 20, lit(20.0)).otherwise(child_amenity_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        not_family = [\"adults only\", \"no children\", \"not suitable for children\", \"party house\"]\n",
    "        for word in not_family:\n",
    "            deduction = deduction + when(combined.contains(word), lit(30.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = family_score + safety_score + space_score + child_amenity_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_business_ready_score(df):\n",
    "        \"\"\"\n",
    "        BUSINESS READY SCORE (0-100 scale)\n",
    "        Combines: WiFi quality, workspace, quiet environment, and professional amenities.\n",
    "        For remote workers and business travelers.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col, amenities_col)\n",
    "        \n",
    "        # Component 1: WiFi/Internet (30 points max)\n",
    "        wifi_score = lit(0.0)\n",
    "        wifi_words = [\"fast wifi\", \"fast internet\", \"wifi\", \"wi-fi\", \"high-speed internet\",\n",
    "                    \"reliable internet\", \"good wifi\", \"great wifi\"]\n",
    "        for word in wifi_words:\n",
    "            wifi_score = wifi_score + when(combined.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        wifi_score = when(wifi_score > 30, lit(30.0)).otherwise(wifi_score)\n",
    "        \n",
    "        # Component 2: Workspace (30 points max)\n",
    "        workspace_score = lit(0.0)\n",
    "        workspace_words = [\"desk\", \"workspace\", \"work from home\", \"office\", \"dedicated workspace\",\n",
    "                        \"work-friendly\", \"laptop-friendly\", \"work area\"]\n",
    "        for word in workspace_words:\n",
    "            workspace_score = workspace_score + when(combined.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        workspace_score = when(workspace_score > 30, lit(30.0)).otherwise(workspace_score)\n",
    "        \n",
    "        # Component 3: Quiet environment (25 points max)\n",
    "        quiet_score = lit(0.0)\n",
    "        for word in SOUNDPROOF_WORDS_POS:\n",
    "            quiet_score = quiet_score + when(combined.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        quiet_score = when(quiet_score > 25, lit(25.0)).otherwise(quiet_score)\n",
    "        \n",
    "        # Component 4: Professional amenities (15 points max)\n",
    "        prof_score = lit(0.0)\n",
    "        prof_amenities = [\"coffee maker\", \"printer\", \"iron\", \"self check-in\", \"24/7\"]\n",
    "        for word in prof_amenities:\n",
    "            prof_score = prof_score + when(combined.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        prof_score = when(prof_score > 15, lit(15.0)).otherwise(prof_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        anti_work = [\"no wifi\", \"slow internet\", \"wifi issues\", \"noisy\"]\n",
    "        for word in anti_work:\n",
    "            deduction = deduction + when(combined.contains(word), lit(20.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = wifi_score + workspace_score + quiet_score + prof_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_social_experience_score(df):\n",
    "        \"\"\"\n",
    "        SOCIAL EXPERIENCE SCORE (0-100 scale)\n",
    "        Combines: group suitability, nightlife access, entertainment, and social atmosphere.\n",
    "        For travelers seeking social/party experiences.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        # Component 1: Group/Party mentions (30 points max)\n",
    "        group_score = lit(0.0)\n",
    "        for word in GROUP_WORDS:\n",
    "            group_score = group_score + when(combined.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        group_score = when(group_score > 30, lit(30.0)).otherwise(group_score)\n",
    "        \n",
    "        # Component 2: Nightlife access (30 points max)\n",
    "        nightlife_score = lit(0.0)\n",
    "        for word in NIGHTLIFE_WORDS:\n",
    "            nightlife_score = nightlife_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        nightlife_score = when(nightlife_score > 30, lit(30.0)).otherwise(nightlife_score)\n",
    "        \n",
    "        # Component 3: Entertainment (25 points max)\n",
    "        entertainment_score = lit(0.0)\n",
    "        entertainment_words = [\"tv\", \"netflix\", \"streaming\", \"games\", \"game room\", \"pool table\",\n",
    "                            \"entertainment\", \"smart tv\", \"cable\"]\n",
    "        for word in entertainment_words:\n",
    "            entertainment_score = entertainment_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        entertainment_score = when(entertainment_score > 25, lit(25.0)).otherwise(entertainment_score)\n",
    "        \n",
    "        # Component 4: Social atmosphere (15 points max)\n",
    "        social_score = lit(0.0)\n",
    "        social_words = [\"vibrant\", \"lively\", \"happening\", \"fun area\", \"great vibe\"]\n",
    "        for word in social_words:\n",
    "            social_score = social_score + when(combined.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        social_score = when(social_score > 15, lit(15.0)).otherwise(social_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        anti_social = [\"no parties\", \"quiet hours\", \"no events\", \"strict rules\"]\n",
    "        for word in anti_social:\n",
    "            deduction = deduction + when(combined.contains(word), lit(10.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = group_score + nightlife_score + entertainment_score + social_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_relaxation_wellness_score(df):\n",
    "        \"\"\"\n",
    "        RELAXATION & WELLNESS SCORE (0-100 scale)\n",
    "        Combines: quietness, views, outdoor space, spa-like amenities, and peaceful atmosphere.\n",
    "        For travelers seeking rest and rejuvenation.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        amenities_col = lower(coalesce(col(\"amenities\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col, amenities_col)\n",
    "        \n",
    "        # Component 1: Quietness/Peace (30 points max)\n",
    "        quiet_score = lit(0.0)\n",
    "        peace_words = [\"quiet\", \"peaceful\", \"tranquil\", \"serene\", \"calm\", \"relaxing\", \"zen\"]\n",
    "        for word in peace_words:\n",
    "            quiet_score = quiet_score + when(combined.contains(word), lit(5.0)).otherwise(lit(0.0))\n",
    "        quiet_score = when(quiet_score > 30, lit(30.0)).otherwise(quiet_score)\n",
    "        \n",
    "        # Component 2: Views/Nature (25 points max)\n",
    "        view_score = lit(0.0)\n",
    "        nature_words = VIEW_WORDS + [\"nature\", \"garden\", \"trees\", \"greenery\", \"park view\"]\n",
    "        for word in nature_words:\n",
    "            view_score = view_score + when(combined.contains(word), lit(2.0)).otherwise(lit(0.0))\n",
    "        view_score = when(view_score > 25, lit(25.0)).otherwise(view_score)\n",
    "        \n",
    "        # Component 3: Outdoor space (25 points max)\n",
    "        outdoor_score = lit(0.0)\n",
    "        for word in OUTDOOR_SPACE_WORDS:\n",
    "            outdoor_score = outdoor_score + when(combined.contains(word), lit(3.0)).otherwise(lit(0.0))\n",
    "        outdoor_score = when(outdoor_score > 25, lit(25.0)).otherwise(outdoor_score)\n",
    "        \n",
    "        # Component 4: Wellness amenities (20 points max)\n",
    "        wellness_score = lit(0.0)\n",
    "        wellness_words = [\"hot tub\", \"jacuzzi\", \"sauna\", \"spa\", \"bathtub\", \"soaking tub\",\n",
    "                        \"massage\", \"yoga\", \"gym\", \"pool\"]\n",
    "        for word in wellness_words:\n",
    "            wellness_score = wellness_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        wellness_score = when(wellness_score > 20, lit(20.0)).otherwise(wellness_score)\n",
    "        \n",
    "        # Negative deductions\n",
    "        deduction = lit(0.0)\n",
    "        anti_relax = [\"noisy\", \"loud\", \"busy street\", \"construction\", \"traffic noise\"]\n",
    "        for word in anti_relax:\n",
    "            deduction = deduction + when(combined.contains(word), lit(15.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = quiet_score + view_score + outdoor_score + wellness_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "\n",
    "    def calculate_local_immersion_score(df):\n",
    "        \"\"\"\n",
    "        LOCAL IMMERSION SCORE (0-100 scale)\n",
    "        Combines: local authenticity, neighborhood character, local tips, and cultural experience.\n",
    "        For travelers seeking genuine local experiences.\n",
    "        \"\"\"\n",
    "        reviews_col = lower(coalesce(col(\"reviews\").cast(StringType()), lit(\"\")))\n",
    "        desc_col = lower(coalesce(col(\"description\").cast(StringType()), lit(\"\")))\n",
    "        combined = concat_ws(\" \", reviews_col, desc_col)\n",
    "        \n",
    "        # Component 1: Local authenticity (30 points max)\n",
    "        authentic_score = lit(0.0)\n",
    "        for word in LOCAL_AUTHENTIC_WORDS:\n",
    "            authentic_score = authentic_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        authentic_score = when(authentic_score > 30, lit(30.0)).otherwise(authentic_score)\n",
    "        \n",
    "        # Component 2: Local tips from host (25 points max)\n",
    "        tips_score = lit(0.0)\n",
    "        for word in LOCAL_TIPS_WORDS:\n",
    "            tips_score = tips_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        tips_score = when(tips_score > 25, lit(25.0)).otherwise(tips_score)\n",
    "        \n",
    "        # Component 3: Neighborhood character (25 points max)\n",
    "        neighborhood_score = lit(0.0)\n",
    "        neighborhood_words = [\"neighborhood\", \"local shops\", \"local restaurants\", \"markets\",\n",
    "                            \"community\", \"residential\", \"locals\", \"culture\"]\n",
    "        for word in neighborhood_words:\n",
    "            neighborhood_score = neighborhood_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        neighborhood_score = when(neighborhood_score > 25, lit(25.0)).otherwise(neighborhood_score)\n",
    "        \n",
    "        # Component 4: Cultural experience (20 points max)\n",
    "        culture_score = lit(0.0)\n",
    "        culture_words = [\"culture\", \"history\", \"historic\", \"museum\", \"art\", \"traditional\",\n",
    "                        \"authentic experience\", \"local experience\"]\n",
    "        for word in culture_words:\n",
    "            culture_score = culture_score + when(combined.contains(word), lit(4.0)).otherwise(lit(0.0))\n",
    "        culture_score = when(culture_score > 20, lit(20.0)).otherwise(culture_score)\n",
    "        \n",
    "        # Negative for overly touristy\n",
    "        deduction = lit(0.0)\n",
    "        touristy = [\"touristy\", \"tourist trap\", \"crowded with tourists\", \"very commercial\"]\n",
    "        for word in touristy:\n",
    "            deduction = deduction + when(combined.contains(word), lit(15.0)).otherwise(lit(0.0))\n",
    "        \n",
    "        total = authentic_score + tips_score + neighborhood_score + culture_score - deduction\n",
    "        final = when(total < 0, lit(0.0)).when(total > 100, lit(100.0)).otherwise(total)\n",
    "        \n",
    "        return final.cast(StringType())\n",
    "\n",
    "    print(\"✓ Composite score functions defined (6-10)\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Register All New Feature Extractors\n",
    "\n",
    "    # =============================================================================\n",
    "    # REGISTRY FOR USER-INTEREST FEATURES\n",
    "    # =============================================================================\n",
    "    USER_INTEREST_EXTRACTORS = {\n",
    "        # Comfort & Quality Features (1-6)\n",
    "        \"bed_comfort_score\": extract_bed_comfort_score,\n",
    "        \"bathroom_quality_score\": extract_bathroom_quality_score,\n",
    "        \"kitchen_quality_score\": extract_kitchen_quality_score,\n",
    "        \"temperature_control_score\": extract_temperature_control_score,\n",
    "        \"natural_light_score\": extract_natural_light_score,\n",
    "        \"space_efficiency_score\": extract_space_efficiency_score,\n",
    "        \n",
    "        # Location & Neighborhood Features (7-12)\n",
    "        \"public_transport_score\": extract_public_transport_score,\n",
    "        \"walkability_score\": extract_walkability_score,\n",
    "        \"nightlife_proximity_score\": extract_nightlife_proximity_score,\n",
    "        \"restaurant_proximity_score\": extract_restaurant_proximity_score,\n",
    "        \"grocery_proximity_score\": extract_grocery_proximity_score,\n",
    "        \"neighborhood_safety_score\": extract_neighborhood_safety_score,\n",
    "        \n",
    "        # Experience & Atmosphere Features (13-18)\n",
    "        \"view_quality_score\": extract_view_quality_score,\n",
    "        \"privacy_level_score\": extract_privacy_level_score,\n",
    "        \"local_authenticity_score\": extract_local_authenticity_score,\n",
    "        \"modern_amenities_score\": extract_modern_amenities_score,\n",
    "        \"aesthetic_appeal_score\": extract_aesthetic_appeal_score,\n",
    "        \"noise_insulation_score\": extract_noise_insulation_score,\n",
    "        \n",
    "        # Service & Process Features (19-24)\n",
    "        \"checkin_ease_score\": extract_checkin_ease_score,\n",
    "        \"listing_accuracy_score\": extract_listing_accuracy_score,\n",
    "        \"communication_quality_score\": extract_communication_quality_score,\n",
    "        \"response_speed_score\": extract_response_speed_score,\n",
    "        \"flexibility_score\": extract_flexibility_score,\n",
    "        \"local_tips_score\": extract_local_tips_score,\n",
    "        \n",
    "        # Trip Type Suitability Features (25-30)\n",
    "        \"solo_traveler_score\": extract_solo_traveler_score,\n",
    "        \"couple_romantic_score\": extract_couple_romantic_score,\n",
    "        \"group_gathering_score\": extract_group_gathering_score,\n",
    "        \"long_stay_score\": extract_long_stay_score,\n",
    "        \"first_time_visitor_score\": extract_first_time_visitor_score,\n",
    "        \"repeat_guest_indicator\": extract_repeat_guest_indicator,\n",
    "    }\n",
    "\n",
    "    # =============================================================================\n",
    "    # REGISTRY FOR COMPOSITE SCORES\n",
    "    # =============================================================================\n",
    "    COMPOSITE_SCORE_EXTRACTORS = {\n",
    "        \"overall_quality_score\": calculate_overall_quality_score,\n",
    "        \"location_convenience_score\": calculate_location_convenience_score,\n",
    "        \"host_excellence_score\": calculate_host_excellence_score,\n",
    "        \"value_perception_score\": calculate_value_perception_score,\n",
    "        \"comfort_index_score\": calculate_comfort_index_score,\n",
    "        \"family_suitability_score\": calculate_family_suitability_score,\n",
    "        \"business_ready_score\": calculate_business_ready_score,\n",
    "        \"social_experience_score\": calculate_social_experience_score,\n",
    "        \"relaxation_wellness_score\": calculate_relaxation_wellness_score,\n",
    "        \"local_immersion_score\": calculate_local_immersion_score,\n",
    "    }\n",
    "\n",
    "    print(\"✓ All feature extractors registered\")\n",
    "    print(f\"  User-Interest Features: {len(USER_INTEREST_EXTRACTORS)}\")\n",
    "    print(f\"  Composite Scores: {len(COMPOSITE_SCORE_EXTRACTORS)}\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Build Extended Features Map Function\n",
    "\n",
    "    def build_extended_features_map(df, \n",
    "                                    existing_features_expr,\n",
    "                                    user_interest_extractors,\n",
    "                                    composite_score_extractors):\n",
    "        \"\"\"\n",
    "        Extend the existing features map with new user-interest features and composite scores.\n",
    "        \n",
    "        Args:\n",
    "            df: Source DataFrame\n",
    "            existing_features_expr: Existing features map column expression\n",
    "            user_interest_extractors: Dict of user-interest feature extractors\n",
    "            composite_score_extractors: Dict of composite score extractors\n",
    "        \n",
    "        Returns:\n",
    "            Extended features map column expression\n",
    "        \"\"\"\n",
    "        # Start with additional map arguments for new features\n",
    "        new_map_args = []\n",
    "        \n",
    "        # Add user-interest features\n",
    "        for feature_name, extractor_func in user_interest_extractors.items():\n",
    "            new_map_args.append(lit(feature_name))\n",
    "            new_map_args.append(extractor_func(df))\n",
    "        \n",
    "        # Add composite scores\n",
    "        for score_name, score_func in composite_score_extractors.items():\n",
    "            new_map_args.append(lit(score_name))\n",
    "            new_map_args.append(score_func(df))\n",
    "        \n",
    "        # Create the new features map\n",
    "        new_features_map = create_map(*new_map_args)\n",
    "        \n",
    "        # Combine with existing features using map_concat\n",
    "        # Use map_concat to merge maps (available in Spark 3.0+)\n",
    "        from pyspark.sql.functions import map_concat\n",
    "        combined_map = map_concat(existing_features_expr, new_features_map)\n",
    "        \n",
    "        return combined_map\n",
    "\n",
    "    print(\"✓ Extended features map builder defined\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Update df_features_vector with New Features\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UPDATING FEATURES VECTOR WITH USER-INTEREST FEATURES AND COMPOSITE SCORES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Import map_concat for combining maps\n",
    "    from pyspark.sql.functions import map_concat, lit\n",
    "\n",
    "    # Step 1: Create the new features map expression\n",
    "    print(\"\\n1. Building new features map expression...\")\n",
    "    new_features_args = []\n",
    "\n",
    "    # Add user-interest features\n",
    "    for feature_name, extractor_func in USER_INTEREST_EXTRACTORS.items():\n",
    "        new_features_args.append(lit(feature_name))\n",
    "        new_features_args.append(extractor_func(df_reloaded))\n",
    "\n",
    "    # Add composite scores\n",
    "    for score_name, score_func in COMPOSITE_SCORE_EXTRACTORS.items():\n",
    "        new_features_args.append(lit(score_name))\n",
    "        new_features_args.append(score_func(df_reloaded))\n",
    "\n",
    "    new_features_map = create_map(*new_features_args)\n",
    "    print(f\"   ✓ New features map created with {len(USER_INTEREST_EXTRACTORS) + len(COMPOSITE_SCORE_EXTRACTORS)} features\")\n",
    "\n",
    "    # Step 2: Rebuild the complete features vector with all features\n",
    "    print(\"\\n2. Rebuilding complete features vector...\")\n",
    "\n",
    "    # Build complete features map including all existing + new features\n",
    "    complete_features_map_expr = build_comprehensive_features_map(\n",
    "        df_reloaded,\n",
    "        VALID_TEXT,\n",
    "        VALID_LOCATION,\n",
    "        VALID_CATEGORICAL_RAW,\n",
    "        VALID_PRICE_DISCOUNT,\n",
    "        VALID_REVIEW,\n",
    "        VALID_URL_METADATA,\n",
    "        VALID_MEDIA,\n",
    "        VALID_HOST_SELLER,\n",
    "        VALID_OTHER,\n",
    "        VALID_NUMERICAL,\n",
    "        VALID_ONEHOT,\n",
    "        VALID_TEXT_DERIVED\n",
    "    )\n",
    "\n",
    "    # Combine existing features with new features\n",
    "    combined_features_map = map_concat(complete_features_map_expr, new_features_map)\n",
    "\n",
    "    # Step 3: Create updated features vector dataframe\n",
    "    print(\"\\n3. Creating updated features vector dataframe...\")\n",
    "    df_features_vector_updated = df_reloaded.select(\n",
    "        col(\"property_id\").alias(\"listing_id\"),\n",
    "        combined_features_map.alias(\"features_dict\")\n",
    "    )\n",
    "\n",
    "    # Unpersist old cached dataframe and cache new one\n",
    "    df_features_vector.unpersist()\n",
    "    df_features_vector = df_features_vector_updated.persist()\n",
    "\n",
    "    print(\"   ✓ Features vector updated and cached\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Update Feature Mappings\n",
    "\n",
    "    # Update FEATURE_NAMES_MAPPING with new features\n",
    "    current_idx = len(FEATURE_NAMES_MAPPING)\n",
    "\n",
    "    # Add user-interest features to mapping\n",
    "    FEATURE_CATEGORIES[\"user_interest\"] = []\n",
    "    for feature_name in USER_INTEREST_EXTRACTORS.keys():\n",
    "        FEATURE_NAMES_MAPPING[feature_name] = current_idx\n",
    "        FEATURE_CATEGORIES[\"user_interest\"].append(feature_name)\n",
    "        current_idx += 1\n",
    "\n",
    "    # Add composite scores to mapping\n",
    "    FEATURE_CATEGORIES[\"composite_scores\"] = []\n",
    "    for score_name in COMPOSITE_SCORE_EXTRACTORS.keys():\n",
    "        FEATURE_NAMES_MAPPING[score_name] = current_idx\n",
    "        FEATURE_CATEGORIES[\"composite_scores\"].append(score_name)\n",
    "        current_idx += 1\n",
    "\n",
    "    print(\"\\nFeature Mappings Updated:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nUSER-INTEREST FEATURES ({len(FEATURE_CATEGORIES['user_interest'])} features):\")\n",
    "    for i, f in enumerate(FEATURE_CATEGORIES[\"user_interest\"]):\n",
    "        print(f\"  {FEATURE_NAMES_MAPPING[f]:3d}: {f}\")\n",
    "\n",
    "    print(f\"\\nCOMPOSITE SCORES ({len(FEATURE_CATEGORIES['composite_scores'])} scores):\")\n",
    "    for f in FEATURE_CATEGORIES[\"composite_scores\"]:\n",
    "        print(f\"  {FEATURE_NAMES_MAPPING[f]:3d}: {f}\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Verify Updated Features Vector\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFICATION OF UPDATED FEATURES VECTOR\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nUpdated Schema of df_features_vector:\")\n",
    "    df_features_vector.printSchema()\n",
    "\n",
    "    total_features_updated = len(FEATURE_NAMES_MAPPING)\n",
    "    print(f\"\\nTotal features in dictionary: {total_features_updated}\")\n",
    "\n",
    "    print(\"\\nFeatures by category:\")\n",
    "    for category, features in FEATURE_CATEGORIES.items():\n",
    "        if features:\n",
    "            print(f\"  {category}: {len(features)}\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Display Sample with New Features\n",
    "\n",
    "    print(\"\\nSample output with new features (first 2 rows):\")\n",
    "    display(df_features_vector.limit(2))\n",
    "\n",
    "    # Show new features in detail for single record\n",
    "    print(\"\\nDetailed view of NEW features for single record:\")\n",
    "    sample_row = df_features_vector.limit(1).collect()[0]\n",
    "    print(f\"\\nListing ID: {sample_row['listing_id']}\")\n",
    "\n",
    "    print(f\"\\n--- USER-INTEREST FEATURES ({len(FEATURE_CATEGORIES['user_interest'])}) ---\")\n",
    "    for f in FEATURE_CATEGORIES[\"user_interest\"]:\n",
    "        value = sample_row['features_dict'].get(f, 'N/A')\n",
    "        print(f\"  {f}: {value}\")\n",
    "\n",
    "    print(f\"\\n--- COMPOSITE SCORES ({len(FEATURE_CATEGORIES['composite_scores'])}) ---\")\n",
    "    for f in FEATURE_CATEGORIES[\"composite_scores\"]:\n",
    "        value = sample_row['features_dict'].get(f, 'N/A')\n",
    "        print(f\"  {f}: {value}\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Final Summary\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL PIPELINE SUMMARY - USER-INTEREST FEATURES AND COMPOSITE SCORES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    total_rows = df_features_vector.count()\n",
    "\n",
    "    print(f\"\"\"\n",
    "    =============================================================================\n",
    "    ADDED FEATURES SUMMARY\n",
    "    =============================================================================\n",
    "\n",
    "    30 USER-INTEREST FEATURES:\n",
    "    --------------------------\n",
    "    These features help users understand specific aspects of the apartment:\n",
    "\n",
    "    COMFORT & QUALITY (6 features):\n",
    "        • bed_comfort_score - Sleep quality indicators\n",
    "        • bathroom_quality_score - Bathroom condition\n",
    "        • kitchen_quality_score - Kitchen usability\n",
    "        • temperature_control_score - AC/heating quality\n",
    "        • natural_light_score - Brightness/light\n",
    "        • space_efficiency_score - Room size perception\n",
    "\n",
    "    LOCATION & NEIGHBORHOOD (6 features):\n",
    "        • public_transport_score - Transit access\n",
    "        • walkability_score - Walking accessibility\n",
    "        • nightlife_proximity_score - Entertainment nearby\n",
    "        • restaurant_proximity_score - Dining options\n",
    "        • grocery_proximity_score - Shopping access\n",
    "        • neighborhood_safety_score - Safety perception\n",
    "\n",
    "    EXPERIENCE & ATMOSPHERE (6 features):\n",
    "        • view_quality_score - Views from property\n",
    "        • privacy_level_score - Privacy vs shared\n",
    "        • local_authenticity_score - Local experience\n",
    "        • modern_amenities_score - Modern features\n",
    "        • aesthetic_appeal_score - Design/decor\n",
    "        • noise_insulation_score - Soundproofing\n",
    "\n",
    "    SERVICE & PROCESS (6 features):\n",
    "        • checkin_ease_score - Check-in smoothness\n",
    "        • listing_accuracy_score - Matches description\n",
    "        • communication_quality_score - Host communication\n",
    "        • response_speed_score - Response time\n",
    "        • flexibility_score - Policy flexibility\n",
    "        • local_tips_score - Local recommendations\n",
    "\n",
    "    TRIP TYPE SUITABILITY (6 features):\n",
    "        • solo_traveler_score - Solo travel fit\n",
    "        • couple_romantic_score - Couples/romance\n",
    "        • group_gathering_score - Groups/parties\n",
    "        • long_stay_score - Extended stays\n",
    "        • first_time_visitor_score - Tourist friendly\n",
    "        • repeat_guest_indicator - Return visits\n",
    "\n",
    "    10 COMPOSITE SCORES (0-100 scale):\n",
    "    ----------------------------------\n",
    "    These aggregate scores provide high-level insights:\n",
    "\n",
    "        • overall_quality_score - Combined quality metric\n",
    "        • location_convenience_score - Location & accessibility\n",
    "        • host_excellence_score - Host quality & service\n",
    "        • value_perception_score - Value for money\n",
    "        • comfort_index_score - Physical comfort\n",
    "        • family_suitability_score - Family-friendliness\n",
    "        • business_ready_score - Work/business suitability\n",
    "        • social_experience_score - Social/entertainment\n",
    "        • relaxation_wellness_score - Peace & relaxation\n",
    "        • local_immersion_score - Authentic local experience\n",
    "\n",
    "    =============================================================================\n",
    "    TOTAL FEATURES IN VECTOR\n",
    "    =============================================================================\n",
    "\n",
    "    Previous features: {total_features_updated - 40}\n",
    "    New user-interest features: 30\n",
    "    New composite scores: 10\n",
    "    ---------------------\n",
    "    TOTAL FEATURES: {total_features_updated}\n",
    "\n",
    "    Total Listings Processed: {total_rows:,}\n",
    "\n",
    "    =============================================================================\n",
    "    HOW THESE FEATURES HELP USERS\n",
    "    =============================================================================\n",
    "\n",
    "    1. QUICK ASSESSMENT: Composite scores (0-100) give instant understanding\n",
    "    of apartment quality across different dimensions.\n",
    "\n",
    "    2. PERSONALIZED MATCHING: Users can filter/sort by features that matter\n",
    "    to them (e.g., business travelers can prioritize business_ready_score).\n",
    "\n",
    "    3. TRIP-TYPE RECOMMENDATIONS: Scores like family_suitability_score or\n",
    "    couple_romantic_score help match apartments to specific trip types.\n",
    "\n",
    "    4. DETAILED INSIGHTS: Individual features provide granular information\n",
    "    about specific aspects like bed comfort or kitchen quality.\n",
    "\n",
    "    5. TRUST INDICATORS: listing_accuracy_score and repeat_guest_indicator\n",
    "    help users identify trustworthy listings.\n",
    "\n",
    "    =============================================================================\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"✓ All features successfully added to df_features_vector!\")\n",
    "    print(\"✓ Ready for use in recommendation system!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef0dcf4-7eea-45c1-a782-d95e57c96ad2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Features Dictionary to Wide Column Format"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Convert Features Dictionary to Wide Column Format (Optimized)\n",
    "\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # =============================================================================\n",
    "    # CONFIGURATION\n",
    "    # =============================================================================\n",
    "\n",
    "    # Get sorted feature names once\n",
    "    all_features = sorted(\n",
    "        FEATURE_NAMES_MAPPING.keys(), \n",
    "        key=lambda x: FEATURE_NAMES_MAPPING[x]\n",
    "    )\n",
    "    num_features = len(all_features)\n",
    "\n",
    "    # =============================================================================\n",
    "    # BUILD WIDE FORMAT DATAFRAME\n",
    "    # =============================================================================\n",
    "\n",
    "    # Build select expression: listing_id + all feature columns extracted from map\n",
    "    select_cols = [col(\"listing_id\")] + [\n",
    "        col(\"features_dict\").getItem(feature_name).alias(feature_name)\n",
    "        for feature_name in all_features\n",
    "    ]\n",
    "\n",
    "    # Create wide format dataframe and cache\n",
    "    df_features_wide = (\n",
    "        df_features_vector\n",
    "        .select(*select_cols)\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    # Trigger cache materialization with single count action\n",
    "    row_count = df_features_wide.count()\n",
    "\n",
    "    # =============================================================================\n",
    "    # VERIFICATION (Single pass - no additional actions)\n",
    "    # =============================================================================\n",
    "\n",
    "    # Get schema info (metadata only - no action needed)\n",
    "    columns_list = df_features_wide.columns\n",
    "    num_columns = len(columns_list)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FEATURES WIDE FORMAT - CONVERSION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n  Rows:            {row_count:,}\")\n",
    "    print(f\"  Total Columns:   {num_columns}\")\n",
    "    print(f\"  Feature Columns: {num_features}\")\n",
    "    print(f\"\\n  Columns (first 20):\")\n",
    "\n",
    "    for i, col_name in enumerate(columns_list[:20], 1):\n",
    "        print(f\"    {i:2d}. {col_name}\")\n",
    "\n",
    "    if num_columns > 20:\n",
    "        print(f\"\\n    ... and {num_columns - 20} more columns\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Display Sample Data\n",
    "\n",
    "    # Display sample - Databricks optimizes this automatically\n",
    "    display(df_features_wide.limit(5))\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Summary Statistics\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CONVERSION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    ✓ CONVERSION COMPLETE\n",
    "\n",
    "    DataFrame:     df_features_wide\n",
    "    Schema:        listing_id + {num_features} feature columns\n",
    "    Dimensions:    {num_columns} columns × {row_count:,} rows\n",
    "    Data Type:     All feature values are StringType\n",
    "    Status:        Cached in memory\n",
    "\n",
    "    Features by Category:\"\"\")\n",
    "\n",
    "    for category, features in FEATURE_CATEGORIES.items():\n",
    "        if features:\n",
    "            print(f\"  • {category}: {len(features)} columns\")\n",
    "\n",
    "    print(\"\"\"\n",
    "    Ready for:\n",
    "    ✓ ML pipelines (VectorAssembler)\n",
    "    ✓ Export to Parquet/Delta\n",
    "    ✓ Statistical analysis\n",
    "    ✓ Feature transformations\n",
    "    \"\"\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # DBTITLE 1,Export Options (Optional - Uncomment as needed)\n",
    "\n",
    "    # Delta format (recommended for Databricks)\n",
    "    # df_features_wide.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"features_wide\")\n",
    "\n",
    "    # Parquet format\n",
    "    # df_features_wide.write.mode(\"overwrite\").parquet(\"/path/to/features_wide\")\n",
    "\n",
    "    # CSV format (use repartition for better parallelism on write)\n",
    "    # df_features_wide.repartition(10).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/path/to/features_csv\")\n",
    "\n",
    "    print(f\"✓ df_features_wide ready: {num_columns} cols × {row_count:,} rows\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117bf05f-162c-4a76-9e43-1d2e4a67bdd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    import json\n",
    "    import os\n",
    "    from pyspark.sql.functions import col, mean, min as spark_min, max as spark_max, stddev\n",
    "    from pyspark.sql.types import NumericType, IntegerType, DoubleType, LongType, FloatType\n",
    "\n",
    "    def generate_df_summary(df, output_path=\"/Workspace/Users/avital.ido@campus.technion.ac.il/app_data/df_summary.json\"):\n",
    "        \"\"\"\n",
    "        Generates a statistical summary of the DataFrame and saves it to a JSON file.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Define relevant columns for the app (to ensure nothing is missed)\n",
    "        # These match the keys in SCHEMA_METADATA inside app.py\n",
    "        relevant_columns = [\n",
    "            'city', 'state', 'location', 'ratings', 'property_number_of_reviews',\n",
    "            'City_Crime_Rate_Per_100K', 'Total_Fatalities', 'Median_Income',\n",
    "            'Disability_Rate', 'Median_AQI', 'amenities_count', 'price',\n",
    "            # New score columns used in the UI bar\n",
    "            'overall_quality_score', 'location_convenience_score', 'host_excellence_score',\n",
    "            'value_perception_score', 'comfort_index_score', 'family_suitability_score',\n",
    "            'business_ready_score', 'social_experience_score', 'relaxation_wellness_score',\n",
    "            'local_immersion_score'\n",
    "        ]\n",
    "        \n",
    "        # Check which columns actually exist in the Spark DataFrame\n",
    "        actual_cols = [c for c in relevant_columns if c in df.columns]\n",
    "        \n",
    "        numeric_types = (IntegerType, DoubleType, LongType, FloatType)\n",
    "        \n",
    "        # Identify numeric columns only from the relevant list\n",
    "        numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, numeric_types) and f.name in actual_cols]\n",
    "        \n",
    "        # Calculate statistics (Min, Max, Mean, StdDev)\n",
    "        agg_exprs = []\n",
    "        for c in numeric_cols:\n",
    "            agg_exprs.append(spark_min(col(c)).alias(f\"{c}_min\"))\n",
    "            agg_exprs.append(spark_max(col(c)).alias(f\"{c}_max\"))\n",
    "            agg_exprs.append(mean(col(c)).alias(f\"{c}_mean\"))\n",
    "            agg_exprs.append(stddev(col(c)).alias(f\"{c}_std\"))\n",
    "\n",
    "        # Execute aggregation\n",
    "        if agg_exprs:\n",
    "            stats_row = df.select(agg_exprs).collect()[0].asDict()\n",
    "        else:\n",
    "            stats_row = {}\n",
    "\n",
    "        summary_dict = {}\n",
    "        \n",
    "        # Build the final dictionary structure\n",
    "        for field in df.schema.fields:\n",
    "            col_name = field.name\n",
    "            \n",
    "            # Process only relevant columns\n",
    "            if col_name in actual_cols:\n",
    "                col_type = field.dataType.simpleString()\n",
    "                \n",
    "                summary_dict[col_name] = {\n",
    "                    \"type\": col_type\n",
    "                }\n",
    "                \n",
    "                # Add stats if it's a numeric column\n",
    "                if col_name in numeric_cols:\n",
    "                    summary_dict[col_name].update({\n",
    "                        \"min\": stats_row.get(f\"{col_name}_min\"),\n",
    "                        \"max\": stats_row.get(f\"{col_name}_max\"),\n",
    "                        \"mean\": stats_row.get(f\"{col_name}_mean\"),\n",
    "                        \"std\": stats_row.get(f\"{col_name}_std\")\n",
    "                    })\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Save dictionary to JSON file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_dict, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"✅ Summary saved successfully to: {output_path}\")\n",
    "        return summary_dict\n",
    "\n",
    "    res_dict = generate_df_summary(df_features_wide)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77cdbb32-4b7b-4493-889b-34a91e6cad87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save the data with 'State' partition"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    import json\n",
    "    import shutil\n",
    "    from pyspark.sql.functions import col, lower, trim, regexp_replace\n",
    "\n",
    "    # 1. Define Paths\n",
    "    base_output_dir = \"/Workspace/Users/avital.ido@campus.technion.ac.il/final_app_data\"\n",
    "    data_dir = os.path.join(base_output_dir, \"data\")\n",
    "    temp_dir = os.path.join(base_output_dir, \"_temp_partitioned\")\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Load & Clean Data in Spark\n",
    "    print(\"⏳ Loading data from Spark...\")\n",
    "    df_spark = df_features_wide\n",
    "    selected_columns = [\n",
    "        'name', \n",
    "        'listing_name', \n",
    "        'url', \n",
    "        'city', \n",
    "        'state', \n",
    "        'location',\n",
    "        'county',\n",
    "        'price', \n",
    "        'guests', \n",
    "        'pets_allowed',\n",
    "        'amenities_count',\n",
    "        'description', \n",
    "        'highlights', \n",
    "        'image',      \n",
    "        'images',      \n",
    "        'seller_info', \n",
    "        'reviews', \n",
    "        'ratings', \n",
    "        'property_number_of_reviews',\n",
    "        'risk_score_raw', \n",
    "        'City_Crime_Rate_Per_100K',\n",
    "        'Total_Fatalities',\n",
    "        'Median_Income',\n",
    "        'Disability_Rate',\n",
    "        'Median_AQI',\n",
    "        'overall_quality_score',\n",
    "        'location_convenience_score',\n",
    "        'host_excellence_score',\n",
    "        'value_perception_score',\n",
    "        'comfort_index_score',\n",
    "        'family_suitability_score',\n",
    "        'business_ready_score',\n",
    "        'social_experience_score',\n",
    "        'relaxation_wellness_score',\n",
    "        'local_immersion_score'\n",
    "    ]\n",
    "    \n",
    "    df_clean = df_spark.select(*[col(c) for c in selected_columns if c in df_spark.columns]) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "    .withColumn(\"final_score\", col(\"effective_rating\")) \\\n",
    "    .withColumn(\"state_clean\", lower(trim(regexp_replace(col(\"state\"), \"\\\\.$\", \"\")))) \\\n",
    "    .withColumn(\"city_clean\", lower(trim(col(\"city\")))) \\\n",
    "    .filter(col(\"state_clean\").isNotNull() & (col(\"state_clean\") != \"\"))\n",
    "\n",
    "    # Cache the DataFrame since we use it twice (mapping + write)\n",
    "    df_clean.cache()\n",
    "\n",
    "    # Trigger cache and get count\n",
    "    total_rows = df_clean.count()\n",
    "    print(f\"\uD83D\uDCCA Total rows to process: {total_rows:,}\")\n",
    "\n",
    "    # 3. Create Location Mapping (City -> State File)\n",
    "    print(\"\uD83D\uDDFA️ Building location map...\")\n",
    "\n",
    "    # Get distinct city-state pairs efficiently\n",
    "    mapping_rows = (\n",
    "        df_clean\n",
    "        .select(\"city_clean\", \"state_clean\")\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    location_map = {}\n",
    "    unique_states = set()\n",
    "\n",
    "    for row in mapping_rows:\n",
    "        city = row[\"city_clean\"]\n",
    "        state = row[\"state_clean\"]\n",
    "        if city and state:\n",
    "            location_map[city] = state\n",
    "            location_map[state] = state\n",
    "            unique_states.add(state)\n",
    "\n",
    "    # Save location map\n",
    "    map_path = os.path.join(base_output_dir, \"location_map.json\")\n",
    "    with open(map_path, \"w\") as f:\n",
    "        json.dump(location_map, f)\n",
    "\n",
    "    print(f\"✅ Location map saved. Mapped {len(location_map)} locations.\")\n",
    "\n",
    "    # 4. Write ALL state parquet files in ONE parallel Spark operation\n",
    "    print(f\"\uD83D\uDCBE Writing parquet files for {len(unique_states)} states using Spark...\")\n",
    "\n",
    "    # Clean temp directory if exists\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "    # Write partitioned by state - Spark handles parallelism automatically\n",
    "    # Repartition ensures one partition per state for single-file output\n",
    "    (\n",
    "        df_clean\n",
    "        .repartition(len(unique_states), \"state_clean\")\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"state_clean\")\n",
    "        .parquet(temp_dir)\n",
    "    )\n",
    "\n",
    "    print(\"\uD83D\uDCC1 Reorganizing files to final structure...\")\n",
    "\n",
    "    # 5. Move files from partitioned structure to flat {state}.parquet format\n",
    "    # This is just file I/O - very fast\n",
    "    for state in unique_states:\n",
    "        partition_path = os.path.join(temp_dir, f\"state_clean={state}\")\n",
    "        \n",
    "        if os.path.exists(partition_path):\n",
    "            # Find the parquet file(s) in the partition directory\n",
    "            parquet_files = [\n",
    "                f for f in os.listdir(partition_path) \n",
    "                if f.endswith(\".parquet\") and not f.startswith(\"_\")\n",
    "            ]\n",
    "            \n",
    "            if parquet_files:\n",
    "                src_file = os.path.join(partition_path, parquet_files[0])\n",
    "                dst_file = os.path.join(data_dir, f\"{state}.parquet\")\n",
    "                \n",
    "                # If multiple part files exist, merge them\n",
    "                if len(parquet_files) > 1:\n",
    "                    # Read all parts and write as single file\n",
    "                    (\n",
    "                        spark.read.parquet(partition_path)\n",
    "                        .coalesce(1)\n",
    "                        .write\n",
    "                        .mode(\"overwrite\")\n",
    "                        .parquet(dst_file + \"_tmp\")\n",
    "                    )\n",
    "                    # Move the actual file\n",
    "                    tmp_dir = dst_file + \"_tmp\"\n",
    "                    for f in os.listdir(tmp_dir):\n",
    "                        if f.endswith(\".parquet\") and not f.startswith(\"_\"):\n",
    "                            shutil.move(os.path.join(tmp_dir, f), dst_file)\n",
    "                            break\n",
    "                    shutil.rmtree(tmp_dir)\n",
    "                else:\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "\n",
    "    # 6. Cleanup\n",
    "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    df_clean.unpersist()\n",
    "\n",
    "    print(f\"\uD83C\uDF89 All Done! Saved {len(unique_states)} state files to {data_dir}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d18e1cb-30c1-455a-85de-e7f9b9c22611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creates the Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4f4c01-15db-45b0-a594-5b08bdf02d05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "important imports"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /databricks/python3/lib/python3.11/site-packages (1.3.0)\nRequirement already satisfied: openai in /databricks/python3/lib/python3.11/site-packages (1.35.3)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (1.5.3)\nRequirement already satisfied: streamlit in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (1.53.1)\nRequirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (1.23.5)\nRequirement already satisfied: scipy>=1.5.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.11/site-packages (from openai) (3.5.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from openai) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /databricks/python3/lib/python3.11/site-packages (from openai) (1.10.6)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.11/site-packages (from openai) (1.2.0)\nRequirement already satisfied: tqdm>4 in /databricks/python3/lib/python3.11/site-packages (from openai) (4.65.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from openai) (4.15.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2022.7)\nRequirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from streamlit) (6.0.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<7,>=5.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from streamlit) (6.2.6)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (8.0.4)\nRequirement already satisfied: packaging>=20 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (23.2)\nRequirement already satisfied: pillow<13,>=7.1.0 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (9.4.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (4.24.1)\nRequirement already satisfied: pyarrow>=7.0 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (14.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (2.31.0)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (8.2.2)\nRequirement already satisfied: toml<2,>=0.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from streamlit) (0.10.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (3.1.27)\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from streamlit) (0.9.1)\nRequirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /databricks/python3/lib/python3.11/site-packages (from streamlit) (6.3.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /databricks/python3/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.17.3)\nRequirement already satisfied: narwhals>=1.27.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-00f141ac-3d90-4de6-97be-2014f2439448/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.15.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (1.26.16)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.1.1)\nRequirement already satisfied: attrs>=17.4.0 in /databricks/python3/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (22.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.18.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mWarning: statements after `dbutils.library.restartPython()` will execute before Python is restarted.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn openai pandas streamlit\n",
    "\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c658b158-5f19-4f1e-9db8-0f4d8cc62a64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "app.py"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import openai\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from typing import Dict, List, Any, Optional\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "# --- Configuration & Setup ---\n",
    "\n",
    "try:\n",
    "    api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "except:\n",
    "    api_key = st.text_input(\"Enter OpenAI API Key\", type=\"password\")\n",
    "\n",
    "if not api_key:\n",
    "    st.warning(\"Please provide an API Key to continue.\")\n",
    "    st.stop()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "@st.cache_data\n",
    "def load_schema_metadata() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads metadata (min/max/mean) from the JSON generated by the Data Engineering pipeline.\n",
    "    \"\"\"\n",
    "    if os.path.exists(SUMMARY_FILE):\n",
    "        try:\n",
    "            with open(SUMMARY_FILE, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            # Fallback if file is corrupt\n",
    "            return {}\n",
    "            \n",
    "    # Fallback to hardcoded defaults if file is missing (Optional)\n",
    "    return {\n",
    "        'price': {'type': 'double', 'mean': 200.0},\n",
    "        'ratings': {'type': 'double', 'mean': 4.5}\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Paths ---\n",
    "BASE_DIR = \"/Workspace/Users/avital.ido@campus.technion.ac.il/final_app_data\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "MAP_FILE = os.path.join(BASE_DIR, \"location_map.json\")\n",
    "SUMMARY_FILE = os.path.join(BASE_DIR, \"df_summary.json\")\n",
    "SCHEMA_METADATA = load_schema_metadata()\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def safe_parse(value: Any, default: Any = None) -> Any:\n",
    "    if value is None or value == \"\": return default\n",
    "    if not isinstance(value, str): return value\n",
    "    try: return json.loads(value)\n",
    "    except:\n",
    "        try: return ast.literal_eval(value)\n",
    "        except: return default\n",
    "\n",
    "def get_score_color(score: float) -> str:\n",
    "    try: score = float(score)\n",
    "    except: return \"#808080\"\n",
    "    score = max(0, min(10, score))\n",
    "    if score < 5: return \"#21c354\"\n",
    "    elif score < 7.5: return \"#ffa500\"\n",
    "    else: return \"#ff4b4b\"\n",
    "\n",
    "def get_gradient_color(score_val: float) -> str:\n",
    "    \"\"\"\n",
    "    Calculates a color from Red (0) to Yellow (50) to Green (100).\n",
    "    Returns a hex string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        val = float(score_val)\n",
    "    except:\n",
    "        return \"#e0e0e0\" # Gray for N/A\n",
    "    \n",
    "    val = max(0, min(10, val)) \n",
    "    \n",
    "    if val <= 5:\n",
    "        # Interpolate Red -> Yellow\n",
    "        ratio = val / 5.0\n",
    "        r = int(255 + (255 - 255) * ratio) # 255 -> 255\n",
    "        g = int(75 + (193 - 75) * ratio)   # 75 -> 193\n",
    "        b = int(75 + (7 - 75) * ratio)     # 75 -> 7\n",
    "    else:\n",
    "        # Interpolate Yellow -> Green\n",
    "        ratio = (val - 5) / 5.0\n",
    "        r = int(255 + (33 - 255) * ratio)  # 255 -> 33\n",
    "        g = int(193 + (195 - 193) * ratio) # 193 -> 195\n",
    "        b = int(7 + (84 - 7) * ratio)      # 7 -> 84\n",
    "        \n",
    "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "# --- Optimized Data Loading ---\n",
    "\n",
    "@st.cache_data\n",
    "def load_location_map() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Loads the JSON mapping that links cities/states to their filename code.\n",
    "    Example: {\"miami\": \"fl\", \"florida\": \"fl\"}\n",
    "    \"\"\"\n",
    "    if os.path.exists(MAP_FILE):\n",
    "        with open(MAP_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    fallback_path = \"/Workspace/Users/avital.ido@campus.technion.ac.il/unique_locations_list.json\"\n",
    "    if os.path.exists(fallback_path):\n",
    "        try:\n",
    "            with open(fallback_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    return {str(item).lower(): str(item).lower() for item in data}\n",
    "                \n",
    "                return data\n",
    "        except Exception:\n",
    "            return {} \n",
    "            \n",
    "    return {}\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def load_data_for_state(state_code: str):\n",
    "    \"\"\"\n",
    "    Loads data and performs strict deduplication based on Title/Name and URL.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DATA_DIR, f\"{state_code}.parquet\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        st.error(f\"Data file for state '{state_code}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Standardize the Name column for deduplication\n",
    "        # Checks 'listing_title', then 'name', then 'listing_name'\n",
    "        if 'listing_title' in df.columns:\n",
    "            df['dup_check_name'] = df['listing_title']\n",
    "        elif 'name' in df.columns:\n",
    "            df['dup_check_name'] = df['name']\n",
    "        else:\n",
    "            df['dup_check_name'] = df.get('listing_name', 'Unknown')\n",
    "\n",
    "        # Drop duplicates based on URL first (fastest)\n",
    "        if 'url' in df.columns:\n",
    "            df = df.drop_duplicates(subset=['url'])\n",
    "            \n",
    "        # Drop duplicates based on Name (stricter)\n",
    "        # This ensures we don't see the same apartment with a slightly different link\n",
    "        if 'dup_check_name' in df.columns:\n",
    "            df = df.drop_duplicates(subset=['dup_check_name'])\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Logic ---\n",
    "\n",
    "def find_similar_properties(property_data: pd.DataFrame, target_record: Dict, top_k: int = 50):\n",
    "    top_k = min(top_k, len(property_data))\n",
    "    # FIXED: Uncommented and fixed logic to ensure it returns a DataFrame\n",
    "    if property_data.empty: return property_data\n",
    "    \n",
    "    numeric_cols = [col for col, meta in SCHEMA_METADATA.items() if meta.get('type') in ['double', 'int', 'bigint', 'float'] and col in property_data.columns]\n",
    "    \n",
    "    df_numeric = property_data[numeric_cols].copy().fillna(0)\n",
    "    target_vector = []\n",
    "    valid_cols = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        val = target_record.get(col)\n",
    "        if val is not None and val != \"\":\n",
    "            try: target_vector.append(float(val)); valid_cols.append(col)\n",
    "            except: pass\n",
    "            \n",
    "    if not valid_cols: \n",
    "        return property_data.head(top_k)\n",
    "\n",
    "    X = df_numeric[valid_cols]\n",
    "    scaler = StandardScaler()\n",
    "    try:\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        target_vector_scaled = scaler.transform([target_vector])\n",
    "        \n",
    "        k = min(len(X), top_k)\n",
    "        nn = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "        nn.fit(X_scaled)\n",
    "        \n",
    "        dists, idxs = nn.kneighbors(target_vector_scaled)\n",
    "        \n",
    "        sim_props = property_data.iloc[idxs[0]].copy()\n",
    "        sim_props['match_score'] = 1 / (1 + dists[0])\n",
    "        \n",
    "        return sim_props\n",
    "    except: \n",
    "        return property_data.head(top_k)\n",
    "\n",
    "def update_best_match(df, row, decision, selections=None):\n",
    "    \"\"\"\n",
    "    Updates the recommendation queue based on user interaction.\n",
    "    If user LIKES a property, the remaining queue is re-sorted to show similar items first.\n",
    "    \"\"\"\n",
    "    if selections is None: selections = []\n",
    "    \n",
    "    # Normalize the name for the selections list (using logic from user request)\n",
    "    current_name = row.get('listing_title', row.get('name', 'Unknown'))\n",
    "    \n",
    "    # Handle LIKE decision\n",
    "    if decision == \"like\":\n",
    "        prop_data = row.to_dict() if hasattr(row, 'to_dict') else row\n",
    "        \n",
    "        # Ensure name consistency in saved data\n",
    "        prop_data['name'] = current_name\n",
    "        \n",
    "        # Add to favorites (Double check against existing names/urls in selections)\n",
    "        existing_identifiers = {item.get('url') for item in selections} | {item.get('name') for item in selections}\n",
    "        \n",
    "        if prop_data.get('url') not in existing_identifiers and current_name not in existing_identifiers:\n",
    "            selections.append(prop_data)\n",
    "            \n",
    "        # --- SMART RE-RANKING LOGIC ---\n",
    "        # If user liked this, we find remaining items that are closest to this one\n",
    "        # This creates a \"More like this\" effect dynamically\n",
    "        \n",
    "        # Define features for similarity (Must allow for missing columns)\n",
    "        potential_features = [\n",
    "            'price', 'ratings', 'property_number_of_reviews', \n",
    "            'amenities_count', 'City_Crime_Rate_Per_100K', \n",
    "            'Median_Income', 'overall_quality_score'\n",
    "        ]\n",
    "        \n",
    "        valid_features = [c for c in potential_features if c in df.columns]\n",
    "        \n",
    "        # We need at least 2 rows (current + others) and valid features to re-rank\n",
    "        if valid_features and len(df) > 1:\n",
    "            try:\n",
    "                # 1. Get vector of the LIKED property\n",
    "                target_vector = row[valid_features].fillna(0).values.reshape(1, -1)\n",
    "                \n",
    "                # 2. Get vectors of REMAINING properties (excluding current)\n",
    "                remaining_df = df.iloc[1:].copy()\n",
    "                candidate_vectors = remaining_df[valid_features].fillna(0).values\n",
    "                \n",
    "                # 3. Calculate distance (Lower is better/more similar)\n",
    "                dists = euclidean_distances(target_vector, candidate_vectors).flatten()\n",
    "                \n",
    "                # 4. Sort remaining DF by distance\n",
    "                remaining_df['sim_dist'] = dists\n",
    "                remaining_df = remaining_df.sort_values('sim_dist', ascending=True).drop(columns=['sim_dist'])\n",
    "                \n",
    "                # Return re-ordered list\n",
    "                return remaining_df.reset_index(drop=True), selections\n",
    "            except:\n",
    "                pass # If math fails, fall back to standard behavior\n",
    "\n",
    "    # Standard behavior: Just remove the current item and move to next\n",
    "    if len(df) > 0:\n",
    "        return df.iloc[1:].reset_index(drop=True), selections\n",
    "        \n",
    "    return df, selections\n",
    "\n",
    "def load_filtered_data(max_price: float, location_filter: str, required_columns: list):\n",
    "    \"\"\"\n",
    "    Loads data and removes duplicates immediately to prevent same property appearing twice.\n",
    "    \"\"\"\n",
    "    data_path = \"/dbfs/FileStore/Users/avital.ido@campus.technion.ac.il/eliezer_data\"\n",
    "    try:\n",
    "        \n",
    "        # filters = [('price', '<=', float(max_price))]\n",
    "        filters = [\n",
    "                [('price', '<=', float(max_price)), ('city', '==', location_filter)],\n",
    "                [('price', '<=', float(max_price)), ('state', '==', location_filter)],\n",
    "                [('price', '<=', float(max_price)), ('county', '==', location_filter)]\n",
    "            ]\n",
    "        df = pd.read_parquet(data_path, columns=required_columns, filters=filters, engine='pyarrow')\n",
    "        \n",
    "        # Drop Duplicates Logic\n",
    "        if 'url' in df.columns:\n",
    "            df = df.drop_duplicates(subset=['url'])\n",
    "        else:\n",
    "            cols_to_check = [c for c in ['name', 'city', 'price'] if c in df.columns]\n",
    "            if cols_to_check:\n",
    "                df = df.drop_duplicates(subset=cols_to_check)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading filtered data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def find_best_match(user_location: str, max_price: float, requirements: str):\n",
    "    # 1. Resolve State File\n",
    "    location_map = load_location_map()\n",
    "    search_term = user_location.lower().strip()\n",
    "    \n",
    "    # Try to find the state code from the map\n",
    "    state_code = location_map.get(search_term)\n",
    "    \n",
    "    if not state_code:\n",
    "        # Fallback logic\n",
    "        required_cols = list(SCHEMA_METADATA.keys()) + ['name', 'listing_name', 'image', 'url', 'seller_info', 'guests', 'highlights', 'description', 'reviews', 'images', 'pets_allowed', 'listing_title']\n",
    "        return load_filtered_data(max_price, user_location, required_cols)\n",
    "\n",
    "    # 2. Load SPECIFIC State Data (Now deduped by name)\n",
    "    df = load_data_for_state(state_code)\n",
    "    \n",
    "    if df.empty: return pd.DataFrame()\n",
    "    \n",
    "    # 3. Filter by Price\n",
    "    df = df[df['price'] <= float(max_price)]\n",
    "    \n",
    "    # 4. Filter by specific City\n",
    "    if search_term != state_code:\n",
    "         df = df[\n",
    "            df['city'].astype(str).str.lower().str.contains(search_term, na=False) | \n",
    "            df['state'].astype(str).str.lower().str.contains(search_term, na=False)\n",
    "        ]\n",
    "    \n",
    "    # --- ENHANCED FILTERING: Remove items already in Favorites by Name AND URL ---\n",
    "    if 'selections' in st.session_state and st.session_state.selections:\n",
    "        liked_urls = {item.get('url') for item in st.session_state.selections}\n",
    "        liked_names = {item.get('name') for item in st.session_state.selections} # Collect names\n",
    "        \n",
    "        # Filter by URL\n",
    "        if 'url' in df.columns:\n",
    "            df = df[~df['url'].isin(liked_urls)]\n",
    "            \n",
    "        # Filter by Name/Title (Handles the user request)\n",
    "        # We check against 'name', 'listing_name', or 'listing_title'\n",
    "        cols_to_check = [c for c in ['name', 'listing_name', 'listing_title'] if c in df.columns]\n",
    "        for c in cols_to_check:\n",
    "             df = df[~df[c].isin(liked_names)]\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    if df.empty: return pd.DataFrame()\n",
    "    # 5. LLM Logic\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a highly intelligent data matching assistant for an Airbnb search engine.\n",
    "    Your task is to translate user requirements into a structured JSON record that follows the provided schema.\n",
    "\n",
    "    RULES FOR VALUE ASSIGNMENT:\n",
    "    1. UNDERSTAND CONTEXT: Analyze the user's free text. If they ask for \"safety\", reduce 'City_Crime_Rate_Per_100K'. If they want \"luxury\", increase 'Median_Income' and 'amenities_count'.\n",
    "    2. NUMERIC FIELDS:\n",
    "       - If the user provides a requirement that maps to a numeric field, set a realistic value based on the [min, max, mean, std] provided.\n",
    "       - IMPORTANT: If the user DOES NOT mention or imply anything about a numeric field, you MUST set its value to exactly the 'mean' provided in the schema.\n",
    "    3. STRING FIELDS:\n",
    "       - If information is provided or can be inferred (like City/State from location), fill it.\n",
    "       - If NO information is provided for a string field, set it to an empty string \"\".\n",
    "    4. CONSISTENCY: Ensure all keys from the schema are present in the output.\n",
    "\n",
    "    Schema Metadata:\n",
    "    {list(SCHEMA_METADATA.keys())}\n",
    "\n",
    "    Return ONLY a valid JSON object.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        res = client.chat.completions.create(model=\"gpt-4o\", messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f\"Loc: {user_location}, Max: {max_price}, Req: {requirements}\"}], response_format={\"type\": \"json_object\"}, temperature=0.7)\n",
    "        target = json.loads(res.choices[0].message.content)\n",
    "        target['price'] = float(max_price)\n",
    "    except: target = {'price': float(max_price)}\n",
    "    \n",
    "    return find_similar_properties(df, target)\n",
    "\n",
    "# --- CALLBACK FUNCTION ---\n",
    "\n",
    "def handle_swipe(decision):\n",
    "    if st.session_state.current_df is None or len(st.session_state.current_df) == 0:\n",
    "        return\n",
    "\n",
    "    current_row = st.session_state.current_df.iloc[0]\n",
    "    \n",
    "    new_df, new_selections = update_best_match(\n",
    "        st.session_state.current_df, \n",
    "        current_row, \n",
    "        decision, \n",
    "        st.session_state.selections\n",
    "    )\n",
    "    \n",
    "    st.session_state.current_df = new_df\n",
    "    st.session_state.selections = new_selections\n",
    "    st.session_state.gallery_idx = 0\n",
    "\n",
    "# --- UI ---\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Airbnb Matcher\", page_icon=\"\uD83C\uDFE1\", layout=\"wide\")\n",
    "    \n",
    "    if 'page' not in st.session_state: st.session_state.page = 'form'\n",
    "    if 'current_df' not in st.session_state: st.session_state.current_df = None\n",
    "    if 'selections' not in st.session_state: st.session_state.selections = []\n",
    "    if 'gallery_idx' not in st.session_state: st.session_state.gallery_idx = 0\n",
    "    if 'last_prop_id' not in st.session_state: st.session_state.last_prop_id = None\n",
    "        \n",
    "    st.markdown(\"\"\"<style>\n",
    "        .block-container { padding-top: 2rem; }\n",
    "        .header-container { text-align: center; margin-bottom: 30px; }\n",
    "        .header-title { \n",
    "            font-size: 4rem; \n",
    "            font-weight: 800; \n",
    "            background: linear-gradient(90deg, #FF5A5F 0%, #FF385C 100%);\n",
    "            -webkit-background_clip: text;\n",
    "            -webkit-text-fill-color: transparent;\n",
    "            margin-bottom: 0;\n",
    "            cursor: pointer;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .header-subtitle { font-size: 1.2rem; color: #717171; font-weight: 500; margin-top: -10px; text-align: center; }\n",
    "        .main-card { background: white; padding: 25px; border-radius: 15px; box-shadow: 0 4px 15px rgba(0,0,0,0.08); margin-bottom: 20px; }\n",
    "        .score-badge { font-size: 2em; font-weight: bold; padding: 10px; border-radius: 12px; color: white; text-align: center; display: block; }\n",
    "        .highlight-item { background: #f8f9fa; border-left: 4px solid #FF5A5F; padding: 10px; margin-bottom: 8px; border-radius: 4px; }\n",
    "        .reviews-container { max-height: 250px; overflow-y: auto; padding: 10px; border: 1px solid #eee; border-radius: 8px; margin-top: 10px; }\n",
    "        .review-bubble { background: #f7f7f7; padding: 12px; border-radius: 12px; margin-bottom: 10px; font-style: italic; border-left: 3px solid #ddd; font-size: 0.9em; color: #444; }\n",
    "        .custom-img { width: 100%; border-radius: 10px; object-fit: cover; height: 400px; display: block; margin-left: auto; margin-right: auto; }\n",
    "        div.stButton > button.title-btn { background: none; border: none; font-size: 4rem; font-weight: 800; color: #FF5A5F; width: 100%; }\n",
    "        .about-card { padding: 20px; border-radius: 10px; background: #f9f9f9; margin-bottom: 15px; border-left: 5px solid #FF5A5F; }\n",
    "        button[data-testid=\"baseButton-secondary\"] { border-color: #ff4b4b !important; color: #ff4b4b !important; font-weight: bold; }\n",
    "        button[data-testid=\"baseButton-secondary\"]:hover { background-color: #ff4b4b !important; color: white !important; }\n",
    "        button[data-testid=\"baseButton-primary\"] { background-color: #21c354 !important; border-color: #21c354 !important; font-weight: bold; }\n",
    "        button[data-testid=\"baseButton-primary\"]:hover { background-color: #1a9c43 !important; border-color: #1a9c43 !important; }\n",
    "\n",
    "        .score-container {\n",
    "            display: flex;\n",
    "            flex-direction: row;\n",
    "            width: 100%;          /* Force container to take full width */\n",
    "            gap: 5px;             /* Small gap to fit all 10 items */\n",
    "            padding: 5px 0;\n",
    "            margin-bottom: 15px;\n",
    "            /* overflow-x is removed so items shrink to fit instead of scroll */\n",
    "        }\n",
    "        .score-container::-webkit-scrollbar {\n",
    "            display: none; /* Hide scrollbar for Chrome/Safari */\n",
    "        }\n",
    "        .score-card {\n",
    "            flex: 1;              /* Grow to fill space equally */\n",
    "            min-width: 0;         /* Allow item to shrink below content size if needed */\n",
    "            text-align: center;\n",
    "            padding: 6px 2px;\n",
    "            border-radius: 6px;\n",
    "            color: #333;\n",
    "            box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
    "            border: 1px solid rgba(0,0,0,0.05);\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "        }\n",
    "        \n",
    "        .score-val { \n",
    "            font-weight: 800; \n",
    "            font-size: 0.9em; \n",
    "            line-height: 1.1;\n",
    "        }\n",
    "        \n",
    "        .score-label { \n",
    "            font-size: 0.55em;    /* Slightly smaller to prevent text overflow */\n",
    "            line-height: 1.1; \n",
    "            font-weight: 600; \n",
    "            text-transform: uppercase; \n",
    "            margin-top: 2px;\n",
    "            white-space: nowrap;  /* Keep text on one line */\n",
    "            overflow: hidden;     /* Hide overflow */\n",
    "            text-overflow: ellipsis; /* Add ... if text is too long */\n",
    "            width: 100%;\n",
    "        }\n",
    "\n",
    "        .score-card {\n",
    "            /* ... (keep existing properties) ... */\n",
    "            position: relative; /* Needed to anchor the tooltip */\n",
    "            cursor: pointer;    /* Indicates interactivity */\n",
    "        }\n",
    "\n",
    "        /* The hidden text box */\n",
    "        .score-card .tooltip-text {\n",
    "            visibility: hidden;\n",
    "            width: 160px;\n",
    "            background-color: #333;\n",
    "            color: #fff;\n",
    "            text-align: center;\n",
    "            border-radius: 6px;\n",
    "            padding: 8px;\n",
    "            font-size: 0.7rem;\n",
    "            font-weight: normal;\n",
    "            line-height: 1.2;\n",
    "            \n",
    "            /* Position the tooltip */\n",
    "            position: absolute;\n",
    "            z-index: 10;\n",
    "            bottom: 110%; /* Place above the card */\n",
    "            left: 50%;\n",
    "            margin-left: -80px; /* Center it (half of width) */\n",
    "            \n",
    "            /* Fade effect */\n",
    "            opacity: 0;\n",
    "            transition: opacity 0.3s;\n",
    "            box-shadow: 0 4px 6px rgba(0,0,0,0.3);\n",
    "        }\n",
    "\n",
    "        /* Show the tooltip on hover */\n",
    "        .score-card:hover .tooltip-text {\n",
    "            visibility: visible;\n",
    "            opacity: 1;\n",
    "        }\n",
    "        \n",
    "        /* Little arrow at the bottom of the tooltip */\n",
    "        .score-card .tooltip-text::after {\n",
    "            content: \"\";\n",
    "            position: absolute;\n",
    "            top: 100%;\n",
    "            left: 50%;\n",
    "            margin-left: -5px;\n",
    "            border-width: 5px;\n",
    "            border-style: solid;\n",
    "            border-color: #333 transparent transparent transparent;\n",
    "        }\n",
    "    </style>\"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    # Header\n",
    "    _, col_center, _ = st.columns([1, 6, 1])\n",
    "    with col_center:\n",
    "        if st.button(\"\uD83C\uDFE1 AirBNB Matcher\", key=\"home_btn\", use_container_width=True):\n",
    "            st.session_state.selections = []\n",
    "            st.session_state.current_df = None\n",
    "            st.session_state.page = 'form'\n",
    "            st.rerun()\n",
    "        st.markdown('<p class=\"header-subtitle\">Developed by Ido Avital & Eliezer Mashivoc & Mike Gruntov & Evgeny Mishlyakov</p>', unsafe_allow_html=True)\n",
    "\n",
    "    # PAGE 1: FORM\n",
    "    if st.session_state.page == 'form':\n",
    "        left_col, center_col, right_col = st.columns([1, 2, 1])\n",
    "        with center_col:\n",
    "            st.markdown(\"<h3 style='text-align: center;'>Find your perfect stay</h3><br>\", unsafe_allow_html=True)\n",
    "            \n",
    "            # Load locations from map\n",
    "            location_map = load_location_map()\n",
    "            available_locs = sorted(list(location_map.keys()))\n",
    "            \n",
    "            if available_locs:\n",
    "                loc_display = [l.title() for l in available_locs]\n",
    "                selected_loc_disp = st.selectbox(\"Select Target Location\", options=loc_display)\n",
    "                loc = selected_loc_disp.lower() \n",
    "            else:\n",
    "                loc = st.text_input(\"Target Location\").lower()\n",
    "            \n",
    "            price = st.number_input(\"Maximum Price per Night ($)\", min_value=50, value=1000, step=10)\n",
    "            req = st.text_area(\"Describe your dream vacation\", height=145, placeholder=\"e.g. Quiet apartment near the beach...\")\n",
    "            st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "            \n",
    "            if st.button(\"\uD83D\uDE80 Start Search\", use_container_width=True):\n",
    "                loading_msg = \"\uD83D\uDD0D Scanning Airbnb ecosystem... (This may take up to 90s)\"\n",
    "                with st.spinner(loading_msg):\n",
    "                    res = find_best_match(loc, price, req)\n",
    "                    if not res.empty:\n",
    "                        st.session_state.current_df = res\n",
    "                        st.session_state.page = 'swipe'\n",
    "                        st.rerun()\n",
    "                    else: st.error(\"No matches found. Please try different criteria.\")\n",
    "            \n",
    "            st.markdown(\"<div style='height: 10px'></div>\", unsafe_allow_html=True)\n",
    "            if st.button(\"ℹ️ About This App\", use_container_width=True):\n",
    "                st.session_state.page = 'about'\n",
    "                st.rerun()\n",
    "\n",
    "    # PAGE 4: ABOUT\n",
    "    elif st.session_state.page == 'about':\n",
    "        st.subheader(\"ℹ️ About AirBNB Matcher\")\n",
    "        st.markdown(\"\"\"\n",
    "        <div class=\"about-card\"><h3>\uD83C\uDFAF The Mission</h3><p>Finding the perfect Airbnb can be overwhelming. This tool uses <b>AI and Big Data</b> to match you with properties that fit your specific needs—not just your budget.</p></div>\n",
    "        <div class=\"about-card\"><h3>⚙️ How It Works</h3><ul><li><b>Smart Filtering:</b> We scan partitioned data for maximum speed.</li><li><b>AI Analysis:</b> GPT-4 understands your text description.</li><li><b>Similarity Matching:</b> KNN algorithms find properties that match your profile.</li></ul></div>\n",
    "        <div class=\"about-card\"><h3>\uD83D\uDC68‍\uD83D\uDCBB Developers</h3><p>Built by <b>Ido Avital, Eliezer Mashivoc, Mike Gruntov, Evgeny Mishlyakov</b>.</p></div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "        if st.button(\"⬅️ Back to Search\", use_container_width=True):\n",
    "            st.session_state.page = 'form'\n",
    "            st.rerun()\n",
    "\n",
    "    # PAGE 2: SWIPE\n",
    "    elif st.session_state.page == 'swipe':\n",
    "        if st.session_state.current_df is None or len(st.session_state.current_df) == 0:\n",
    "            st.success(\"\uD83C\uDF89 Reviewed all!\"); st.session_state.page = 'results'; st.rerun(); return\n",
    "\n",
    "        row = st.session_state.current_df.iloc[0]\n",
    "        listing_name = row.get('name', row.get('listing_name', 'Listing'))\n",
    "        parts = listing_name.split(' · ', 2)\n",
    "        name = parts[0]\n",
    "        rating = parts[1]\n",
    "        short_description = parts[2]\n",
    "\n",
    "        title = row.get('listing_title', name)\n",
    "\n",
    "        pets_allowed = row.get('pets_allowed', 'No')\n",
    "        if pets_allowed != 'true':\n",
    "            pets_allowed = 'No Pets Allowed'\n",
    "        else:\n",
    "            pets_allowed = 'Pets Allowed'\n",
    "            \n",
    "        df_len = len(st.session_state.current_df)\n",
    "        prop_key = f\"{df_len}_{name[:10]}\" \n",
    "        \n",
    "        if st.session_state.last_prop_id != name:\n",
    "            st.session_state.gallery_idx = 0\n",
    "            st.session_state.last_prop_id = name\n",
    "        \n",
    "        main_img = row.get('image', None)\n",
    "        extra_imgs = safe_parse(row.get('images'), [])\n",
    "        gallery_images = (extra_imgs if isinstance(extra_imgs, list) else [])\n",
    "        if not gallery_images: gallery_images = [\"https://via.placeholder.com/800x600?text=No+Image\"]\n",
    "        \n",
    "        idx = st.session_state.gallery_idx\n",
    "        if idx >= len(gallery_images): idx = 0\n",
    "        \n",
    "        with st.container():\n",
    "            st.markdown('<div class=\"main-card\">', unsafe_allow_html=True)\n",
    "            st.markdown(f\"<h2>{title}</h2>\", unsafe_allow_html=True)\n",
    "            st.markdown(f\"\uD83D\uDCCD {row.get('city')}, {row.get('state')}<hr>\", unsafe_allow_html=True)\n",
    "            \n",
    "            score_columns = [\n",
    "                ('Overall', 'overall_quality_score'),\n",
    "                ('Location', 'location_convenience_score'),\n",
    "                ('Host', 'host_excellence_score'),\n",
    "                ('Value', 'value_perception_score'),\n",
    "                ('Comfort', 'comfort_index_score'),\n",
    "                ('Family', 'family_suitability_score'),\n",
    "                ('Business', 'business_ready_score'),\n",
    "                ('Social', 'social_experience_score'),\n",
    "                ('Wellness', 'relaxation_wellness_score'),\n",
    "                ('Local', 'local_immersion_score')\n",
    "            ]\n",
    "            SCORE_EXPLANATIONS = {\n",
    "                'overall_quality_score': \"Aggregated score combining all metrics for a final verdict.\",\n",
    "                'location_convenience_score': \"Proximity to city center, transit, and major landmarks.\",\n",
    "                'host_excellence_score': \"Based on host response time, superhost status, and ratings.\",\n",
    "                'value_perception_score': \"Price-per-night relative to amenities and location quality.\",\n",
    "                'comfort_index_score': \"Evaluates bed count, bathroom privacy, and space.\",\n",
    "                'family_suitability_score': \"Safety, kid-friendly amenities, and spaciousness.\",\n",
    "                'business_ready_score': \"Wifi quality, dedicated workspace, and self check-in.\",\n",
    "                'social_experience_score': \"Nightlife proximity and dining options nearby.\",\n",
    "                'relaxation_wellness_score': \"Quietness, views, and spa-like amenities.\",\n",
    "                'local_immersion_score': \"Authentic neighborhood feel vs tourist trap vibes.\"\n",
    "            }\n",
    "            # Construct the HTML for the score bar\n",
    "            scores_html = '<div class=\"score-container\">'\n",
    "            for label, col_name in score_columns:\n",
    "                try:\n",
    "                    raw_val = row.get(col_name, 90)\n",
    "                    raw_val = float(raw_val) if raw_val is not None else 90\n",
    "                except: raw_val = 0\n",
    "                raw_val = max(5, min(100.0, raw_val))\n",
    "                normalized = raw_val / 100.0\n",
    "                boost_factor = 0.2\n",
    "                val = (normalized ** boost_factor) * 10\n",
    "\n",
    "                bg_color = get_gradient_color(val)\n",
    "                tooltip_text = SCORE_EXPLANATIONS.get(col_name, \"Analysis of property features.\")\n",
    "\n",
    "                scores_html += f'<div class=\"score-card\" style=\"background-color: {bg_color};\">'\n",
    "                scores_html += f'<div class=\"score-val\">{val:.0f}</div>'\n",
    "                scores_html += f'<div class=\"score-label\">{label}</div>'\n",
    "                scores_html += f'<span class=\"tooltip-text\">{tooltip_text}</span>'\n",
    "                scores_html += '</div>'\n",
    "            scores_html += '</div>'\n",
    "            st.markdown(scores_html, unsafe_allow_html=True)\n",
    "\n",
    "            c_img, c_stats = st.columns([1.5, 1])\n",
    "            with c_img:\n",
    "                st.markdown(f'<img src=\"{gallery_images[idx]}\" class=\"custom-img\">', unsafe_allow_html=True)\n",
    "                if len(gallery_images) > 1:\n",
    "                    st.markdown(\"<div style='height: 10px;'></div>\", unsafe_allow_html=True)\n",
    "                    c_spacer_l, c_prev, c_text, c_next, c_spacer_r = st.columns([3, 1, 3, 1, 3])\n",
    "                    with c_prev:\n",
    "                        if st.button(\"⬅️\", key=f\"prev_{prop_key}\", use_container_width=True): st.session_state.gallery_idx = (idx - 1) % len(gallery_images); st.rerun()\n",
    "                    with c_text:\n",
    "                        st.markdown(f\"<div style='text-align:center;color:#555;font-weight:500;padding-top:8px;'>{idx + 1} / {len(gallery_images)}</div>\", unsafe_allow_html=True)\n",
    "                    with c_next:\n",
    "                        if st.button(\"➡️\", key=f\"next_{prop_key}\", use_container_width=True): st.session_state.gallery_idx = (idx + 1) % len(gallery_images); st.rerun()\n",
    "\n",
    "            with c_stats:\n",
    "                score = float(row.get('risk_score_raw', 0) or 0)*10\n",
    "                st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "                st.markdown(f'<div class=\"score-badge\" style=\"background:{get_score_color(score)}\">Fraud Risk Score: {score:.1f}/10</div>', unsafe_allow_html=True)\n",
    "                st.write(f\"\")\n",
    "                st.write(f\"**\uD83D\uDCB0 Price:** ${row.get('price')}\")\n",
    "                st.write(f\"**\uD83C\uDFE0 Arrangement:** {short_description}\")\n",
    "                st.write(f\"**\uD83D\uDC65 Guests:** {row.get('guests', 'N/A')}\")\n",
    "                st.write(f\"**⭐ Rating:** {row.get('ratings')} ({row.get('property_number_of_reviews')} reviews)\")\n",
    "                st.write(f\"**\uD83D\uDC15 Pets:** {pets_allowed}\")\n",
    "                \n",
    "                st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "                st.link_button(\"\uD83C\uDFE0 View in Airbnb\", row.get('url', '#'))\n",
    "                s = safe_parse(row.get('seller_info'), {})\n",
    "                st.link_button(\"\uD83D\uDC64 Host details\", s.get('url', '#') if isinstance(s, dict) else '#')\n",
    "\n",
    "            hls = safe_parse(row.get('highlights'), [])\n",
    "            if hls and isinstance(hls, list):\n",
    "                st.markdown(\"#### ✨ Highlights\")\n",
    "                ch = st.columns(min(3, len(hls)))\n",
    "                for i, h in enumerate(hls[:3]):\n",
    "                    if isinstance(h, dict):\n",
    "                        with ch[i]: st.markdown(f'<div class=\"highlight-item\"><b>{h.get(\"name\")}</b><br><small>{h.get(\"value\")}</small></div>', unsafe_allow_html=True)\n",
    "\n",
    "            with st.expander(\"\uD83D\uDCDD Description\"): st.write(row.get('description', 'No desc.'))\n",
    "            revs = safe_parse(row.get('reviews'), [])\n",
    "            st.markdown(f\"#### \uD83D\uDCAC Reviews ({len(revs) if isinstance(revs, list) else 0})\")\n",
    "            if isinstance(revs, list) and revs:\n",
    "                rh = \"\".join([f'<div class=\"review-bubble\">\"{r}\"</div>' for r in revs])\n",
    "                st.markdown(f'<div class=\"reviews-container\">{rh}</div>', unsafe_allow_html=True)\n",
    "            else: st.info(\"No text reviews.\")\n",
    "            st.markdown('</div>', unsafe_allow_html=True)\n",
    "        \n",
    "        ca1, ca2, ca3 = st.columns([1, 2, 1])\n",
    "        with ca1:\n",
    "            st.button(\"❌ Pass\", on_click=handle_swipe, args=(\"dislike\",), use_container_width=True, key=f\"pass_btn_{prop_key}\", type=\"secondary\")\n",
    "        with ca2:\n",
    "            if st.button(\"\uD83D\uDCCB Favorites\", use_container_width=True): st.session_state.page = 'results'; st.rerun()\n",
    "        with ca3:\n",
    "            st.button(\"❤️ Like\", on_click=handle_swipe, args=(\"like\",), use_container_width=True, type=\"primary\", key=f\"like_btn_{prop_key}\")\n",
    "\n",
    "    # PAGE 3: RESULTS\n",
    "    elif st.session_state.page == 'results':\n",
    "        st.subheader(\"\uD83C\uDF89 Your Favorites\")\n",
    "        \n",
    "        if not st.session_state.selections: \n",
    "            st.info(\"No likes yet.\")\n",
    "        else:\n",
    "            for i, item in enumerate(st.session_state.selections):\n",
    "                # Display logic... (Keep your existing expander code here)\n",
    "                with st.expander(f\"{i+1}. {item.get('name', 'Prop')} - ${item.get('price')}\", expanded=True):\n",
    "                    cr1, cr2 = st.columns([1, 3])\n",
    "                    with cr1: \n",
    "                        img = item.get('image')\n",
    "                        if img: st.markdown(f'<img src=\"{img}\" class=\"custom-img\" style=\"height: 150px;\">', unsafe_allow_html=True)\n",
    "                    with cr2:\n",
    "                        st.write(f\"Location: {item.get('city')}\")\n",
    "                        st.link_button(\"Book\", item.get('url', '#'))\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        cb1, cb2 = st.columns(2)\n",
    "        \n",
    "        with cb1: \n",
    "            # --- UI LOGIC CHANGE: Check if any properties remain ---\n",
    "            if st.session_state.current_df is None or st.session_state.current_df.empty:\n",
    "                st.info(\"✅ You have viewed all properties.\")\n",
    "            else:\n",
    "                if st.button(\"⬅️ Back to Swipe\"): \n",
    "                    st.session_state.page = 'swipe'\n",
    "                    st.rerun()\n",
    "            # -------------------------------------------------------\n",
    "            \n",
    "        with cb2:\n",
    "            if st.button(\"\uD83D\uDD04 New Search\"): \n",
    "                st.session_state.selections = []; st.session_state.current_df = None; st.session_state.page = 'form'; st.rerun()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e81a47-5919-49e5-8d0e-4f3e5aedc51b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "run the app"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Starting Streamlit server...\n--------------------------------------------------------------------------------\n✅ App is running! Click the link below to open:\n\n\uD83D\uDD17 https://adb-983293358114278.18.azuredatabricks.net/driver-proxy/o/983293358114278/1223-140823-io2f79uq/8501/\n\n--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 1. Kill old streamlit processes to free the port\n",
    "try:\n",
    "    subprocess.run([\"pkill\", \"-f\", \"streamlit\"], check=False)\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "log_file = open(\"streamlit.log\", \"w\")\n",
    "\n",
    "# 2. Run the streamlit app in the background\n",
    "process = subprocess.Popen(\n",
    "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\", \"--server.headless\", \"true\"],\n",
    "    # stdout=subprocess.PIPE,\n",
    "    # stderr=subprocess.PIPE\n",
    "    stdout=log_file, \n",
    "    stderr=log_file\n",
    ")\n",
    "\n",
    "print(\"\uD83D\uDE80 Starting Streamlit server...\")\n",
    "time.sleep(5) # Wait for server to initialize\n",
    "\n",
    "# 3. Construct the Driver Proxy URL\n",
    "try:\n",
    "    cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    org_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "    \n",
    "    # Try getting workspace URL dynamically\n",
    "    workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\", None)\n",
    "    \n",
    "    if workspace_url:\n",
    "        dashboard_url = f\"https://{workspace_url}/driver-proxy/o/{org_id}/{cluster_id}/8501/\"\n",
    "    else:\n",
    "        dashboard_url = f\"https://adb-{org_id}.{cluster_id[0:2]}.azuredatabricks.net/driver-proxy/o/{org_id}/{cluster_id}/8501/\"\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"✅ App is running! Click the link below to open:\")\n",
    "    print(f\"\\n\uD83D\uDD17 {dashboard_url}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error constructing URL: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab - final project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}